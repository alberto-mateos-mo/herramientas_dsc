# (PART) Modelos Lineales {-}

# Regresión Lineal

## Un poco de história

Los primeros problemas prácticos tipo regresión iniciaron en el siglo XVIII, relacionados con la navegación basada en la Astronomía.
Legendre desarrolló el método de mínimo cuadrados en 1805. Gauss afirma que él desarrolló este método algunos años antes y demuestra, en 1809, que mínimos cuadrados proporciona una solución óptima cuando los errores se distribuyen normal. Francis Galton acuña el término regresión al utilizar el modelo para explicar el fenómeno de que los hijos de padres altos, tienden a ser altos en su generación, pero no tan altos como lo fueron sus padres en la propia, por lo que hay un efecto de _regresión_.

El modelo de regresión lineal es, probablemente, el modelo de su tipo más conocido en estadística.

El modelo de regresión se usa para explicar o modelar la relación entre una sola variable, $y$, llamada dependiente o respuesta, y una o más variables predictoras, independientes, covariables, o explicativas, $x_1, x_2, ..., x_p$. Si $p = 1$, se trata de un modelo de regresión simple y si $p > 1$, de un modelo de regresión múltiple. En este modelo se asume que la variable de respuesta, $y$, es aleatoria y las variables explicativas son fijas, es decir, no aleatorias.

La variable de respuesta debe ser continua, pero los regresores pueden tener cualquier escala de medición.

## Objetivos del análisis de regresión

Existen varios objetivos dentro del análisis de regresión, entre otros:

- Determinar el efecto, o relación, entre las variables explicativas y la respuesta.
- Predicción de una observación futura.
- Describir de manera general la estructura de los datos.

## El algorítmo de regresión lineal

Sea $\Phi: \mathcal{X} \rightarrow \mathbb{R}^N$ y consideremos la familia de hipótesis lineales $$H=\{x\mapsto w \cdot \Phi(x)+b | w\in\mathbb{R}^N, b\in\mathbb{R}\}$$

La regresión lineal consiste en buscar la hipótesis $h\in H$ con el menor error cuadrático medio, es decir, se debe resolver el problema de optimización: $$\min \frac{1}{m}\sum_{i=1}^{m}(h(x_i)-y_i)^2$$

## Regresión lineal simple

```{r, message=FALSE, warning=FALSE, echo=FALSE}
require(ggplot2)

x <- c(609, 629, 620, 564, 645, 493, 606, 660, 630, 672, 778, 616, 727, 810, 778, 823, 755, 710, 701, 803, 855, 838, 830, 864, 635, 565, 562, 580, 596, 597, 636, 559, 615, 740, 677, 675, 629, 692, 710, 730, 763, 686, 717, 737, 816)

y <- c(241, 222, 233, 207, 247, 189, 226, 240, 215, 231, 263, 220, 271, 284, 279, 272, 268, 278, 238, 255, 308, 281, 288, 306, 236, 204, 216, 225, 220, 219, 201, 213, 228, 234, 237, 217, 211, 238, 221, 281, 292, 251, 231, 275, 275)

ggplot(data.frame(x = x, y = y))+
  geom_point(aes(x, y), colour = "#f6583a", size = 6)+
  geom_smooth(aes(x, y), method = "lm", se = FALSE, colour = "#85037e", size = 2)+
  theme_minimal()+
  labs(y = "y", x = bquote(Phi(x)))
```

Para este modelo supondremos que nuestra respuesta, $y$, es explicada únicamente por una covariable, $x$. 

Entonces, escribimos nuestro modelo como:

$$y^{(i)}=\beta_0+\beta_1x^{(i)}+\epsilon^{(i)},\ \ i=1,2,\dots,n$$
Como podemos observar, se ha propuesto una relación lineal entre la variable $y$ y la variable explicativa $x$, que es nuestro primer supuesto sobre el modelo: La relación funcional entre $x$ y $y$ es una línea recta.

Observamos que la relación no es perfecta, ya que se agrega el término de error, $\epsilon$. Dado que la parte aleatoria del modelo es la variable $y$, asumimos que al error se le “cargan” los errores de medición de $y$, así como las perturbaciones que le pudieran ocasionar los términos omitidos en el modelo. 
Gauss desarrolló este modelo a partir de la teoría de errores de medición, que es de donde se desprenden los supuestos sobre este término:

- $\mathbb{E}(\epsilon^{(i)})=0$
- $\mathbb{V}ar(\epsilon^{(i)})=\sigma^2$
- $\mathbb{C}ov(\epsilon^{(i)},\epsilon^{(j)})=0, \ \forall i\neq j$

N.B. Los errores $\epsilon^{(i)}$ son variables aleatorias no observables.

## Solución al problema de regresión lineal simple

### Mínimos cuadrados ordinarios

En una situación real, tenemos $n$ observaciones de la variable de respuesta así como de la variable explicativa, que conforman las parejas de entrenamiento $(x_i, y_i), \ i = 1, 2, ..., n$.

Entonces, nuestro objetivo será encontrar la recta que mejor ajuste a los datos observados.

Utilizaremos el método de mínimos cuadrados para estimar los parámetros del modelo, que consiste en minimizar la suma de los errores al cuadrado, esto es:

$$\sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n(y_i-(\beta_0+\beta_1x^{(i)}))^2$$
Al minimizar la expresión anteriore obtenemos las siguientes expresiones para los estimadores:

$$\hat{\beta_1}=\frac{\sum_{i=1}^ny_i(x_i-\bar{x})}{\sum_{i=1}^n(x_i-\bar{x})^2}$$
$$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$$
Una desventaja del método de mínimos cuadrados, es que no se pueden hacer procesos de inferencia sobre los parámetros de interés $\beta_0$ y $\beta_1$; procesos como intervalos de confianza o pruebas de hipótesis.

Para subsanar esta deficiencia, es necesario asumir una distribución para el error, $\epsilon_i$, que, siguiendo la teoría general de errores, se asume que tiene distribución normal, con media cero y varianza $\sigma^2$.

Este supuesto garantiza que las distribuciones de $y_i,\ \hat{\beta_0},\ \hat{\beta_1}$ sean normales, lo que permite tanto la construcción de intervalos de confianza como de pruebas de hipótesis.

N.B El estimador de $\sigma^2$ está dado por $\hat{\sigma^2}=\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n-2}$

#### Pruebas de hipótesis

En el modelo de regresión lineal simple, la prueba de hipótesis más importante es determinar si estadísticamente existe la dependencia líneal entre $x$ y $y$, y que no sea producto del muestreo (debido al azar). Es decir, realizar la prueba de hipótesis:

$$H_0:\beta_1=0 \ vs.\ H_a:\beta_1\neq 0$$
No rechazar la hipótesis nula, implicaría que la variable $x$ _no ayuda a explicar_ a $y$ o bien que, tal vez, la relación entre estas variables __no es lineal__. 

En este modelo, esta  última explicación es un poco cuestionable, ya que se parte, de inicio, del diagrama de dispersión de los datos.

Si rechazamos la hipótesis nula, implicará que $x$ _es importante para explicar la respuesta_ $y$ y que la relación lineal entre ellas puede ser adecuada.

Rechazar esta hipótesis nula, también podría implicar que existe una relación lineal entre las variables pero, tal vez, se pueda mejorar el ajuste con algún otro término no lineal.

#### Interpretación de los parámetros

Cuando se tiene una recta en el sentido determinista, los parámetros $\beta_0$ y $\beta_1$ tienen una interpretación muy clara; $\beta_0$ se interpreta como el valor de $y$ cuando $x$ es igual a cero y $\beta_1$ como el cambio que experimenta la variable de respuesta $y$ por unidad de cambio en $x$. 

La interpretación, desde el punto de vista estadístico, de los parámetros estimados en el modelo de regresión es muy similar:

- $\hat{\beta_0}$ es el promedio esperado de la respuesta $y$ cuando $x = 0$ (este parámetro tendrá una interpretación dentro del
modelo, si tiene sentido que $x$ tome el valor cero, de lo contrario, no tiene una interpretación razonable) y 

- $\hat{\beta_1}}$ es el cambio promedio o cambio esperado en $y$ por unidad de cambio en $x$.

## Regresión lineal múltiple

La mayoría de los fenómenos reales son multicausales, por esta razón, un modelo de regresión más acorde a estudios reales es el modelo de regresión lineal múltiple, que es la generalización del modelo simple. 

En este modelo supondremos que la variable de respuesta, $y$, puede explicarse a través de una colección de $k$ covariables $x_1,\dots,x_k$.

El modelo se escribe de la siguiente manera:

$$y_i = \beta_0+\beta_1 x_1^{(i)}+\beta_2 x_2^{(i)}+\dots++\beta_k x_k^{(i)}+\epsilon_i$$
Al igual que en el caso simple, los parámetros del modelo se pueden estimar por mínimos cuadrados, con el inconveniente de que no se pueden realizar inferencias sobre ellos. Nuevamente, para poder hacer intervalos de confianza y pruebas de hipótesis sobre los verdaderos parámetros hay que suponer que el vector de errores se distribuye normal, en este caso multivariada, es decir:

$$\epsilon\sim N_n(0,\sigma^2\mathbb{I})$$

Esta estructura del error permite tener las mismas propiedades distribucionales que en regresión simple, es decir, $y_i$ se distribuye normal y $\beta_i$ tiene distribución normal, facilitando las inferencias sobre cada parámetro y la construcción de intervalos de predicción para las $y$'s.

## Solución al problema de regresión lineal múltiple.

### Ecuaciones normales

Las expresiones para estimar los parámetros involucrados en el modelo son:

$$\hat{\beta}=(X^TX)^{-1}X^Ty$$
$$\hat{\sigma}^2=\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n-p}$$

donde $p=k+1$ es el número total de parámetros en el modelo.

Tanto en el modelo simple como en el múltiple, la variación total de las $y$'s se puede descomponer en una parte que explica el modelo, i.e., los $k$ regresores o variables explicativas y otra no explicada por estas variables, llamada error.

$$\sum_{i=1}^n(y_i-\bar{y})^2=\sum_{i=1}^n(\hat{y_i}-\bar{y})^2+\sum_{i=1}^n(\hat{y_i}-y_i)^2$$
#### Pruebas de hipótesis

La descomposición anterior ayuda para realizar la importante prueba de hipótesis:

$$H_0:\beta_1=\beta_2=\dots=\beta_k=0\ vs.\ H_a:\beta_i\neq0 \ p.a. \ i$$

misma que se realiza a través del cociente entre los errores cuadráticos medios:

$$F_0=\frac{SS_R/k}{SS_E/(n-k-1)}=\frac{MS_R}{MS_E}\sim F_{(k,n-k-1)}$$
Esta estadística se desprende de la tabla de análisis de varianza, que es muy similar a la tabla ANOVA que se utiliza para hacer pruebas de hipótesis.

En este caso la tabla es:

| Fuente de variación | Grados de libertad | Suma de cuadrados |   Cuadrados medios  |           F           |
|:-------------------:|:------------------:|:-----------------:|:-------------------:|:---------------------:|
|      Regresión      |          k         |       $SS_R$      |    $MS_R=SS_R/k$    |                       |
|        Error        |        n-k-1       |       $SS_E$      | $MS_E=SS_E/(n-k-1)$ | $F=\frac{MS_R}{MS_E}$ |
|        Total        |         n-1        |      $S_{yy}$     |                     |                       |

Por lo general, esta estadística rechaza la hipótesis nula, ya que de lo contrario, implicaría que ninguna de las variables contribuye a explicar la respuesta, $y$. Como se puede observar en la hipótesis alternativa, el rechazar $H_0$ solo implica que al menos uno de los regresores contribuye significativamente a explicar $y$.

Asimismo, el rechazar $H_0$ no implica que todos contribuyan ni tampoco dice cuál o cuáles contribuyen, por esta razón, una salida estándar de regresión múltiple tiene pruebas individuales sobre la significancia de cada regresor en el modelo. 

El estadístico para hacer tanto los contrastes de hipótesis como los intervalos de confianza individuales, es:

$$t=\frac{\hat{\beta_i}-\beta_0^{(i)}}{\sqrt{\hat{\mathbb{V}ar}(\hat{\beta_i})}}\sim t_{(n-p)}$$
Podemos apreciar que los constrastes de hipótesis se pueden hacer contra cualquier valor particular del parámetro $\beta_0^{(i)}$, en general. No obstante, en las pruebas estándar sobre los parámetros de un modelo, este valor particular es 0, ya que se intenta determinar si la variable asociada al $i$-ésimo parámetro es estadísticamente significativa para explicar la respuesta. 

Por lo que el estadístico para este caso es:

$$t=\frac{\hat{\beta_i}}{\sqrt{\hat{\mathbb{V}ar}(\hat{\beta_i})}}\sim t_{(n-p)}$$

De este estadístico se desprenden también los intervalos de confianza para cada parámetro:

$$\beta_i\in(\hat{\beta_i}\pm t_{(n-p,1-\alpha/2)} \sqrt{\hat{\mathbb{V}ar} (\hat{\beta_i})})$$
#### Interpretación de los parámetros

La interpretación de cada parámetro es similar a la del coeficiente de regresión $\hat{\beta_1}$ en el modelo simple, anexando la frase: "manteniendo constantes el resto de las variables". 

Esto es, $\hat{\beta_i}}$ es el cambio promedio o cambio esperado en $y$ por unidad de cambio en $x_i$, sin considerar cambio alguno en ninguna de las otras variables dentro del modelo, es decir, suponiendo que estas otras variables permanecen fijas. Esta interpretación es similar a la que se hace de la derivada parcial en un modelo determinista. 

Nuevamente, la interpretación de $\hat{\beta_0}$  estará sujeta a la posibilidad de que, en este caso, todas las variables puedan tomar el valor cero.

#### Predicción de nuevos valores

Uno de los usos más frecuentes del modelo de regresión es el de predecir un valor de la respuesta para un valor particular de las covariables en el modelo. Si la predicción se realiza para un valor de las covariables dentro del rango de observación de las mismas, se tratará de una interpolación, y si se realiza para un valor fuera de este rango, hablaremos de una extrapolación. 

En cualquiera de los dos casos, estaremos interesados en dos tipos de predicciones:

- Predicción de la respuesta media: $y_0=\mathbb{E}(y|X_0)$

- Predicción de una nueva observación: $y_0$

En ambos casos, la estimación puntual es la misma: $\hat{y_0}=X_0^T\hat{\beta}$

Lo que difiere es el intervalo de predicción. 

Para la respuesta media es: $y_0=(\hat{y_0}\pm t_{(n-p,1-\alpha/2)}\sqrt{\hat{\sigma^2}X_0^T(X^TX)^{-1}X_0})$

Y para predecir una observación: $y_0=(\hat{y_0}\pm t_{(n-p,1-\alpha/2)}\sqrt{\hat{\sigma^2}(1+X_0^T(X^TX)^{-1}X_0)})$

#### Coeficiente de determinación

Un primer elemento de juicio sobre el modelo de regresión lo constituye el coeficiente de determinación $R^2$, que es la proporción de variabilidad de las $y$'s que es explicada por las $x$'s y que se escribe como:

$$R^2=\frac{SS_R}{S_{yy}}=1-\frac{SS_E}{S_{yy}}$$
Una $R^2$ cercana a uno implicaría que mucha de la variabilidad de la respuesta es explicada por el conjunto de regresores incluidos en el modelo.

Es deseable tener una $R^2$ grande en nuestro modelo, pero esto no significa, como mucha gente piensa, que ya el modelo está bien ajustado.

### Evaluación de supuestos

Los dos modelos de regresión presentados, el simple y el múltiple, se construyeron sobre los supuestos de:

- La relación funcional entre la variable de respuesta $y$ y cada regresor $x_i$ es lineal

- La esperanza de los errores es cero, $\mathbb{E}(\epsilon_i=0)$

- La varianza de los errores es constante, $\mathbb{V}ar(\epsilon_i) = \sigma^2$

- Los errores no están correlacionados, $\mathbb{C}ov(\epsilon_i, \epsilon_j) = 0;\ i\neq j$

- Los errores tienen distribución normal con media cero y varianza $\sigma^2$

Entonces, para garantizar que el modelo es adecuado, es indispensable verificar estos supuestos.

#### Residuos

Los elementos más importantes para verificar estos supuestos son los residuos, definidos como:

$$e_i=y_i-\hat{y}_i$$

Estos residuos representan la discrepancia entre la respuesta predicha por el modelo ajustado, $\hat{y}_i$ y el correspondiente valor observado, $y_i$.

En la literatura de regresi ́on lineal existen cuatro tipos de residuos, a saber

- __Residuo crudo__: $e_i$

- __Residuo estandarizado__: $d_i=\frac{e_i}{\sqrt{\hat{\sigma}^2}}$

- __Residuo estudentizado interno__: $r_i=\frac{e_i}{\sqrt{\hat{\sigma}^2(1-h_{ii}})}$

- __Residuo estudentizado externo__: $t_i=\frac{e_i}{\sqrt{\hat{\sigma_{(-i)}}^2(1-h_{ii})}}$

Estos residuos se utilizan en los distintos procedimientos para evaluar los supuestos y lo adecuado del ajuste del modelo. La mayoría de las pruebas conocidas para la verificación de los supuestos, son pruebas gráficas.

Indudablemente, la prueba más importante es sobre la normalidad de los errores, ya que sobre este supuesto descansan todas la inferencias de este modelo. 

La manera de verificarlo es a través de la gráfica conocida como QQ-plot o QQ-norm, que grafica los cuantiles teóricos de una distribución normal (eje x) vs. los cuantiles asociados a los residuos. Entonces, si los residuos realmente provienen de una normal, la gráfica debe mostrar la función identidad. Fuertes desviaciones de esta línea darían evidencia de que los errores no se distribuyen normal.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
require(ggplot2)
require(ggfortify)
require(magrittr)

model <- lm(dist~speed, data = cars)

autoplot(model)[[2]]+
  theme_minimal()
```

#### Linealidad de los predictores

La manera estándar de evaluar la linealidad de las variables explicativas es a través de la gráfica de cada una de ellas contra los residuos. Si la variable en cuestión ingresa al modelo de manera lineal, esta gráfica debe mostrar un patrón totalmente aleatorio entre los puntos dispuestos en ella.

Cuando la variable explicativa es politómica, este tipo de gráficas son poco ilustrativas en este sentido.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
autoplot(model)[[1]]+
  theme_minimal()
```

#### Supuestos sobre los errores

Si la gráfica entre los valores ajustados y los residuos estandarizados, muestra un patrón aleatorio, es simétrica alrededor del cero y los puntos están comprendidos entre los valores -2 y 2, entonces se tendrá evidencia de que los errores tienen media cero, varianza constante y no están correlacionados.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
autoplot(model)[[3]]+
  theme_minimal()
```

Los métodos mostrados hasta ahora, permiten evaluar el modelo de manera global y no por cada observación dentro del mismo. Dado que una observación puede resultar determinante sobre alguna(s) característica(s) del modelo, es conveniente verificar el impacto que cada observación pueda tener en los distintos aspectos del modelo. Las estadísticas para evaluar el impacto que tiene una observación sobre todo el vector de parámetros, alguno de los regresores y sobre los valores predichos, se basan en la misma idea, que consiste en cuantificar el cambio en la característica de interés con y sin la observación que se está evaluando.

##### Puntos palanca

Antes de presentar las estadísticas que servirán para hacer este diagnóstico, introduciremos un elemento que es común a ellas: la llamada palanca (leverage) de una observación. 

Recordemos que el ajuste del modelo se expresaba como:

$$\hat{\beta}=(X^TX)^{-1}X^Ty \Rightarrow \hat{y}=X\hat{\beta}=Hy$$

Con $H$ conocida como la matriz sombrero.

Un resultado fundamental sobre esta matriz sombrero es:

$$\mathbb{V}ar(e)=(I-H)\sigma^2 \Rightarrow \mathbb{V}ar(e_i)=(1-h_i)\sigma^2$$
Con $h_i$ el i-ésimo elemento de la diagonal de la matriz $H$. 

Observemos que esta palanca sólo depende de $X$, entonces, una observación con una palanca, $h_i$, grande, es aquella con valores extremos en alguna(s) de su(s) covariable(s). 

Ya que el promedio de las $h_i's$ es $p/n$, consideraremos una observación con palanca grande si su palanca es mayor a $2p/n$. En este sentido, $h_i$ corresponde a la _distancia de Mahalanobis_ de $X$ definida como $(X-\bar{X})^T\hat{\Sigma}^{-1}(X-\bar{X})$.

La dependencia de las estadísticas para el diagnóstico de las observaciones, estriba en que sus cálculos dependen de los valores de la palanca de cada individuo. Estas estadísticas son:

- Distancia de Cook
- Dfbetas
- Dffits

__Distancia de Cook__: Sirve para determinar si una observación es influyente en todo el vector de parámetros. Una observación se considera influyente, si su distancia de Cook sobrepasa el valor uno.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(model, aes(seq_along(.cooksd)))+
  geom_linerange(aes(ymin = 0, ymax = .cooksd))+
  labs(x = "Observation", y = "Cook's distance")
  theme_minimal()
```

__Dfbetas__: Sirven para determinar si una observación es influyente en alguno de los coeficientes de regresión. Hay un dfbeta por cada parámetro dentro del modelo, incluido, por supuesto, el de la ordenada al origen. La regla de dedo es que la observación $i$ es influyente en el j-ésimo coeficiente de regresión si:

$$|Dfbetas_{j,i}|>\frac{2}{\sqrt{n}}$$
```{r, echo=FALSE, message=FALSE, warning=FALSE}
dfbetas <- janitor::clean_names(as.data.frame(dfbetas(model)))

ggplot(dfbetas, aes(1:nrow(dfbetas)))+ 
  geom_linerange(aes(ymin = 0, ymax = intercept))+
  labs(x = "Observation", y = "dfbeta")+
  ggtitle("DFbeta0")+
  theme_minimal()

ggplot(dfbetas, aes(1:nrow(dfbetas)))+ 
  geom_linerange(aes(ymin = 0, ymax = speed))+
  labs(x = "Observation", y = "dfbeta")+
  ggtitle("DFbeta1")+
  theme_minimal()
```

__Dffits__: Se utilizan para determinar si una observación es influyente en la predicción de $y$. Se dice que la i-ésima observación es influyente para predecir $y$, si:

$$|Dffits_i|>2\sqrt{\frac{p}{n}}$$
```{r, echo=FALSE, message=FALSE, warning=FALSE}
dffits <- janitor::clean_names(as.data.frame(dffits(model)))

ggplot(dffits, aes(1:nrow(dffits)))+
  geom_linerange(aes(ymin = 0, ymax = dffits_model))+
  labs(x = "Observation", y = "dffits")+
  ggtitle("DFfits")+
  theme_minimal()
```

#### Multicolinealidad

El modelo de regresión lineal múltiple, se construye bajo el supuesto de que los regresores son ortogonales, i.e., son independientes.

Desafortunadamente, en la mayoría de las aplicaciones el conjunto de regresores no es ortogonal. Algunas veces, esta falta de ortogonalidad no es seria; sin embargo, en algunas otras los regresores están muy cerca de una perfecta relación lineal, en tales casos las inferencias realizadas a través del modelo de regresión lineal pueden ser erróneas. Cuando hay una cercana dependencia lineal entre los regresores, se dice que estamos en presencia de un problema de multicolinealidad.

__Efectos de la multicolinealidad__:

- Varianzas de los coeficientes estimados son muy grandes.

- Los estimadores calculados de distintas sub muestras de la misma población, pueden ser muy diferentes.

- La significancia de algún regresor se puede ver afectada (volverse no significativo) por que su varianza es más grande de lo que debería ser en realidad o por la correlación de la variable con el resto dentro del modelo.

- Es común que algún signo de un parámetro cambie, haciendo ilógica su interpretación dentro del modelo.

##### ¿Cómo detectar multicolinealidad?

__Matriz de correlación.__ 

Examinar las correlaciones entre pares de variables: 

$$r_{ij}\ \ \  i, j = 1, 2, \dots, k\ \  i\neq j$$

Pero, si dos o más regresores están linealmente relacionados, es posible que ninguna de las correlaciones entre cada par de variables, sea grande.

__Factor de inflación de la varianza.__

$$VIF_j=(1-R_j^2)^{-1}$$
Con $R_j^2$ el coeficiente de determinación del modelo de regresión entre el j-ésimo regresor, $x_j$ (tomado como variable de respuesta) y el resto de los regresores $x_i$, $i\neq j$.

Experiencias prácticas indican que si algunos de los VIF’s excede a 10, su coeficiente asociado es pobremente estimado por el modelo debido a multicolinealidad.

__Análisis del eigensistema.__

Basado en los eigenvalores de la matriz $X^TX$.

__Número de condición.__

$$K=\frac{\lambda_{max}}{\lambda_{min}}$$
Si el número de condición es menor que 100, no existen problemas serios de multicolinealidad. Si está entre 100 y 1000 existe de moderada a fuerte multicolinealidad y si excede a 1000, hay severa multicolinealidad.

__Índice de condición.__

$$k_j=\frac{\lambda{max}}{\lambda_j}$$
Si el  índice de condición es menor que 10, no hay ningún problema. Si está entre 10 y 30, hay moderada multicolinealidad, y si es mayor que 30, existe una fuerte colinealidad en la j-ésima variable en el modelo.

N.B. En algunos paquetes estos índices se presentan aplicando la raíz cuadrada a su expresión, entonces hay que extraer raíz a los puntos de corte de los criterios correspondientes.

#### Relación funcional

Un supuesto importante en el modelo de regresión es el que considera que debe existir una relación funcional lineal entre cada regresor y la variable de respuestas. Pero, ¿qué debemos hacer si no se cumple esta relación lineal de la respuesta con alguno(s) de los regresor(es)?

Primero, ya dijimos que este supuesto se evalúa realizando la gráfica de dispersión entre los residuos del modelo y los valores de la variable en cuestión. Cuando no hay una asociación lineal entre la respuesta y la covariable, generalmente este diagrama de dispersión muestra un patrón (tendencia) que sugiere qué tipo de transformación se debería hacer a la covariable para lograr linealidad con la respuesta.

Debe quedar claro que la transformación puede realizarse a la variable explicativa o a la variable de respuesta. 

A muchos investigadores no les gusta transformar la respuesta porque argumentan que pierden _interpretabilidad_ del modelo. Aunque esto puede ser cierto, existen transformaciones de la respuesta que pueden _regresarse_ para interpretar el modelo con la respuesta original.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
data.frame(x = cars$speed, y = model$residuals) %>% 
  ggplot(aes(x = x, y = y))+
  geom_point()+
  geom_smooth(se = FALSE)+
  labs(x = "x", y = "residuals")
  theme_minimal()
  
rm(model)
```

Un problema asociado a esta identificación por parte del usuario, es que debe tener experiencia para asociar estas formas a una función analítica específica; hecho no necesariamente cierto. Por lo tanto, requiere de alguna herramienta técnica que pudiera auxiliarlo en esta labor.

Un buen auxiliar, en el caso de que se crea que es necesario transformar la respuesta, es usar la llamada trasformación Box-Cox.

##### Transformación Box-Cox

La transformación Box-Cox de la respuesta, es una función que sirve para normalizar la distribución del error, estabilizar la varianza de este error y mejorar la relación lineal entre $y$ y las $X’s$. 

Se define como:

$$y_i^{\lambda} = \left\{ \begin{array}{ll} \frac{y_i^{\lambda-1}}{\lambda}, &  \lambda \neq 0;\\ ln(y_i), & \lambda=0 .\end{array} \right.$$
La siguiente tabla muestra el rango de valores de $\lambda$ que estarían asociados a una transformación analítica común.

| Rango $\lambda$ | Transformación Asociada |
|:---------------:|:-----------------------:|
|   (-2.5, -1.5]  |     $\frac{1}{y^2}$     |
|  (-1.5, -0.75]  |       $\frac{1}{y}$      |
|  (-0.75, -0.25] |   $\frac{1}{sqrt{y}}$   |
|  (-0.25, 0.25]  |         $ln(y)$         |
|   (0.25, 0.75]  |        $\sqrt{y}$       |
|   (0.75, 1.25]  |           $y$           |
|   (1.25, 2.5)   |          $y^2$          |

##### Transformación Box-Tidwell

Box y Tidwell implementan un proceso iterativo para encontrar la mejor transformación de las variables predictoras en el modelo de regresión lineal. 

Definiendo como $X_j^{\gamma_j}$ la correspondiente transformación Box-Tidwell de la variable $j$.

La tabla anterior para las transfomaciones analíticas de la respuesta, también aplican para estas transformaciones de los predictores.

## Aplicación en R


# Modelos lineales generalizados

En el capítulo anterior exploramos el modelo básico que nos permite responder a la pregunta: ¿puede ser la variable de interés predicha por un conjunto de variables explicativas?

Sin embargo, para poder utilizar dicho modelo, es necesario que la variable respuesta sea continua y cumpla las hipótesis estándar del modelo lineal (datos normales, varianza constante, etc.)

Si la variable de interés es, por ejemplo binaria podemos ajustar un modelo de regresión logística en donde lo que predecimos son las probabilidades de la ocurrencia del evento medido con la variable binaria.

En 1944, Berkson utilizó por primera vez la regresión logística como una forma de solucionar el problema de explicar una variable dicotómica a través de una variable continua. En este caso, la función logit hace que en lugar de trabajar con valores de la variable respuesta entre $(0, 1)$, trabajemos con una variable respuesta que puede tomar cualquier valor. 

No fue hasta 1972 cuando John Nelder introdujo los modelos lineales generalizados (GLM por sus siglas en inglés), de ahí que en general se considere a la regresión logística como algo distinto a los GLM, cuando lo que ocurre es que tanto la regresión múltiple como la logística, de Poisson, ordinal, etcétera, son casos particulares de un GLM.

Para entender lo que es un GLM, volvamos al modelo de regresión múltiple, en este modelos suponemos que:

$$Y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{k}X_{k}+\epsilon$$

$$E[Y]=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{k}X_{k}$$

Es decir, que existe una relación lineal entre las $X$ y $E[Y]$ (el valor medio de Y dado un cierto valor de las variables explicativas).

Si las observaciones son binarias, entonces:

$$P(Y = 1) = p$$

$$P(Y = 0) = 1-p$$

Y $E[Y] = 0\times P(Y = 0) + 1 \times P(Y = 1) = p$, por lo tanto un modelo de regresión múltiple relacionará directamente la probabilidad de que ocurra un suceso con las variables explicativas, lo cual no es lo que se busca al ajustar un modelo de regresión lineal. 

Lo que hacen los GLM es establecer esa relación lineal no entre la media de la variable respuesta y los predictores, sino entre una función de la media de variable respuesta y los predictores, es decir:

$$g(E[Y])=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{k}X_{k}$$

Según de qué tipo sea la varible $Y$, así será la función $g(\cdot)$.

Entonces se puede decir que un GLM tiene 3 componentes:

- __Componente aleatorio:__ La variable respuesta $Y$ . Para poder utilizar un GLM, la distribución de $Y$ ha de pertenecer a la __familia exponencial__, es decir, su función de densidad ha de poder escribirse como:

$$f(y;\theta,\phi)=exp\{\frac{y\theta-b(\theta)}{a(\phi)}+c(y,\phi)\}$$

donde $a(\cdot)$, $b(\cdot)$ y $c(\cdot)$ son funciones específicas. El parámetro $\theta$ es lo que se llama parámetro canónico de localización y $\phi$ es un parámetro de dispersión. Pertenecen a la familia exponecial la distribución Normal, Bernouilli, Binomial, Poisson, Exponecial, Gamma, entre otras.

- __Componente sistemático:__ Las variables predictoras $X_i \ \ i = 1,...,k$

- __Función liga:__ La función que relaciona la media, $E[Y]$, con las variables predictoras $X$. En el caso del modelo de regresión ordinaria, $\mu = \nu$, por lo tanto la función liga es la identidad.

Hay muchas opciones par la función liga. La función liga canonica es una función que transforma la media en el parámetro canónico $\theta$

$$g(E[Y])=\theta$$

Entoces $g(\cdot)$ es la función liga canónica.

La siguiente tabla muestra las funciones link canónicas para las distribuciones más comunes usadas en los GLMs:

| Distribución | Liga canónica                             |
|--------------|-------------------------------------------|
| Normal       | $X \beta = E[Y]$ (identidad)              |
| Binomial     | $X \beta = ln(\frac{P}{1-P})$ (logística) |
| Poisson      | $X \beta = ln(E[Y])$ (logarítmica)        |
| Exponencial  | $X \beta = \frac{1}{E[Y]})$ (recíproca)   |
| Gamma        | $X \beta = \frac{1}{E[Y]})$ (recíproca)   |

La diferencia que hay entre usar la función liga y usar una transformación, es que la función liga transforma la media, $E[Y]$, y no los datos, $Y$.

Los GLM generalizan la regresión _ordinaria_ de dos modos: permitiendo que la variable de respuesta $Y$ tenga distribuciones diferentes a la normal y, por otro lado, incluyendo distintas funciones liga de la media, lo cual resulta muy útil para datos categóricos.

## Regresión logística

El modelo de regresión logistica es un GLM donde la distribución de probabilidad es Bernoulli o Binomial, y la función liga es el logit (ya que relaciona a la media, que en una Bernouilli es la probabilidad con el predictor lineal). Por lo tanto la estimación de los parámetros y los contrastes de hipótesis utilizan la teoría desarrollada para los GLMs.

Estos modelos se utilizan cuando se desea conocer la relación entre

- Una __variable dependiente cualitativa__, dicotómica.
- Una o más variables explicativas independientes, llamadas __covariables__ ya sean cualitativas o cuantitativas

Por tanto, el objetivo de la regresión logística no es, como en regresión lineal, predecir el valor de la variable $Y$ a partir de una o varias variables predictoras, sino que queremos predecir la __probabilidad__ de que ocurra $Y$ conocidos los valores de las variables $X_i's$.

Recordemos que las covariables cualitativas deben transformarse en las covariables cualitativas dicotómicas ficticias necesarias (variables dummy). De manera que al hacer esta transformación cada categoría de la variable entrará en el modelo de forma individual.

## Modelo de regresión logísitica simple

Para este modelo supondremos que nuestra respuesta, $Y$, es explicada únicamente por una covariable, $X$. Asumimos que la variable independiente $Y$ está codificada como un 0 o un 1. 

Entonces, escribimos nuestro modelo como:

$$ln(\frac{p}{1-p})=\beta_{0}+\beta_{1}X$$
$$\frac{p}{1-p}=e^{\beta_{0}+\beta_{1}X}$$
$$p=e^{\beta_{0}+\beta_{1}X}-p\times e^{\beta_{0}+\beta_{1}X}$$
$$p(1+e^{\beta_{0}+\beta_{1}X})=e^{\beta_{0}+\beta_{1}X}$$
$$p=\frac{e^{\beta_{0}+\beta_{1}X}}{1+e^{\beta_{0}+\beta_{1}X}}$$

Y:

$$1-p=\frac{1}{1+e^{\beta_{0}+\beta_{1}X}}$$

Los valores posibles de estas ecuaciones varían entre 0 y 1. Un valor cercano a 0 significa que es muy improbable que $Y$ haya ocurrido, y un valor cercano a 1 significa que es muy probable que tuviese lugar.

Similar a regresión lineal los valores de los parámetros se estiman utilizando el método de máxima verosimilitud que selecciona los coeficientes que hacen más probable que los valores observados ocurran.

Para este análisis tenemos la razón de momios (odds ratio), que corresponde a la razón entre las posibilidades de respuesta.

$$OR=\frac{\frac{P(Y=1|X=1)}{1-P(Y=1|X=1)}}{\frac{P(Y=1|X=0)}{1-P(Y=1|X=0)}}$$

El valor nulo para la razón de momios es el 1. Un $OR = 1$ implica que las dos categorías comparadas son iguales.

El valor mínimo posible es 0 y el máximo teóricamente posible es infinito. 

Un OR inferior a la unidad se interpreta como que el desenlace es menos frecuente en la categoría o grupo que se ha elegido como de interés con respecto al otro grupo o categoría de referencia. Un OR = 3 se interpreta como una ventaja 3 veces superior de una de las categorías $X = 1$ relativamente a la otra categoría $X=0$.

## Modelo de regresión logísitica multiple

Análogo a lo que observamos en los modelos de regresión lineal, el modelo de regresión logística se puede facilmente generalizar de un  modelo simple a un múltiple.

$$ln(\frac{p}{1-p})=\beta_{0}+\beta_{1}X_1+\beta_{2}X_{2}+...\beta_{k}X_{k}$$
$$p=P(Y)=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}X_1+\beta_{2}X_{2}+...\beta_{k}X_{k})}}$$

De nuevo los valores posibles de estas ecuaciones varían entre 0 y 1.

El propósito del análisis es predecir la probabilidad de que un evento $Y$ ocurra para el $i-eismo$ individuo. Para dicha $i-ésima$ persona, $Y$ será 0 (la respuesta no ocurre) o 1 (la respuesta ocurre), y el valor predicho, $\mathbb{P}(Y)$, tendrá un valor 0 (no hay probabilidad de que el resultado ocurra) o 1 (el resultado seguro que ocurre).

### Regresión logística simple en R

En el siguiente ejercicio se busca analizar si los productos salen o no defectuosos de acuerdo de la temperatura de la máquina que los produce.

Los datos son los siguientes:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
require(dplyr)
```


```{r}
temperatura <-c(66,70,69,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58)
defecto <-c( 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1)
```

Con ambos vectores construimos un dataframe:

```{r}
datos <- data.frame(temperatura = temperatura, defecto = defecto)
rm(temperatura, defecto)
```

Resumen visual de los datos:

```{r}
colores <- NULL
colores[datos$defecto == 0] <- "green"
colores[datos$defecto == 1] <- "red"
plot(datos$temperatura, datos$defecto, 
     pch = 21, bg = colores, xlab = "Temperatura", 
     ylab = "Prob.defecto")
legend("bottomleft", c("No defecto", "Si defecto"), 
       pch = 21, col = c("green", "red"))
```

Creamos el modelo de regresión logística (modelo de regresión lineal generalizado y parametrizamos por familia binomial).

```{r}
reg <- glm(defecto ~ temperatura, data = datos, family = binomial)
```

Tabla resumen:

```{r}
summary(reg)
```

Creamos una nueva variable al dataframe de nuestros datos con las probabilidades de pertenencia a la clase 1 predichas por el modelo.

```{r}
datos$predict<-reg$fitted.values
```

Dibujamos la recta de probabilidad para cada una de las temperaturas:

```{r}
datos_probab <- data.frame(temperatura = seq(50, 85, 0.1))

datos.predict <- predict(reg, datos_probab, type = "response")  


plot(datos$temperatura, datos$defecto, pch = 21, bg = colores,  xlab = "Temperatura", ylab = "Prob.defecto")
legend("bottomleft", c("No defecto", "Si defecto"), 
       pch = 21, col = c("green", "red"))
lines(datos_probab$temperatura, datos.predict, 
      col = "blue", lwd = 2)
```

Bondad de Ajuste:

```{r}
dev <- reg$deviance
nullDev <- reg$null.deviance
modelChi <- nullDev - dev
modelChi

chidf <- reg$df.null - reg$df.residual
chisq.prob <- 1 - pchisq(modelChi, chidf)
chisq.prob
```

chisq.prob es el p-value de la estadística "modelChi", para valores pequeños se dice que el modelo es estadisticamente significativo.

### Ejercicio.

Se tiene la siguiente tabla donde se eligen varios niveles de ronquidos y se ponen en relación con una enfermedad cardíaca. Se toman como puntuaciones relativas de ronquidos los valores $\{0, 2, 4, 5\}$.

|     Ronquido    | Presencia de enfermedad cardiaca | Ausencia de enfermedad cardiaca |
|:---------------:|:--------------------------------:|:-------------------------------:|
|      Nunca      |                24                |               1355              |
|    Ocasional    |                35                |               603               |
| Casi cada noche |                21                |               192               |
|    Cada noche   |                30                |               224               |

Fijamos los niveles de manera ordinal:

```{r}
roncas <-  c(0, 2, 4, 5)

frecuencia <-  cbind (SI=c(24 , 35, 21, 30) , NO=c (1355 ,603 , 192 , 224))

logit.irls <- glm( frecuencia~roncas , family = binomial ( link = logit ))

summary ( logit.irls )$ coefficients
```

El modelo queda de la siguiente forma:

$$ln(\frac{p}{1-p})=-3.87+.40X$$

Como $\beta = 0.40 > 0$ entonces la probabilidad de ataque cardíaco aumenta cuando los niveles de ronquidos se incrementan.

Con un  nivel de ronquido $X=0$ obtenemos:

$$ln(\frac{p}{1-p})=-3.87$$ y

```{r}
p <- exp(-3.87)/(1+exp(-3.87))

print(p)
```

La probabilidad de tener la enfermedad es 2.04%. 

Mientras que si $X=5$ obtenemos:

$$ln(\frac{p}{1-p})=-3.87+.40*5=-1.87$$ y

```{r}
p <- exp(-1.87)/(1+exp(-1.87))

print(p)
```

La probabilidad de tener la enfermedad  aumenta a 13.35%.

- Calcular la probabilidad de presentar la enfermedad cardíaca cuando el nivel de ronquido es _Ocasional_.

- ¿Cuántas veces más probable es la ocurrencia de la enfermedad cardíaca cuando el nivel de ronquidos es _cada noche_ en comparación con _ocasional_?

### Regresión logística múltiple en R

Los datos corresponden a información sobre la respuesta a anuncios en redes sociales, se tienen datos de género, edad, salario de los individuos
asi como la información de si se realizó o no la compra del producto anunciado.

```{r}
datos <- read.csv("example_data/social_network_ads.csv", header = TRUE)
```

__Descriptivos básicos de los datos:__

```{r}
str(datos)

summary(datos)
```

__Resumen de cuantos elementos hay en cada caso para la variable de compra del producto:__

```{r}
table(datos$Purchased)
```

__Creación del modelo de regresión logística(modelo de regresión lineal generalizado y parametrizamos por binomial) aplicando a datos[,-1] estamos dejando fuera la variable "User.ID":__

```{r}
modelo <- glm(Purchased ~ ., data = datos[,-1], family = binomial)
summary(modelo)
```

__Bondad de ajuste:__

```{r}
dev <- modelo$deviance
nullDev <- modelo$null.deviance
modelChi <- nullDev - dev
modelChi
```

```{r}
chidf <- modelo$df.null - modelo$df.residual
chisq.prob <- 1 - pchisq(modelChi, chidf)
chisq.prob
```


El modelo queda de la siguiente forma:

$$ln(\frac{p}{1-p})=-12.78+.33*Genero+.237*Edad+0.000036*Salario$$

La prueba Ji-Cuadrada para la significancia del modelo da un p-value de cero, por lo tanto el modelo propuesto con las variables de género, edad y salario resulta ser significativo.

__Agregamos a los datos la columna de probabilidad de compra calculada con nuestro modelo:__

```{r}
datos <- cbind(datos, modelo$fitted.values)
```

```{r}
plot(modelo$fitted.values, col = as.factor(datos$Gender),
     main="Probabilidad de compra por género",
     ylab="probabilidad de compra")
```

```{r}
plot(modelo$fitted.values, datos$Age, col = as.factor(datos$Gender),
     main="Probabilidad de compra vs edad coloreado por edad",
     xlab="probabilidad de compra",ylab="edad")
```

```{r}
plot(modelo$fitted.values, datos$EstimatedSalary,
     main="Probabilidad de compra vs salario",
     xlab="probabilidad de compra",ylab="salario")
```

Observamos que de las 3 variables consideradas, la que parece tener más efecto en la probabilidad de compra es la edad.

Veamos ahora algunas probabilidades. Según nuestro modelo ¿cuál es la probabilidad de compra de una mujer 55 años con salario de 80,000?

```{r}
1/(1+exp(-(-12.78+.33*0+.237*55+0.000036*80000)))
```

¿Y el de una mujer de 35 años con el mismo salario?

```{r}
1/(1+exp(-(-12.78+.33*0+.237*35+0.000036*80000)))
```

Mientrás que la probabilidad de compra de una mujer de 55 años con salario de 80,000 es $96\%$ una mujer con el mismo salario pero 20 años mas joven tendrá una probabilidad de sólo $17\%$ de comprar el producto anunciado.


- Calcular la probailidad de que un hombre de 45 años con salario de 50,000 compre el producto anunciado.

- ¿Cuántas veces más probable es que un hombre de 45 años con salario de 100,000 compre el producto que un hombre de la misma edad pero con salario de 50,000?

## Regresión Multinomial

Hasta ahora hemos revisado el caso en el que la variable respuesta era dicotómica. Ahora nos centramos en el caso en el que la variable de interés tiene más de dos categorías, por ejemplo, afiliación política; resultado de un partido de fútbol; marcas de teléfonos celulares, etc. 

Por simplicidad, se ilustrará la metodología para el caso de tres categorías, ya que la generalización a más de tres es inmediata.

Supongamos que codificamos las tres categorías de la variable respuesta como 0, 1 y 2. En el caso de regresión logística, el logit es:

$$ln(\frac{p}{1-p})=ln(\frac{P[Y=1]}{P[Y=0]})$$

Ahora el modelo necesita dos funciones logit ya que tenemos tres categorías, y necesitamos decidir que categorías queremos comparar. Lo más general es utilizar $Y = 0$ como referencia y formar logits comparándola con $Y = 1$ y $Y = 2$. 

Supongamos que tenemos k variables explicativas, entonces:

$$ln(\frac{P[Y=1]}{P[Y=0]})=\beta_{10}+\beta_{11}X_1+\beta_{12}X_{2}+...\beta_{1k}X_{k}$$
$$ln(\frac{P[Y=2]}{P[Y=0]})=\beta_{20}+\beta_{21}X_1+\beta_{22}X_{2}+...\beta_{2k}X_{k}$$

Y ahora tenemos el doble de coeficientes que en el caso de regresión logística.

Las probabilidades se calcularán como:

$$P[Y=0|X]=\frac{1}{1+e^{g_1(X)}+e^{g_2(X)}}$$
$$P[Y=1|X]=\frac{e^{g_1(X)}}{1+e^{g_1(X)}+e^{g_2(X)}}$$
$$P[Y=2|X]=\frac{e^{g_2(X)}}{1+e^{g_1(X)}+e^{g_2(X)}}$$
$$g_1(X)=\beta_{10}+\beta_{11}X_1+\beta_{12}X_{2}+...\beta_{1k}X_{k}$$
$$g_2(X)=\beta_{20}+\beta_{21}X_1+\beta_{22}X_{2}+...\beta_{2k}X_{k}$$

### Regresión Multinomial en R

Datos "rh_satisfaction" corresponde a información del área de recursos humanos de una empresa que mide el nivel de satisfacción de sus empleados (en escala de 1 a 5).

También se tiene algunas características de los empleados (edad, area, salario, etc). El primer análisis a realizar que queremos analizar es la satisfacción de los empleados a través del salario y la edad. Para ello aplicaremos primero una regresión para cada variable por separado y posteriormente un modelo con ambas.

La función para realizar la regresión multinomial es __multinom__, la cual es similar a la de los comandos para regresión logística. 
Esta función está en la librería __nnet__.

__Cargamos la libreria y los datos:__

```{r,warning=FALSE}
library(nnet)

datos <- read.csv("example_data/hr_satisfaction.csv", header=TRUE)
```

Ajustemos el modelo de regresión multinominal para la satisfacción de los empleados con la variable de salario.

```{r}
multi1 <- multinom(satisfied~salary, data = datos)

print(multi1)
```

Tenemos entonces 4 ecuaciones y el análisis está tomando la categoría 1 como la base para construir los modelos _logit_ como vimos en la teoría.

Uno de los usos de este modelo puede ser el calcular probabilidades especifícas para ciertos niveles de satisfacción que sean de interés. Por ejemplo la probabilidad de que un empleado con salario de 50,000 (la media es 50,416.06) esté muy satisfecho en la empresa es $21\%$ y se calcula de la siguiente forma:

```{r}
s <- 50000

denom <- 1+exp(-0.108+.0000031*s)+exp(-0.219+.0000027*s) + exp(-0.152+.0000066*s)+exp(-0.410+.0000103*s)

exp(-0.410+.0000103*s)/denom
```

Mientras que la probabilidad de que este empleado esté completamente insatisfecho será:

```{r}
1/denom
```

Ahora ajustemos el modelo de regresión multinominal para la satisfacción de los empleados con la variable de edad.

```{r}
multi2 <- multinom(satisfied~age, data = datos)

multi2
```

La probabilidad de que un empleado de edad 40 años esté muy satisfecho en la empresa es 20\% y se calcula de la siguiente forma:

```{r}
e <- 40

denom <- 1+exp(-0.068+.0028*e)+exp(-0.438+.0088*e) + exp(0.636-.0120*e)+exp(1.178-.0281*e)

exp(1.178-.0281*e)/denom
```

Finalmente ajustamos el modelo de regresión con las variables explicativas de salario y edad. 

```{r}
multi3 <- multinom(satisfied~salary+age, data = datos)

multi3
```

Con este modelo entonces podemos calcular la probabilidad de que un empleado de edad 25 años esté muy satisfecho en la empresa es $27\%$ y se 
calcula de la siguiente forma:

```{r}
e <- 25
s <- 50000

denom <- 1+exp(-0.232+.0000031*s+0.003*e)+exp(-0.587+.0000028*s+0.009*e) + exp(0.295+.0000063*s-0.011*e)+exp(0.640+.0000098*s-0.027*e)

exp(0.640+.0000098*s-0.027*e)/denom
```

Mientras que la probabilidad de estar completamente insatisfecho será:

```{r}
1/denom
```

- Calcular la probabilidad de un empleado de 50 años con salario de 65,000 asigne un nivel de satisfacción de 3.

- Ajustar un modelo con las variables de salario y educación.

- Calcular la probabilidad de que un empleado con estudios de postgrado con salario de 65,000 este completamente satisfecho con la empresa.

- Si quisiera analizar la satisfacción por departamento ¿qué procedimiento propondría usar?

## Modelos para conteos

En muchos casos las variables respuesta son conteos, y en ocasiones estos recuentos aparecen al resumir en tablas de contingencia otras variables. 

Hay cuatro razones por las que sería erroneo utlizar un modelo de regresión normal para datos de conteo :

- Puede dar lugar a predicciones negativas.
- La varianza de la variable respuesta no es independiente de la media.
- Los errores no siguen una distribución Normal.
- Los ceros que aparecen en la variable respuesta dan problemas a la hora de transformar la variables.

Sin embargo, si la variable es de conteo pero los datos toman valores elevados, entonces si podría ser posible utilizar la distribución Normal.

El modelo más simple para cuando la variable de respuesta son recuentos es asumir que el componente aleatorio $Y$ sigue una distribución de __Poisson__. Esta distribución es unimodal y su propiedad más destacada es que la media y la varianza coinciden.

$$E(Y)=Var(Y)=\mu$$

De modo que cuando el número de recuentos es mayor en media, también tienden a tener mayor variabilidad.

La principal diferencia entre la distribución de Poisson y la Binomial, es que, aunque ambas cuentan el número de veces que ocurre algo, en la distribución de Poisson no sabemos cuántas veces no ocurrio, y en la Binomial sí lo sabemos.

Supongamos que estamos haciendo un estudio sobre cuantas larvas de insectos hay en ciertos árboles, los datos de los que disponemos corresponden al número de larvas por hoja $(Y)$. Habrá hojas que no tengan ninguna, y otras que tenga hasta 5 ó 6. Si el número medio de larvas por hoja es $\mu$, la probabilidad de observar $y_0$ larvas por hoja viene dada por la siguiente ecuación:

$$P(y_0)=\frac{e^{\mu}\mu^{y_0}}{y_0!}$$

Donde $\mu$ se puede aproximar con $\mu=np$, para $n$ grande y $p$ pequeño. Es decir, que una distribución de Poisson se obtiene a partir de una Binomial con $p$ pequeño y $n$.

Entonces el modelo GLM para los conteos se basará en modelar la relación entre la media muestral $\mu$ y las variables explicativas. 

$$\mu=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{k}X_{k}$$

Por las características de la variable (son conteos) buscamos que la parte derecha de la ecuación sólo tome valores positivos. Por esta razón habitualmente se usa el logaritmo de la media como la función liga, de modo que el modelo log-lineal se puede expresar como:

$$log(\mu) = log(E[Y])=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{k}X_{k}$$

de modo que al despejar $\mu$ obtenemos:

$$\mu = e^{\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{k}X_{k}}$$

### Sobredispersión en GLM Poisson

En una distribución de Poisson, la media y la varianza son iguales, pero en la práctica, los datos de conteo muestran mayor variabilidad de la que se espera en un modelo binomial o Poisson. En el caso de este último, es común que la varianza sea mucho mayor que la media $\mathbb{V}\left(Y\right)>>\mathbb{E}\left(Y\right)=\mu$, este fenómeno se conoce como _sobredispersión_. 

Por ejemplo, podemos suponer que cada individuo tienen igual probabilidad de padecer cierta enfermedad; no obstante, siendo más realistas, es claro que que estas probabilidades varían debido a factores genéticos, de salubridad y de localización geográfica, entre otros, propiciando mayor variabilidad sobre el número de sujetos enfermos en un periodo determinado, que los que puede predecir el modelo Poisson asociado.

Una forma de medir la sobredispersión en los datos es ajustando una distribución _quasipoisson_, la cual ajustará el  modelo con distribución Poisson pero no asumirá varianza igual a la media y calculará el parámetro de dispersión. 

### GLM Poisson R

Entre los cangrejos cacerola se sabe que cada hembra tiene un macho en su nido, pero puede tener más machos concubinos. Considerando que deseamos relacionar la variable respuesta, el número de concubinos, con las variables explicativas son: color, estado de la espina central, peso y anchura del caparazón, procederemos a ajustar un modelo lineal a los datos. 

En un primer análisis sólo consideramos la anchura del caparazón como variable explicativa.

```{r}
tabla <- read.csv ( "http://www.hofroe.net/stat557/data/crab.txt" , header =TRUE, sep = "\t" )

plot.tabla <-  aggregate (rep (1,nrow (tabla)), list (Sa= tabla$Satellite , W=tabla$Width), sum)

plot(tabla$Width,tabla$Satellite, xlab="Ancho de caparazón", ylab="Número de Concubinos", bty="L", axes = FALSE, type ="n")
axis (2, at =1:15)
axis (1, at=seq (20 , 34, 2))
text (y= plot.tabla$Sa , x= plot.tabla$W, labels = plot.tabla$x)
```

Ahora agruparemos los datos según cortes específicos en la variable de ancho de caparazón (_Width_) para poder ver visualmente el ajuste del modelo de regresión.

__Discretizamos el ancho del caparazón__

```{r}
tabla$W.fac = cut( tabla$Width , breaks =c(0, seq (23.25 , 29.25), Inf ))
```

__Calculamos el numero medio de concubinos para cada categoria según el ancho del caparazón:__

```{r}
plot.y <- aggregate ( tabla$Satellite , by= list (W= tabla$W.fac), mean )$x
```

__Determinamos la media del ancho del caparazon por categoría:__

```{r}
plot.x <- aggregate ( tabla$Width , by = list (W= tabla$W.fac ), mean )$x
```

__Representamos las medias de anchura y la media del numero de concubinos:__

```{r}
plot (x = plot.x , y = plot.y , ylab = " Numero de Concubinos ", xlab = " Anchura (cm) ", bty = "L", axes = FALSE, type = "p", pch = 16)
axis(2, at =0:5)
axis(1, at=seq (20 , 34, 2))
```

En este caso podemos observar una relación lineal entre la media del número de concubinos y la media del ancho del caparazón por categoría. 

Ahora ajustamos el modelo lineal entre el número de concubinos y el ancho del caparazón definiendo una función de distribución Poisson. 

```{r}
m1 <- glm(Satellite~Width , family = poisson , data = tabla )

summary( m1 )
```

Recordando que para medir el ajuste del modelo utilizamos la devianza, en donde la devianza nula es la desviación para el modelo que no depende de ninguna variable. Miéntras que la devianza residual es la diferencia entre la desviación del modelo que no depende de ninguna variable menos la del modelo que incluye a la variable "Width". La diferencia entre ambas se distribuirá como una distribución Ji-cuadrada con 1 grado de libertad.

__Bondad de Ajuste:__

```{r}
dev <- m1$deviance
nullDev <- m1$null.deviance
modelChi <- nullDev - dev
modelChi

chisq.prob <- 1 - pchisq(modelChi, 1)
chisq.prob
```

Se puede rechazar claramente la hipótesis nula. Por lo que concluimos que hay un aportación significativa de la variable de ancho del caparazón al modelo del número de concubinos.

El modelo queda de la siguiente forma:

$$ln(\mu)=-3.30+0.16*Width$$

Y los valores estimados por el modelo se pueden encontrar en `m1$fitted.values`. Y se puede predecir la media del números de concubinos para un valor de ancho de caparazón dado, por ejemplo, para una anchura igual a 26.3:

```{r}
predict.glm ( m1 , type = "response", newdata = data.frame( Width = 26.3 ))
```

Entonces siguiendo este modelo, decimos que los cangrejos hembra con ancho de caparazón del 26.3cm tienen en promedio 2.7 parejas.

Podriamos ajustar ahora el modelo con las variables de peso, color y estado de la espina. Se deja al lector el ejercicio de utilizando el algoritmo stepAIC encontrar el mejor modelo para el número de concubinos.

## GLM binomial negativa

Una distribución que puede usarse como alternativa a una Poisson es la  \textit{Binomial Negativa}. Dado que su varianza es más grande que su media, constituye una excelente alternativa para modelar datos de conteo sobredispersos, que son muy comunes en aplicaciones reales.

Si una variable aleatoria $Y$ se distribuye como una binomial negativa, entonces la función de probabilidad es:

$$P(y|k,\mu)=\frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)}\left( \frac{k}{\mu+k}  \right)^k\left( 1-\frac{k}{\mu+k}  \right)^y$$

con $y=0,1,2,...$ donde $k$ y $\mu$ son los parámetros de la distribución y se tiene que 

$$E(Y)=\mu$$

$$Var(Y)=\mu+\frac{\mu^2}{k}$$

El parámetro $\frac{1}{k}$ es un parámetro de dispersión, de modo que si $\frac{1}{k} \rightarrow 0$ entonces $Var(Y)\rightarrow \mu$ y la distribución binomial negativa converge a una distribución Poisson.

Por otro lado, para un valor fijo de $k$ esta distribución pertenece a la familia exponencial natural, de modo que se puede definir un modelo GLM binomial negativo. En general, se usa una función liga de tipo logaritmo.

La regresión binomial negativa se puede utilizar para datos sobredispersos de recuentos, es decir cuando la varianza condicional es mayor que la media condicional. Se puede considerar como una generalización de la regresión de Poisson, ya que tiene su misma estructura de medias y además un parámetro adicional para el modelo de sobredispersión.

Si la distribución condicional de la variable observada es más dispersa, los intervalos de confianza para la regresión binomial negativa es probable que sean más estrechos que los correspondientes a un modelo de regresión de Poisson.

### GLM Binomial Negativa en R

Usando los mismos datos de los cangrejos ahora ajustaremos un modelo con distribución binomial negativa:

```{r, message=FALSE, warning=FALSE}
require( MASS )

m2 <-  glm.nb(Satellite~Width , data = tabla )

summary( m2 )
```

En este caso el modelo queda de la siguiente forma:

$$ln(\mu)=-4.05+0.19*Width$$

Observamos que tanto la devianza como el coeficiente de Akaike son menores en comparación con el modelo que usa una distribución Poisson.

Adicionalmente habiamos visto que los datos indicaban que la varianza era poco mas de tres veces la media. Por estas razones para este ejemplo se puede decir que un modelo con distribución binomial negativa es mas adecuado.

### Ejercicio

La base de datos "productos.csv" contiene información sobre el número de productos financieros que posee cada cliente, adicional a esa información se tiene la edad, género, número de ofertas, antigüedad y el número de créditos que tiene. Deseamos obtener información respecto a la relación entre el número de productos y el resto de las variables que nos permitan incrementar la venta de productos.

- Ajustar un modelo al número de productos financieros en función de la edad del cliente. 

- Asumiendo una distribución Poisson

- Calcular con la distribución quasiPoisson el valor del parámetro de dispersión para estos datos. 

- ¿Qué puede concluir de este modelo?

- ¿Valdrá la pena cambiar a una distribución binomial negativa?

- Encontrar el mejor modelo que explique el número de productos financieros a partir de todas las variables disponibles.

- ¿Qué recomendación puede dar al área de mercadotecnia que esta buscando incrementar el número de productos por cliente?

## Exponencial

Se dice que la variable respuesta $Y$ es de tipo Exponencial cuando hemos observado el tiempo transcurrido hasta que ocurre un evento de interés como resultado de un conjunto de variables predictoras que pueden ser de tipo numérico o categórico. En este caso similar al caso de conteos también se asume que $Y$ sólo toma valores positivos y es continua.

En este caso la función liga utilizada es el recíproco, de modo que el modelo lineal se puede expresar como:

$$\frac{1}{E[Y]}=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{k}X_{k}$$

### GLM Exponencial en R

Los datos de este ejemplo corresponden a la información de un banco qué busca entender la transición de sus clientes hacia un estado de alto riesgo de crédito. Con la finalidad de entender mejor el momento en que el cliente pasa a un estado de alto riesgo crediticio por falta de pago, el banco hizo un seguimiento a una muestra representativa de sus clientes.

Deseamos identificar cuáles de las características de los clientes influyen en la transición de un estado de bajo riesgo a alto riesgo.  

```{r}
datos <- read.csv("example_data/default.csv", header=TRUE)
```

__Breve análisis exploratorio:__

```{r}
str(datos)
summary(datos)
```

__Selección de observaciones en donde si hubo falta de pago:__

```{r}
datos <- datos[datos$default==1,]
```

__Construcción de la variable del tiempo transcurrido al default:__

```{r}
datos$meses_default <- datos$duration-datos$installment_commitment

hist(datos$meses_default)
```

Lo primero será ajustar un modelo que explique el tiempo en que occure la falta de pago en función de la variable tenencia de teléfono propio.

N.B. La distribución exponencial es un caso particular de la distribución gamma y por esa razón en la función glm sólo aparece la familia _Gamma_

```{r}
m1 <- glm(meses_default ~ own_telephone, data = datos, family = Gamma)

summary(m1)
```

__Bondad de ajuste:__

```{r}
dev <- m1$deviance
nullDev <- m1$null.deviance
modelChi <- nullDev - dev
modelChi
```

```{r}
chidf <- m1$df.null - m1$df.residual
chisq.prob <- 1 - pchisq(modelChi, chidf)
chisq.prob
```

La prueba de bondad de ajuste da una probabilidad de $7\%$ que es menor al $10\%$ y puede considerarse suficiente para decir que el modelo con esta variable si aporta información estadísticamente significativa.

El modelo queda de la siguiente forma:

$$\frac{1}{E(Y)}=0.050-0.009*TeléfonoPropio$$

Con este modelo podriamos calcular, en promedio, en cuantos meses de iniciado el crédito se presentará el no pago en función de si el teléfono es propio o no.  

```{r}
table(m1$fitted.values) %>% 
  knitr::kable()
```

Recordando que el objetivo del ejercicio es entender mejor la relación entre las caracteristicas de los clientes y su riesgo crediticio nos podemos plantear las siguientes preguntas:

- ¿A qué se debe que sólo tenemos 2 estimaciones de tiempo?

- ¿Con estos datos un modelo en donde la variable explicativa es la edad, es mejor? 

- ¿Cómo podemos mejorar el modelo y/o las estimaciones de los tiempo de falta de pago?

## Gamma

Como se mencionó en la sección anterior las distribución exponencial es un caso particular de la distribución gamma. En general cuando la variable de respuesta es de tipo numérico, pero sólo puede tomar valores positivos de forma asimétrica, es decir, se encuentra concentrada en un conjunto de valores y su frecuencia disminuye cuando aumenta el valor de la respuesta, se dice que la variable se distribuye gamma.  

La distribución gamma sólo está definida para valores mayores a cero, por lo que si la variable de respuesta $Y$ toma valores negativos o cero, para poder utilizar esta distribución será necesario realizar una transformación de los datos, sumando una constante lo suficientemente grande que haga todas las obervaciones positivas.

Análogo al caso exponencial, la función liga utilizada es el recíproco, de modo que el modelo lineal se expresa como:

$$\frac{1}{E[Y]}=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{k}X_{k}$$

### GLM Gamma en R

La base de datos que utilizaremos para este ejemplo, corresponde a las reclamaciones recibidas por cierta aseguradora para su producto de seguro de automóviles. Los siniestros corresponden al primer trimestre de 2015 y se registraron además del monto total reclamado, características del asegurado asi como del siniestro.

La aseguradora busca modelar el monto total reclamado en función de las otras variables medidas para con ello mejorar su cálculo de primas e identificación de grupos que deberian tener sobreprima.

Cargamos los datos y realizamos un breve análisis exploratorio de los datos. 

```{r,message=FALSE}
datos <- read.csv("example_data/insurance_claims.csv", header = TRUE)

str(datos)
summary(datos)
```

__Eliminar las columnas que contienen información específica:__

```{r}
datos <- datos[,c(-3,-12)]
```

__Ajustaremos un primer modelo con la variable de género del asegurado:__

```{r}
m1 <- glm(total_claim_amount ~ insured_sex, data = datos, family = Gamma)
summary(m1)
```

__Bondad de ajuste:__

```{r}
dev <- m1$deviance
nullDev <- m1$null.deviance
modelChi <- nullDev - dev
modelChi
```

```{r}
chidf <- m1$df.null - m1$df.residual
chisq.prob <- 1 - pchisq(modelChi, chidf)
chisq.prob
```

La prueba de bondad de ajuste nos dice que el modelo es malo ya que la explicación de la varianza ganada con el modelo no es estadísticamente significativa.

Intentemos ahora con la variable de edad

```{r}
m2 <- glm(total_claim_amount ~ age, data = datos, family = Gamma)
summary(m2)
```

__Bondad de ajuste:__

```{r}
dev <- m2$deviance
nullDev <- m2$null.deviance
modelChi <- nullDev - dev
modelChi
```

```{r}
chidf <- m2$df.null - m2$df.residual
chisq.prob <- 1 - pchisq(modelChi, chidf)
chisq.prob
```

¿Es un buen modelo?, ¿Es mejor que el modelo de género?

Probar con la variable de tipo de siniestro.

```{r}
m3 <- glm( total_claim_amount ~ incident_type, data = datos, family = Gamma)

summary(m3)
```

__Bondad de ajuste:__

```{r}
dev <- m3$deviance
nullDev <- m3$null.deviance
modelChi <- nullDev - dev
modelChi
```

```{r}
chidf <- m3$df.null - m3$df.residual
chisq.prob <- 1 - pchisq(modelChi, chidf)
chisq.prob
```

¿Qué podemos decir de este modelo?, ¿Nos sirve este modelo para clasificar la sobreprima?

¿Podemos encontrar un mejor modelo?

# Modelos Lineales Generalizados (Construcción y Evaluación)

En la construcción de modelos lineales generalizados es importante tener en cuenta que NO existe un único modelo que sea válido. En la mayoría de los casos, habrá un número variable de modelos plausibles que puedan ajustarse a un conjunto determinado de datos. Parte del trabajo de construcción y evaluación del modelo es determinar cuál de todos estos modelos son adecuados, y entre todos los modelos adecuados, cuál es el que explica la mayor proporción de la varianza sujeto a la restricción de que todos los parámetros del modelo deberán ser estadísticamente significativos. En algunos casos habrá más de un modelo que ajuste igual de bien a los datos y en esos casos queda a criterio del modelador elegir uno u otro. Los pasos que hay que seguir en la construcción y evaluación de un GLM son muy similares a los de cualquier modelo estadístico. 

## Exploración de los datos.

Siempre es conveniente conocer los datos con los que se esta trabajando. Puede resultar interesante obtener gráficos que nos muestren la relación entre la variable de respuesta y cada una de las variables explicativas, gráficos de caja (box-plot) para variables categóricas, o matrices de correlación entre las variables explicativas. El objetivo de este análisis exploratorio es: 

- Buscar posibles relaciones de la variable respuesta/dependiente con la(s) variable(s) explicativa(s). 

- Considerar la necesidad de aplicar transformaciones de las variables. 

- Eliminar variables explicativas que estén altamente correlacionadas.


## Elección de la estructura de errores y función liga. 

A veces resultará fácil elegir estas propiedades del modelo basandose en las características de la variable de respuesta. Pero en otras ocasiones resultará tremendamente difícil, y será a posteriori cuando comprobemos, analizando los residuos, la idoneidad de la distribución de errores elegida. Por otro lado, puede ser una práctica recomendable el comparar modelos con distintas funciones liga para ver cuál se ajusta mejor a nuestros datos.

## Bondad de ajuste.

- Los tests de significación para los estimadores del modelo. (p-values de los estimadores)
- La cantidad de varianza explicada por el modelo. Esto en GLM se conoce como devianza.

La devianza nos da una idea de la variabilidad del los datos. Por ello, para obtener una medida de la variabilidad explicada por el modelo, hemos de comparar la devianza del modelo nulo (Null deviance) con la devianza residual (Residual deviance), esto es, una medida de cuánto de la variabilidad de la variable respuesta no es explicado por el modelo. (Prueba de la Chi-cuadrada sobre la devianza)

## Simplificación del modelo. 

El principio de parsimonia requiere que el modelo sea tan simple como sea posible. Esto significa que no debe contener parámetros o niveles de un factor que sean redundantes. La simplificación del modelo implica por tanto:

- La eliminación de las variables explicativas que no sean significativas.

- La agrupación de los niveles de factores (variables categóricas) que no difieran
entre sí. 

La simplificación del modelo tiene que tener, además, una cierta lógica para el analista y no debe incrementar de manera significativa la devianza
residual.

## Criterios de evaluación de modelos.

Podemos utilizar la reducción de la devianza como una medida del ajuste del modelo a los datos. Los tests de significación para los parámetros del modelo son también útiles para ayudarnos a simplificar el modelo. 

Un criterio comúnmente utilizado es el llamado Criterio de Información de Akaike (AIC del inglés Akaike Information Criterion).

Este índice evalúa tanto el ajuste del modelo a los datos como la complejidad del modelo. Cuanto más pequeño es el AIC mejor es el ajuste. 

El AIC es muy útil para comparar modelos similares con distintos grados de complejidad o modelos iguales (mismas variables) pero con funciones liga distintas.

## Análisis de los residuos. 

Los residuos son las diferencias entre los valores estimados por el modelo y los valores observados. Sin embargo, muchas veces se utilizan los residuos estandarizados, que tienen que seguir una distribución normal.

Conviene analizar los siguientes gráficos:

- Histograma de los residuos.

- Gráfico de residuos frente a valores estimados. 

Estos gráficos pueden indicar falta de linealidad, heterocedasticidad (varianza no constante) y valores atípicos.

- Gráficos de valores atípicos. 

Existen tests que permiten detectar valores atípicos. Los índices más comunes son el índice de Cook y el de apalancamiento o leverage.

Estos gráficos ayudan la evaluación del modelo utilizado.

En caso necesario, sería preciso volver a plantear el modelo, tal vez utilizando una estructura de errores más adecuada, otra función liga o
incluso eliminando ciertos datos que pueden estar desviando el análisis.

## Evaluación de GLMs en R

Ejemplo de meses para default crediticio usando una distribución exponencial.

```{r}
datos <- read.csv("example_data/default.csv", header = TRUE)

datos <- datos[datos$default==1,]
```

__Construcción de la variable del tiempo transcurrido al default:__

```{r}
datos$meses_default <- datos$duration-datos$installment_commitment

m1 <- glm(meses_default ~ own_telephone, data = datos, family = Gamma)
```

__Análisis de puntos influyentes.__


Distancia de Cook:

```{r}
plot(m1, 4)
```

Puntos palanca para betas:

```{r}
plot(dfbetas(m1)[,1],type="h", ylab="Beta0")
```

```{r}
plot(dfbetas(m1)[,2],type="h", ylab="Beta1")
```

Puntos palanca para los datos ajustados:

```{r}
plot(dffits(m1),type="h")
```

