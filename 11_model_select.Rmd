# (PART) Selección de modelos {-}

# Motivación

En estadística nos enfrentaremos a una número importante de modelos que pueden usarse para explicar el fenómeno presente en nuestros datos. Y además para cada uno de ellos siempre existirá la pregunta ¿qué variables deberíamos incluir?

Para dar respuesta a esta necesidad es importante contar con un esquema de selección de modelos que nos permita llegar al más adecuado de manera óptima.

Un esquema de selección debe combinar, por un lado, una estrategia de busqueda en el espacio de modelos posibles y, por otro lado, un criterio de comparación que nos permita evaluar la calidad de cada modelo.

En ocasiones será sencillo escoger al grupo de modelos candidatos sin embargo para otros problemas las psoibilidades pueden ser enormes.

Cuando el número de posibles modelos es muy grande generalmente se emplea una técnica _ambiciosa_ que parte de un modelo inicial y en cada paso se explora el espacio de modelos posibles escogiendo aquel que sea mejor de entre los _cercanos_ al último explorado.

### Ejemplo

Para un modelo lineal cuyo un espacio de inputs de tamaño _p_ hay $2^p$ posibles sub-modelos. Si _p_ fuera suficientemente pequeño podríamos listar todos los modelos sin embargo en la práctica _p_ suele ser grande.

En este caso, para llegar a un modelo adecuado suele utilizarse alguna de las siguientes dos técnicas:

- __Forward Selection:__ Se empieza con el modelo sin variables y éstas se agregan una por una escogiendo aquella que cumple cierto criterio e.g. que la variable pase la prueba de significancia o que al agregarla al modelo el _accuracy_ mejore en cierto grado.

- __Backward Selection:__ Empezamos con el modelo de todas las variables y vamos quitando aquellas menos _importantes_ de acuerdo a algún otro criterio e.g. nivel de significancia para el modelo.

## Esquemas básicos

- Akaike Information Criterion

- Bayesian Information Criterion

- Cross-validation

El esquema Akaike Information Criterion (AIC) busca maximizar la probabilidad de seleccionar el mejor modelo bajo el supuesto de que éste estuvo dentro del espacio de modelos evaluados.

Por otro lado, los esquemas Bayesian Information Criterion (BIC) y validación cruzada buscan optimizar el desempeño predictivo del modelo elegido.

### Akaike Information Criterion.

Es una aproximación asíntótica a la divergencia Kullback-Leibler entre el modelo de interés y _la verdad_. También llamada entropía relativa, se define como la esperanza del logaritmo de las diferencias entre _P_ y _Q_.



Supongamos una colección de modelos $\mathbb{M} = \{\mathcal{M_1},\dots,\mathcal{M_K}\}$ donde $\mathcal{M_k}:=\{f(y|\theta_k):\theta_k\in\Theta_k\}$.

Para cada $\mathcal{M}_k$ sea $\hat{\theta}_k$ el estimador máximo verosímil y $\hat{f}_k=f(.|\hat{\theta}_k)$.

Usando la divergencia Kullback-Leibler podemos calcular $$D(f_0||\hat{f}_k)=\int f_0\ log(f_0)-\int f_0\ log(\hat{f}_k)$$

Dado que en realidad observamos la distribución empírica podemos estimar el término negativo como $$\hat{H}_k=\frac{l_k(\hat{\theta}_k)}{n}$$

Akaike propuse el siguiente estimador: $$\hat{H}_k=\frac{l_k(\hat{\theta}_k)-dim(\Theta_k)}{n}$$

De donde se deriva el número AIC como: $$AIC(\mathcal{M}_k)=-2n \hat{H}_k$$

Un error común es pensar que el AIC solo puede usarse en modelos anidados sin embargo puede usarse entre modelos distintos siempre que la verosimilitud se calcule con los mismos datos.

### Bayesian Information Criterion.

BIC es una aproximación para la selección de modelos Bayesiana _a posteriori_, máxima, dados los datos.

Supongamos que establecemos una probabilidad _a priori_ $p_k$ para el modelo $\mathcal{M}_k$ y una _a priori_ para $\theta_k|\mathcal{M}_K$ de $\pi_k$.

Buscamos elegir el modelo con la mayor probabilidad _a posteriori_; del teorema de Bayes la log-probabilidad _a posteriori_ es: $$log(\mathbb{P[\mathcal{M}|_1,...,Y_n]})=const+log(p_k)+log(\int exp(l_k(\theta_k))\pi_k(\theta_k) d\theta_k$$

De donde derivamos que el mejor modelo se obtiene al minimizar $$BIC(\mathcal{M}_k)=-2l_k(\hat{\theta}_k)+dim(\Theta_k)log(n)$$

Comparado con el AIC, se impone una mayor penalización por cada parámetro adicional, por lo que el BIC tenderá a seleccionar modelos más simples.

### Cross-validation

Si buscamos elegir el modelo cuyo desempeño predictivo sea el mejor, lo ideal es contar con un conjunto de prueba _aislado_. En ausencia de esto podemos probar el modelo con una parte de los datos de entrenamiento.

Esto puede hacerse repetidamente escogiendo porciones distintas cada vez.

#### V-fold cross-validation

Los datos se dividen en $V$ subconjuntos del _mismo_ tamaño. En cada paso usamos $V-1$ subconjuntos para estimar los parámetros (entrenar el modelo) y probamos en el subconjunto restante. 

Repertimos $V$ ocasiones y se reporta el error promedio.

Elecciones comunes del valor $V$ son: 5, 10 y n.

Para _n_ se define leave-one-out cross validation. Donde se usan todos los datos salvo una observación y se predice para ella.

Para el caso continuo se utiliza MSE y para clasificación el número de observaciones mal clasificadas.

## Esquemas adicionales

- Bootstrap.

- Matrices de confusión.

- Information Value.

- Curvas CAP y ROC.

### Bootstrap

Dado que nuestros modelos están construidos con la información que pudimos observar y no con la información de la población, surgen las siguientes preguntas:

- ¿Hasta qué grado podemos confiar en que nuestros resultados serán _ciertos_ para toda la población?

- ¿Qué tanto podrían variar bajo distintos sesgos en la información usada?

La técnica de _bootstrapping_ trata de resolver estas preguntas y con ello evaluar la calidad de un modelo a través del resampleo de estadísticos de un modelo.

#### Ejemplo

Utilizando la técnica de _bootstrapping_ evaluaremos las variaciones en la $R^2$ de un modelo de regresión lineal.

En primer llugar debemos definir una función que extraiga la(s) métrica(s) de interés, en este caso la $R^2$:

```{r, message=FALSE, warning=FALSE}
library(boot)

r2 <- function(formula, data, index){
  d <- data[index,]
  mod <- lm(formula, data = d)
  
  return(summary(mod)$r.square)
}
```

Posteriormente usamos el paquete `boot` para aplicar la técnica y evaluar las variaciones de la $R^2$ bajo diferentes escenarios muestrales:

```{r}
results <- boot(data = mtcars, statistic = r2, R = 1000, formula = mpg~wt+disp)
```

Podemos imprimir los resultados:

```{r}
results
```

O bien, podemos graficarlos para leerlos más facilmente:

```{r}
plot(results)
```

#### Ejercicios

1. Aplique la técnica de _bootstrapping_ para evaluar variaciones en la $R^2$ de los siguientes modelos al mismo tiempo:

- mpg~data$wt+disp

- mpg~data$wt+disp+cyl

- mpg~data$wt+disp+cyl+hp

2. Aplique la técnica de _bootstrapping_ para otro estadístico que le parezca relevante.

## Information Value & WoE

Derivaron del uso de regresión logística particularmente en problemas de riesgo de crédito.

Se utilizan para medir qué _tan bien_ una variable logra _distinguir_ una respuesta binaria.

### Weight of Evidence (WoE)

Se calcula de la siguiente forma:

$$WoE_{x=i} = log(\frac{P[y=1 |x=i]}{P[y=0|x=i]})$$

### Information Value

Su cálculo se realiza de la siguiente forma:

$$IV_{x_i} = (P[Y = 1|x=i]-P[y=0|x=i])*WoE_{x_i}$$

#### Interpretación del Information Value

| IVx      | Poder predictivo |
|----------|------------------|
| <0.02    | Variable no útil |
| 0.02-0.1 | Poder débil      |
| 0.1-0.3  | Poder medio      |
| 0.3-0.5  | Poder alto       |
| 0.5      | Sospechosa       |


## Matrices de confusión (y derivados...)

- Son una forma de medir el desempeño de un algorítmo de clasificación.

- De ellas derivan diferentes medidas que nos ayudan a entender más a fondo el desempeño de nuestro algorítmo.

- En un problema de clasificación de 2 clases, la matriz se construye con la tabla cruzada entre las clases reales y las clases ajustadas/predichas

|                   | Positivo Real | Negativo Real |
|-------------------|---------------|---------------|
| Positivo predicho |       TP      |       FP      |
| Negativo predicho |       FN      |       TN      |

Las matrices de confusión se pueden extender a problemás de más de dos clases.

### Estadísticos derivados de las matrices de confusión

- $Sensitivity = \frac{TP}{TP+FN}$

También llamada tasa de verdaderos positivos, mide la proporción de positivos predichos de entre los verdaderos reales.

- $Specificity = \frac{TN}{TN+FP}$

O tasa de verdaderos negativos, mide la proporción de negativos predichos de entre los negativos reales.

- $Prevalence = \frac{TP+FN}{TP+FP+TN+FN}$

Mide cuántos valores reales hay

- $PPV = \frac{sensitivity*prevalence}{(sensitivity*prevalence)+((1-specificity)*(1-prevalence))}$

PPV: positive predicted values

- $NPV = \frac{sensitivity*(1-prevalence)}{((1-sensitivity)*prevalence)+((specificity)*(1-prevalence))}$

NPV: negative predicted values

- $Detection \ rate = \frac{TP}{TP+FP+TN+FN}$

Mide cuántos verdaderos positivos esta detectando el modelo

- $Detection \ prevalence = \frac{TP+FP}{TP+FP+TN+FN}$

Mide cuántos positivos predichos tiene el modelo

- $Balanced \ accuracy = \frac{sensitivity+specificity}{2}$

- $Precision = \frac{TP}{TP+FP}$

Proporción de verdaderos positivos de entre los positivos predichos

- $Recall = \frac{TP}{TP+FN}$

Proporción de verdaderos positivos de entre los positivos reales

- $F-beta = \frac{(1+beta^2)*precision*recall}{(beta^2*precision)+recall}$

## Curvas AUC-ROC

Muchos algoritmos de clasificación no generan directamente el vector de clases predichas sino que primero obtienen el vector de probabilidades de pertenencia a cada clase.

Dado un umbral o punto de corte para el vector de probabilidades se puede generar entonces un vector de clases asociado a ese vector.

Es claro que el valor de ese umbral afectará directamente al _accuracy_ de clasificación del modelo.

Las curvas AUC-ROC nos permiten medir el desempeño del algorítmo bajo distintos umbrales.

ROC es una curva probabilística mientras que AUC es una medida de _separación._

### Construcción de una curva AUC-ROC

Para generar una curva de ROC debemos graficar la métrica __1-specificity__ vs __sensitivity__.

Para obtener el valor AUC debemos calcular el área bajo la curva ROC.