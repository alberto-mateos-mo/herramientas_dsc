# (PART) Reducción de dimensiones {-}

# Análisis de Componentes Principales

## Introducción

Supongamos que tenemos un conjunto de datos en $\mathbb{R}^p$ y que queremos visualizarlos. Si $p$ es muy grande no podremos visulizar todo el conjunto con un solo gráfico, una manera de visualizar los datos es hacer gráficos de dsipersión de dos dimensiones.

Sin embargo la cantidad de gráficos de dos dimensiones que podemos hacer son $p(1-p)/2$. Lo cual vuelve la tarea compleja conforme el valor de $p$ aumenta.

Lo que nos gustaría es poder encontrar una representación de los datos con menos dimensiones pero que conserva la mayor cantidad de información posible.

El análisis de componentes principales es una herramienta que nos permitirá lograr eso, enocntrar representaciones en menos dimensiones que conserven la mayor cantidad de varianza.

## Los componentes principales

Los componentes principales de un conjunto de datos en $\mathbb{R}^p$ son una serie de combinaciones lineales de rango $q \leq p$ que, colectivamente, explican la mayoría de la varianza original.

Si denotamos las observaciones por $x_1, x_2, \dots, x_n$, sea $$f(\lambda) = \mu+V_q\lambda$$ el modelo lineal de rango $q$ que las representa; esta ecuación es la representación paramétrica de un hiperplano afín de rango $q$.

Si resolvemos el problema que nos permita encontrar el plano afín que minimice el _error de reconstrucción_: $$min_{\mu,\lambda_i,V_q} \sum_{i=1}^N ||x_i-\mu -V_q\lamda_i||^2$$ obtenemos 

$$\hat{\mu}=\bar{x}$$

$$\hat{\lambda}_i=V_q^T(x_i-\bar{x})$$

Con lo cual debemos encontrar la matriz ortogonal $V_q$ tal que:

$$min_{V_q}\sum_{i=1}^N||x_i-\bar{x}-V_q V_q^T(x_i-\bar{x})||^2$$

Por facilidad, asumimos que $\bar{x}=0$. En caso contrario centramos las variables para tener media cero.

Entoncesla matriz $H_q = V_q V_q^T$ es una matriz de proyección que mapea cada punto $x_i$ en su reconstrucción $H_q x_i$  que es la proyección ortogonal de $x_i$ en el subespacio generado por las columnas de $V_q$

La solución puede ser expresada como:

\begin{equation} 
  X = UDV^T
  
  (\#eq:svd)
\end{equation}

esto es, la descomposicion en valores singulares de $X$ donde:

- $U$ es una matriz ortogonal de $N\times p$ cuyas columnas llamamos _valores singulares izquierdos_

- $V$ es una matriz ortogonal de $p\times p$ cuyas columnas llamamos _valores singulares derechos_

- $D$ es una matriz diagonal de $p \times p$ cuyos elementos cumplen $d_1\geq d_2\geq \dots \geq d_p\geq 0$ llamados valores singulares

A las columnas de $UD$ las llamamos componentes principales de $X$

El primer componente principal está dado por la combinación lineal normalizada de las columnas de los datos originales cuya varianza es máxima:

$$Z_1 = \phi_{11}X_1+\phi_{21}X_2+ \dots+\phi_{p1}X_p$$
N.B. $\sum_{j=1}^p\phi_{j1}^2=1$

A los valores $\phi_{j1}$ los llamamos _pesos_ del primer componente principal. Éstos tienen la restricción de que su suma debe ser igual a uno para evitar que el primer componente tenga varianza arbitraria.

## Interpretación geométrica

Para el primer componente principal, $\phi_1$ define la dirección en el espacio para la cual los datos presentan mayor variación.

El segundo componente principal es aquella combinación lineal ortogonal al primer componente que tenga máxima varianzay aspi sucesivamente con el resto de componentes.

Los scores de los componentes principales son las proyecciones de los datos en las direcciones descritas por $\phi_i$

## Consideraciones

### Escalamiento de variables

Los resultados del análisis de componentes principales serán distintos si las variables no se centran (media cero) e incluso serán distintos si éstas se escalan individualmente.

Las diferencias en los resultados están ligadas directamente con la escala en la que estén medidas las diferentes variables del conjunto de datos.

Consideremos por ejemplo los datos `USArrests`, en este conjunto las variables `Murder`, `Rape` y `Assault` están medidas como ocurrencia por cada 100,000 habitantes y `UrbanPop` como porcentaje de población, en este sentido, si analizamos las varianzas:

```{r}
lapply(USArrests, var)
```

veremos que `Assault` tiene la varianza más grande lo cual causará si no escalaramos los datos, que el primer componente tengo un peso muy grande para esta variable.

Para evitar que los componentes principales dependan de la escala o de la elección de algún factor de escala deberemos transformar las variables para que tengan desviación estandar unitaria antes de aplicar el algoritmo de componentes principales.

N.B. Si las variables fueron medidas en la misma escala entonces no es necesario aplicar ninguna transformación.


### Varianza explicada por los componentes principales

La varianza total del conjunto de datos está dada por $$\sum_{j=1}^pVar(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2$$

La varianza explicada por el m-ésimo componente principal es $$\frac{1}{n}\sum_{i=1}^n(\sum_{j=1}^p\phi_{jm}x_{ij})^2$$

Y la proporción de varianza explicada por el m-ésimo componente es $$\frac{\sum_{i=1}^n(\sum_{j=1}^p\phi_{jm}x_{ij})^2}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2}$$

### Número de componentes a usar

De manera general, un conjunto de datos n-dimensional tiene $min(n-1,p)$ componentes principales distintos. Sin embargo, dado que no típicamente usaremos este análisis para reducir la dimensión de los datos no estamos interesados en usarlos todos.

De hecho lo que buscamos es el menor número de componentes que nos permitan capturar una _buena_ cantidad de información.

Para encontrar este número de componentes no existe una respuesta única. Comúnmente usaremos como ayuda el _screeplot_ o gráfica de codo de la varianza del m-ésimo componente principal y escogeremos la cantidad que nos permita capturar ya sea cierta cantidad de varianza e.g. el 70% de ella o bien aquel número de componentes a partir del cual el incremento de varianza por cada componente adicional sea marginal.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(palmerpenguins)
library(ggplot2)
library(dplyr)
library(tidyr)

penguins_data <- penguins %>%
  dplyr::select(bill_length_mm:year, -sex)

pca_obj <- prcomp(drop_na(penguins_data), scale. = TRUE)

var_explained_df <- data.frame(PC = paste0("PC",1:5),
                               var_explained = (pca_obj$sdev)^2/sum((pca_obj$sdev)^2))

var_explained_df %>%
  ggplot(aes(x=PC,y=var_explained, group=1))+
  geom_point(size=4)+
  geom_line()+
  labs(title="Scree plot: PCA on scaled data")+
  unam.theme::theme_unam()
```

## PCA en R

Usaremos datos de números escritos a mano para reducir las dimensiones.

```{r}
digitos <- read.csv("example_data/train.csv")
```

El conjunto de datos tiene información sobre 784 pixeles para 1000 números.

```{r}
un_numero <- matrix(as.matrix(digitos[315,]), ncol = 28, byrow = TRUE)

heatmap(x = un_numero, Colv = NA, Rowv = NA, revC = T, scale = "none", col = grey.colors(1000))
```

Cada número consta de 784 pixels, la idea es reducir la dimensión de los número para representarlo con una menor cantidad de ellos.

```{r, eval=FALSE}
componentes <- prcomp(digitos, center = TRUE, scale. = F, rank. = 100, retx = TRUE)
```

```{r, echo=FALSE}
componentes <- prcomp(digitos[,sample(1:784, 784)], center = TRUE, scale. = F, rank. = 100, retx = TRUE)
```

```{r}
summary(componentes)
```

```{r}
cumulative_prop <- data.frame(pc = 1:100, cumvar = cumsum(componentes$sdev^2/sum(componentes$sdev^2))[1:100])

ggplot(cumpro, aes(pc, cumvar))+
  geom_line()+
  geom_point()+
  labs(x = "Componente", y = "Varianza acumulada")+
  theme_unam()
```

El gráfico de varianza acumulada nos sugiere que 100 componentes son más que suficientes para representar las 784 dimensiones originales.

```{r}
nueva_rep <- matrix(compo$x[315,], nrow = 10, byrow = TRUE)

heatmap(nueva_rep, Colv = NA, Rowv = NA, revC = T, scale = "none", col = grey.colors(2))
```

Naturalmente, para nosotros, esta representación tiene poco (o ningún) sentido pero es una buena representación de la información original que puede usarse como _input_ para otros modelos e.g. predictivos.

# Análisis factorial

## Variables Latentes

En muchas ocasiones los datos multivariados son vistos como mediciones indirectas provenientes de una fuente subyacente que típicamente no puede ser medida directamente.

Por ejemplo:

- Tests psicológicos y pedagógicos se usan como una forma de medir la inteligencia u otras habilidades mentales.

- Los electroencefalogramas miden la actividad cerebral de forma indirecta usando señales electromagnéticas grabadas por sensores colocados en diferentes partes de la cabeza.

- Los precios de las acciones en el mercado financiero reflejan factores no medidos como la confianza en el mercado o influencias externas que serían dificiles de medir o identificar.


El análisis factorial es una herramienta que permite identificar las fuentes latentes subyacentes a un conjunto de datos.

## Solución factorial

La descomposición en valores singulares \@ref(eq:svd) tiene una representación en variables latentes.

Escribiendo $S = \sqrt{N}U$, $A^T = \frac{DV^T}{\sqrt{N}}$, tenemos $$X = SA^T$$

Y por tanto cada columna de $X$ es una combinación lineal de las columnas de $S$.

Dado que $U$ es una matriz ortogonal y asumiendo que las columnas de $X$ (y de $U$) tienen media cero tenemos que las columnas de $S$ también tienen media cero, no están correlacionadas y tienen varianza unitaria.

En términos de variables aleatorias podemos interpretar a los componentes principales como una estimación del modelo de variables latentes:

$$X_1 = a_{11}S_1+a_{12}S_2+\dots +a_{1p}S_p$$
$$X_2 = a_{21}S_1+a_{22}S_2+\dots +a_{2p}S_p$$
$$\vdots$$
$$X_p = a_{p1}S_1+a_{p2}S_2+\dots +a_{pp}S_p$$

o simplemente $X = AS$.

Las variables $X_j$, que están correlacionadas son representadas como una expasión lineal de las variables $S_l$, no correlacionadas y de varianza unitaria.

Sin embargo, esto no es demasiado satisfactorio puesto que dada cualquier matriz ortogonal de $p\times p$ en $\mathbb{R}$ podemos escribir:

$$X = AS$$
$$X = AR^TRS$$
$$X = A^*S^*$$

Y $Cov(S^*) = R Cov(S)R^T = I$

Por lo tanto ha una infinidad de tales descomposiciones haciendo imposible identificar las variables latentes como fuentes subyacentes únicas.

El modelo clásico de factores resuelve este problema escribiendo el modelo como $X = AS+\epsilon$. Donde $A$ es la matriz de pesos de los factores, $S$ un vector de $q$ variables latentes y $\epsilon_j$ son errores no correlacionados de media cero.


Comúnmente tanto $S_j$ y $\epsilon_j$ se modelan como variables aleatorias normales y el modelo factoria se ajusta por máxima verosimilitud.

Los parámetros dependen de la matriz de covarianzas

$$\Sigma = AA^T+D_{\epsilon}$$

Donde $D_{\epsilon} = diag[Var(\epsilon_1),\dots , Var(\epsilon_p)]$.

Y dado que $S_j$ son variables normales no correlacionadas entonces son variables aleatorias independientes.

La presencia de los errores $\epsilon_j$ hace que el análisis de factores pueda verse como un modelo de la estructura de correlación de $X_j$ en lugar de la estructura de covarianza (la usada en el análisis de componentes principales).

## Rotaciones de factores

Como se menciona anteriormente las soluciones al modelo factorial podrían no ser únicas para encontrar la unicidad se introducen restricciones meramente arbitrarias.

Diferentes tipos de restricciones derivan en soluciones distintas al modelo.

Al proceso de cambiar de una solución a otra se le llama _rotación_ y proviene de la representación geométrica del procedimiento.

La razón principal para rotar una solución es dar claridad a las cargas factoriales. Si la solución inicial es confusa una rotación puede proporcionar una estructura más fácil de interpretar.

### Rotaciones ortogonales

Uno de los patrones de cargas factoriales más usuales y de hecho más deseables es la llamada _estructura simple de pesos factoriales_. 

Se dice que los pesps factoriales presentan una estructura simple si cada variable tiene un gran peso en un solo factor, con pesos cercanos a cero en el resto de los factores. 

Una de las rotaciones que procura generar una estructura de pesos simple son las rotaciones ortogonales (los nuevos ejes después de la rotación siguen siendo ortogonales).

### Rotaciones oblicuas

Contrario a las rotaciones ortogonales, las rotaciones oblicuas permiten relajar la restricción de ortogonalidad con el fin de ganar simplicidad en la interpretación de los factores. 

Con este método los factores resultan correlacionados, aunque generalmente esta correlación es pequeña. 

El uso de rotaciones oblicuas se justifica porque en muchos contextos es lógico suponer que los factores están correlacionados.

## Análisis factorial en R

Usando datos de gastos en hogares buscaremos identificar, a través de un análisis factorial, grupos de gastos _generales_.

```{r}
library(psych)
library(GPArotation)

gastos <- read.csv("example_data/pob2.csv", sep = "\t")
```

```{r}
str(gastos)
```

Trabajaremos solamente con la información relacionada a gastos.

```{r}
gastos <- gastos[,19:59]
```

```{r, warning=FALSE}
factores <- fa(gastos, nfactors = 4, rotate = "oblimin")
```

```{r}
autoplot(abs(factores$loadings[,1:4]))
```

Ejercicios:

1. ¿Considera que los factores son buenos?

2. ¿Qué problema encuentra en los datos que puedan causar una mala optimización en la búsqueda de los factores?

3. Analize detalladamente los datos y proponga alguna solución si es que considera que hay problemas en los datos.

4. Vuelva aplicar el análisis factorial.

5. ¿Cómo interpretaría los factores?