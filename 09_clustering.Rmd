# (PART) Clusterización {-}

# ¿Qué son los métodos de clusterización?

Son un serie de técnicas que permiten encontrar subgrupos/clústers dentro de un conjunto de datos

Estos subgrupos tienen la característica de que las observaciones dentro de cada uno _similares_ mientras que los subgrupos son distintos entre ellos.

Estas técnicas nos permitirán indentificar la estructura subyacente a los datos.

Por ejemplo, supongamos que un club de precios cuenta con información sobre las compras de sus clientes, podemos estar interesados en segmentar a los clientes de acuerdo a sus patrones de consumo con la intención de generar estrategias de marketing específicas para cada segmento. El análisis de clústers nos sería útil para identificar estos grupos de consumo.

Las técnicas de clusterización son muy usadas y existen una gran variedad de ellas; en esta sección nos enfocaremos en las dos más conocidas __k-medias__ y __clusterización jerárquica__.

# Conceptos teóricos

```{definition}
$C_1, C_2, \dots,C_k$ es una __partición__ de $C$ de tamaño $k$ si $\cup_{i=1}^k C_i = C$, y $C_i \cap C_j = \emptyset$ si $i \neq j$. Una clusterización de $X$ es una partición de $X$.
```

```{definition}
Una medida de disimilaridad en un conjunto finito $X$ es una función $d: X\times X \rightarrow \mathbb{R}$ simétrica.
```

En particular una métrica es medida de disimilaridad pero una medida de disimilaridad no es necesariamente una métrica.

## Medidas de disimilaridad

### Datos numéricos

Si $X \subset \mathbb{R}^p$ es un conjunto de $N$ datos, entonces tenemos las siguientes medidas de disimilaridad:

- $d_{euc}: X\times X \rightarrow \mathbb{R}^+\cup\{0\}$; $d_e(x_i,x_j)=||x_i-xj||$

- $d_{abs}: X\times X \rightarrow \mathbb{R}^+\cup\{0\}$; $d_a(x_i,x_j)=\sum_{l=1}^p|x_{il}-x_{jl}|$

- $d_{cor}: X\times X \rightarrow \mathbb{R}$; $d_c(x_i,x_j)=\rho(x_i,x_j)$

### Datos ordinales

Si $X=\{x_1,\dots,x_N\}$ representa un conjunto de $N$ datos univariados ordinales, podemos definir la métrica de valor absoluto en $X$ guiándonos por el ordenamiento de los datos, entonces podemos definir $f:\ Rango(X)\rightarrow \mathbb{N}$ de tal manera que $f$ preserve el orden y definir $$d(x_i,x_j) = |f(x_i)-f(x_j)|$$

### Datos categóricos

Si $X$ representa un conjunto de datos categóricos podemos definir como medida de disimilaridad a la __delta de Kronocker__

$$d(x_i,x_j) = \left\{\begin{matrix}
0 & x_i=x_j\\ 
1 & e.o.c.
\end{matrix}\right.$$

Cuando $X_{N\times p}$ representa un conjunto de datos arbitrario podemos definir

$$d(x_i,x_j) = \sum_{l=1}^p\alpha_l d_l(x_{il},x_{jl})$$
donde $0\leq \alpha_l$ y $sum_{l=1}^p\alpha_l=1$, es decir, $d$ es una combinación lineal convexa de las $d_l$

# K-medias

Es un algorítmo iterativo descendiente cuyo uso está limitado a conjuntos de datos numéricos.

La idea detrás del algorítmo es encontrar clústers cuya variación interna sea _tan pequeña como sea posible_.

La variación interna de un clúster $C_k$ es una medida $W(C_k)$ de qué tanto difieren las observaciones pertenecientes a un clúster, por lo que buscamos resolver el problema:

\begin{equation} 
  \min_{C_1,\dots,C_k}\{\sum_{k=1}^K W(C_k)\}
  
  (\#eq:kmeans)
\end{equation}

Comúnmnete se utiliza la distancia euclideana como medida de disimilaridad.

No se ha probado que exista algún algorítmo que resuelva el problema \@ref(eq:kmeans), sin embargo
existen heurísticas que intentan resolverlo parcialmente.

## Algorítmo k-medias

1. Tomar una partición de tamaño $k$ de manera aleatoria.

2. Para $x\in X$, se asigna a $C_j$, donde $$||x-\bar{x}_{C_j}|| = inf||x-\bar{x}_{C_l}||$$

Repetir el paso 2 hasta que ningún $C_j$ cambie.

```{lemma, name="Suma de variaciones internas"}
En el algorítmo k-medias, el paso 2 nunca incrementa la suma de variaciones internas
```

Es decir, el algoritmo k-medias es un proceso iterativo que divide un conjunto de datos en $K$ grupos excluyentes. Este método es usado extensamente en la literatura, porque es un método sencillo de implementar que utiliza centroides (prototipos) para la representación de los clusters. 

La calidad de los clusters es mediada por el criterio de variación interna.

Este algoritmo produce clusters compactos (de poca dispersión en el interior), pero no toma en consideración la distancia entre clusters, y además el uso de la norma dos hace que el algoritmo sea sensible en presencia de valores atípicos.

## Variantes

En términos generales las variantes del algoritmo k-medias difieren en el momento del algoritmo en que se hace la asignación de clusters, entre las variantes más utilizadas en la práctica están las siguientes:

- __Algoritmo de Forgy-Lloyd__: los centroides son recalculados después de que todos los individuos fueron asignados. El algoritmo realiza iteraciones hasta que se obtiene la convergencia.

- __Algoritmo de McQueen__: Los centroides son recalculados inmediatamente después de cada asignación, y al final de todas las asignaciones. El algoritmo da un único barrido a los datos.

- __Algoritmo de Hartigan__: Se selecciona un elemento de cada partición, después se recalculan los centroides sin considerar los elementos seleccionados. Por último se asignan los elementos seleccionados
al cluster cuyo centroide sea el más cercano.

## Desventajas

- Sensibilidad a la partición inicial: como el algoritmo consiste de una búsqueda local,  este es muy sensible a la selección inicial de clusters.

- Falta de robustéz: esta desventaje se hereda del hecho de que la media y varianza son sensibles ante valores atípicos.

- Número de clusters desconocido: el algoritmo no proporciona información alguna del número de clusters.

- No es adecuado para variables nominales: no hay una definición de media muestral para tales variables.

## K-medias en R

Usaremos los datos de personajes de marvel para tratar de identificar grupos subyacentes de ellos.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
require(dplyr)
require(ggfortify)
require(factoextra)
```


```{r}
marvel <- read.csv("example_data/charcters_stats.csv")

marvel <- marvel[-419,]

nombres <- marvel$Name

marvel <- marvel %>% 
  dplyr::select(-Total, -Alignment, -Name)

rownames(marvel) <- nombres

grupos <- kmeans(marvel, centers = 2, algorithm = "Hartigan-Wong", nstart = 1)
```

__Variación interna__

```{r}
grupos$withinss
```
```{r}
grupos$tot.withinss
```
__Visualización de los grupos__

```{r}
fviz_cluster(grupos, marvel)+
  scale_color_unam()+
  theme_unam()
```

__Distinta cantidad de grupos__

```{r, animation.hook='gifski', echo=FALSE}
for (i in 1:10) {
  grupos <- kmeans(marvel, centers = i, algorithm = "Hartigan-Wong", nstart = 1)

  g <- fviz_cluster(grupos, marvel)+
  labs(subtitle = paste("Variación interna:", round(grupos$tot.withinss,1)))+
  scale_color_unam()+
  theme_unam()
  
  plot(g)
}
```

__Ejercicios__

1. Diseñe resultados que le permitan hacer evidentes las características de cada grupo.

2. ¿Qué cantidad de grupos considera más adecuados?, ¿por qué?

3. Analize los resultados de los grupos con el resto de variables y comente.

# Clusterización jerárquica

A diferencia del método k-medias, la clústerización jerárquica no requiere de una previa especificación del número de clústers a encontrar ni de la configuración inicial de ellos.

Como el nombre lo dice, estos clusterización producen representaciones jerárquicas en las cuales los clústers de cada nivel jerárquico son creados a través de la unión de clústers en el nivel inmediato siguiente.

Existen dos paradigmas dentro de la clusterización jerárquica: el aglomerativo (bottom-up) y el divisivo (top-down); en adelante nos centraremos en el paradigma aglomerativo.

## Paradígma aglomerativo

En este paradigma la construcción de la clusterización empieza en el nivel más bajo y en cada nivel se unen pares de clústers para formar uno nuevo; es decir, en el nivel más bajo todos los elementos del conjunto forman un clúster cada uno; en el nivel más alto todos los elementos forman un único clúster.

La elección del par de clústers que serán unidos en uno nuevo se hace considerando la disimilaridad más pequeña entre los distintos clústers.

Cada nivel de la jerarquía representa una agrupación particular de los datos y depende del analista decidir qué nivel corresponde a una agrupación _natural_.

## Representación gráfica

Esta clusterización binaria puede ser fácilmente representada por un árbol binario donde los nodos del árbol representan los distintos grupos.

El nodo raíz representa todo el conjunto de datos, los $N$ nodos terminales representan cada observación individual y cada nodo no terminal tiene dos nodos hijos que representan los clústers que fueron unidos.

El paradigma aglomertivo poseé una propiedad monótona sobre la disimilaridad: la disimilaridad entre clústers es monótona creciente en cada nivel. 

Esto permite que el árbol resultante pueda graficarse de forma que la altura de cada nodo sea proporcional al valor de la disimilaridad entre sus nodos hijos.

A esta representación gráfica se le conoce como _dendrograma_.

Los dendrogramas proveen una excelente representación interpretativa de los resultados y es en gran parte una de las razones que han hecho popular a este método de agrupación.

## El algorítmo aglomerativo de clusterización

Naturalmente, para construir la clusterización necesitamos usar una medida de disimilaridad para las observaciones, sin embargo, una vez que las observaciones están agrupadas es necesario extender la defición de medidas de disimilaridad a grupos de observaciones.

Esta extensión se define como _linkage_.

Los cuatro tipos de _linkage_ más comunes son:

- Complete (máxima disimilaridad intraclúster): Se calculan todas las disimilaridades entre pares de observaciones entre dos clústers y conservamos la mayor de ellas.

- Single (mínima disimilaridad intraclúster): Se calculan todas las disimilaridades entre pares de observaciones entre dos clústers y conservamos la menos de ellas.

- Average (disimilaridad intraclúster promedio): Se calculan todas las disimilaridades entre pares de observaciones entre dos clústers y conservamos el promedio de ellas.

- Centroid: Se calcula la disimilaridad entre el centroide de cada clúster.

Aún con la definición de _linkage_ es necesario elegir una medida de disimilaridad particular y de hecho esta elección tiene una efecto muy importante en los resultados de la clusterización.

Como siempre no existe respuesta única a la _mejor_ elección de la medida de disimilaridad pues esta depende de la naturaleza de los datos y de la pregunta que se busca responder.

## Clusterización jerárquica en R

Usaremos nuevamente los datos de personajes de marvel para aplicarles clusterización jerárquica.

```{r}
x <- dist(marvel)
  
jerarquico <- hclust(x, method = "complete")
```

```{r}
fviz_dend(jerarquico, k = 3)
```

__Ejercicios__

1. Considera que la clusterización jerárquica realizada es buena, ¿cuántos grupos considera adecuados?.

2. ¿Haría alguna modificación a los datos o al algorítmo de clústerización?

3. Analize algunos personajes de su interés para identificar con qué otros personajes son _cercanos_.

4. Realice la clusterización con otros métodos o con transformaciones a los datos y compare.

5. Compare los resultados con la clusterización kmedias usando el mismo número de grupos y discuta al respecto.