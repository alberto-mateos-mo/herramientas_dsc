<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 8 ¿Qué es una SVM? | Herramientas Estadísticas para Ciencia de Datos</title>
  <meta name="description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 8 ¿Qué es una SVM? | Herramientas Estadísticas para Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="github-repo" content="alberto-mateos-mo/seminario_est_libro" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 8 ¿Qué es una SVM? | Herramientas Estadísticas para Ciencia de Datos" />
  
  <meta name="twitter:description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  

<meta name="author" content="Sofía Villers Gómez" />
<meta name="author" content="David Alberto Mateos Montes de Oca" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="qué-es-una-red-neuronal.html"/>
<link rel="next" href="antecedentes.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Herramientas Estadísticas para Ciencia de Datos</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#objetivos"><i class="fa fa-check"></i><b>0.1</b> Objetivos</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estructura"><i class="fa fa-check"></i><b>0.2</b> Estructura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#detalles-técnicos"><i class="fa fa-check"></i><b>0.3</b> Detalles técnicos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licencia"><i class="fa fa-check"></i>Licencia</a></li>
</ul></li>
<li class="part"><span><b>I El argot de ciencia de datos</b></span></li>
<li class="chapter" data-level="1" data-path="notación.html"><a href="notación.html"><i class="fa fa-check"></i><b>1</b> Notación</a></li>
<li class="chapter" data-level="2" data-path="glosario-dscml-estadística.html"><a href="glosario-dscml-estadística.html"><i class="fa fa-check"></i><b>2</b> Glosario DSc/ML - Estadística</a></li>
<li class="chapter" data-level="3" data-path="entrenamiento-de-modelos.html"><a href="entrenamiento-de-modelos.html"><i class="fa fa-check"></i><b>3</b> Entrenamiento de modelos</a></li>
<li class="part"><span><b>II Modelos Lineales</b></span></li>
<li class="chapter" data-level="4" data-path="regresión-lineal.html"><a href="regresión-lineal.html"><i class="fa fa-check"></i><b>4</b> Regresión Lineal</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#un-poco-de-história"><i class="fa fa-check"></i><b>4.1</b> Un poco de história</a></li>
<li class="chapter" data-level="4.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#objetivos-del-análisis-de-regresión"><i class="fa fa-check"></i><b>4.2</b> Objetivos del análisis de regresión</a></li>
<li class="chapter" data-level="4.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#el-algorítmo-de-regresión-lineal"><i class="fa fa-check"></i><b>4.3</b> El algorítmo de regresión lineal</a></li>
<li class="chapter" data-level="4.4" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-simple"><i class="fa fa-check"></i><b>4.4</b> Regresión lineal simple</a></li>
<li class="chapter" data-level="4.5" data-path="regresión-lineal.html"><a href="regresión-lineal.html#solución-al-problema-de-regresión-lineal-simple"><i class="fa fa-check"></i><b>4.5</b> Solución al problema de regresión lineal simple</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#mínimos-cuadrados-ordinarios"><i class="fa fa-check"></i><b>4.5.1</b> Mínimos cuadrados ordinarios</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-múltiple"><i class="fa fa-check"></i><b>4.6</b> Regresión lineal múltiple</a></li>
<li class="chapter" data-level="4.7" data-path="regresión-lineal.html"><a href="regresión-lineal.html#solución-al-problema-de-regresión-lineal-múltiple."><i class="fa fa-check"></i><b>4.7</b> Solución al problema de regresión lineal múltiple.</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#ecuaciones-normales"><i class="fa fa-check"></i><b>4.7.1</b> Ecuaciones normales</a></li>
<li class="chapter" data-level="4.7.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#evaluación-de-supuestos"><i class="fa fa-check"></i><b>4.7.2</b> Evaluación de supuestos</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="regresión-lineal.html"><a href="regresión-lineal.html#aplicación-en-r"><i class="fa fa-check"></i><b>4.8</b> Aplicación en R</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html"><i class="fa fa-check"></i><b>5</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística"><i class="fa fa-check"></i><b>5.1</b> Regresión logística</a></li>
<li class="chapter" data-level="5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-de-regresión-logísitica-simple"><i class="fa fa-check"></i><b>5.2</b> Modelo de regresión logísitica simple</a></li>
<li class="chapter" data-level="5.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-de-regresión-logísitica-multiple"><i class="fa fa-check"></i><b>5.3</b> Modelo de regresión logísitica multiple</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística-simple-en-r"><i class="fa fa-check"></i><b>5.3.1</b> Regresión logística simple en R</a></li>
<li class="chapter" data-level="5.3.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejercicio."><i class="fa fa-check"></i><b>5.3.2</b> Ejercicio.</a></li>
<li class="chapter" data-level="5.3.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística-múltiple-en-r"><i class="fa fa-check"></i><b>5.3.3</b> Regresión logística múltiple en R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-multinomial"><i class="fa fa-check"></i><b>5.4</b> Regresión Multinomial</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-multinomial-en-r"><i class="fa fa-check"></i><b>5.4.1</b> Regresión Multinomial en R</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelos-para-conteos"><i class="fa fa-check"></i><b>5.5</b> Modelos para conteos</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#sobredispersión-en-glm-poisson"><i class="fa fa-check"></i><b>5.5.1</b> Sobredispersión en GLM Poisson</a></li>
<li class="chapter" data-level="5.5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-poisson-r"><i class="fa fa-check"></i><b>5.5.2</b> GLM Poisson R</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-binomial-negativa"><i class="fa fa-check"></i><b>5.6</b> GLM binomial negativa</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-binomial-negativa-en-r"><i class="fa fa-check"></i><b>5.6.1</b> GLM Binomial Negativa en R</a></li>
<li class="chapter" data-level="5.6.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejercicio"><i class="fa fa-check"></i><b>5.6.2</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#exponencial"><i class="fa fa-check"></i><b>5.7</b> Exponencial</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-exponencial-en-r"><i class="fa fa-check"></i><b>5.7.1</b> GLM Exponencial en R</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#gamma"><i class="fa fa-check"></i><b>5.8</b> Gamma</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-gamma-en-r"><i class="fa fa-check"></i><b>5.8.1</b> GLM Gamma en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html"><i class="fa fa-check"></i><b>6</b> Modelos Lineales Generalizados (Construcción y Evaluación)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#exploración-de-los-datos."><i class="fa fa-check"></i><b>6.1</b> Exploración de los datos.</a></li>
<li class="chapter" data-level="6.2" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#elección-de-la-estructura-de-errores-y-función-liga."><i class="fa fa-check"></i><b>6.2</b> Elección de la estructura de errores y función liga.</a></li>
<li class="chapter" data-level="6.3" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#bondad-de-ajuste."><i class="fa fa-check"></i><b>6.3</b> Bondad de ajuste.</a></li>
<li class="chapter" data-level="6.4" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#simplificación-del-modelo."><i class="fa fa-check"></i><b>6.4</b> Simplificación del modelo.</a></li>
<li class="chapter" data-level="6.5" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#criterios-de-evaluación-de-modelos."><i class="fa fa-check"></i><b>6.5</b> Criterios de evaluación de modelos.</a></li>
<li class="chapter" data-level="6.6" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#análisis-de-los-residuos."><i class="fa fa-check"></i><b>6.6</b> Análisis de los residuos.</a></li>
<li class="chapter" data-level="6.7" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#evaluación-de-glms-en-r"><i class="fa fa-check"></i><b>6.7</b> Evaluación de GLMs en R</a></li>
</ul></li>
<li class="part"><span><b>III Redes neuronales</b></span></li>
<li class="chapter" data-level="7" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html"><i class="fa fa-check"></i><b>7</b> ¿Qué es una red neuronal?</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#ejemplo"><i class="fa fa-check"></i><b>7.0.1</b> Ejemplo</a></li>
<li class="chapter" data-level="7.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#teorema-de-universalidad"><i class="fa fa-check"></i><b>7.1</b> Teorema de Universalidad</a></li>
<li class="chapter" data-level="7.2" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#entrenamiento-de-una-red-neuronal"><i class="fa fa-check"></i><b>7.2</b> Entrenamiento de una red neuronal</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#back-propagation"><i class="fa fa-check"></i><b>7.2.1</b> Back-propagation</a></li>
<li class="chapter" data-level="7.2.2" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#saturación"><i class="fa fa-check"></i><b>7.2.2</b> Saturación</a></li>
<li class="chapter" data-level="7.2.3" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#regularización"><i class="fa fa-check"></i><b>7.2.3</b> Regularización</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#redes-neuronales-en-r"><i class="fa fa-check"></i><b>7.3</b> Redes Neuronales en R</a></li>
</ul></li>
<li class="part"><span><b>IV Maquinas de Soporte Vectorial (SVM)</b></span></li>
<li class="chapter" data-level="8" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html"><i class="fa fa-check"></i><b>8</b> ¿Qué es una SVM?</a>
<ul>
<li class="chapter" data-level="8.1" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#estimación-de-los-coeficientes"><i class="fa fa-check"></i><b>8.1</b> Estimación de los coeficientes</a></li>
<li class="chapter" data-level="8.2" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#rkhs-y-el-método-kernel"><i class="fa fa-check"></i><b>8.2</b> RKHS y el método kernel</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#cómo-escoger-un-kernel-k"><i class="fa fa-check"></i><b>8.2.1</b> ¿Cómo escoger un kernel k?</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#margen-suave"><i class="fa fa-check"></i><b>8.3</b> <em>Margen suave</em></a></li>
<li class="chapter" data-level="8.4" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#svms-en-r"><i class="fa fa-check"></i><b>8.4</b> SVMs en R</a></li>
</ul></li>
<li class="part"><span><b>V Árboles de regresión y clasificación</b></span></li>
<li class="chapter" data-level="9" data-path="antecedentes.html"><a href="antecedentes.html"><i class="fa fa-check"></i><b>9</b> Antecedentes</a>
<ul>
<li class="chapter" data-level="9.1" data-path="antecedentes.html"><a href="antecedentes.html#árboles-de-regresión"><i class="fa fa-check"></i><b>9.1</b> Árboles de regresión</a></li>
<li class="chapter" data-level="9.2" data-path="antecedentes.html"><a href="antecedentes.html#árboles-de-clasificación"><i class="fa fa-check"></i><b>9.2</b> Árboles de clasificación</a></li>
<li class="chapter" data-level="9.3" data-path="antecedentes.html"><a href="antecedentes.html#algunos-problemas-en-los-árboles"><i class="fa fa-check"></i><b>9.3</b> Algunos problemas en los árboles</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="antecedentes.html"><a href="antecedentes.html#covariables-categóricas"><i class="fa fa-check"></i><b>9.3.1</b> Covariables categóricas</a></li>
<li class="chapter" data-level="9.3.2" data-path="antecedentes.html"><a href="antecedentes.html#la-matriz-de-pérdida"><i class="fa fa-check"></i><b>9.3.2</b> La matriz de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="antecedentes.html"><a href="antecedentes.html#árboles-en-r"><i class="fa fa-check"></i><b>9.4</b> Árboles en R</a></li>
</ul></li>
<li class="part"><span><b>VI Random Forests</b></span></li>
<li class="chapter" data-level="10" data-path="qué-es-un-random-forest.html"><a href="qué-es-un-random-forest.html"><i class="fa fa-check"></i><b>10</b> ¿Qué es un random forest?</a></li>
<li class="chapter" data-level="11" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html"><i class="fa fa-check"></i><b>11</b> ¿Cómo funciona Random Forest?</a>
<ul>
<li class="chapter" data-level="11.1" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html#ventajas-y-desventajas-de-random-forest"><i class="fa fa-check"></i><b>11.1</b> Ventajas y Desventajas de Random Forest</a></li>
<li class="chapter" data-level="11.2" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html#hiper-parámetros-más-útiles-del-random-forest"><i class="fa fa-check"></i><b>11.2</b> Hiper-parámetros más útiles del Random Forest:</a></li>
<li class="chapter" data-level="11.3" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html#otros-parámetros-también-disponibles-para-árboles"><i class="fa fa-check"></i><b>11.3</b> Otros parámetros también disponibles para árboles:</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="random-forest-en-r.html"><a href="random-forest-en-r.html"><i class="fa fa-check"></i><b>12</b> Random forest en R</a></li>
<li class="part"><span><b>VII Reducción de dimensiones</b></span></li>
<li class="chapter" data-level="13" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html"><i class="fa fa-check"></i><b>13</b> Análisis de Componentes Principales</a>
<ul>
<li class="chapter" data-level="13.1" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#introducción"><i class="fa fa-check"></i><b>13.1</b> Introducción</a></li>
<li class="chapter" data-level="13.2" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#los-componentes-principales"><i class="fa fa-check"></i><b>13.2</b> Los componentes principales</a></li>
<li class="chapter" data-level="13.3" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#interpretación-geométrica"><i class="fa fa-check"></i><b>13.3</b> Interpretación geométrica</a></li>
<li class="chapter" data-level="13.4" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#consideraciones"><i class="fa fa-check"></i><b>13.4</b> Consideraciones</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#escalamiento-de-variables"><i class="fa fa-check"></i><b>13.4.1</b> Escalamiento de variables</a></li>
<li class="chapter" data-level="13.4.2" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#varianza-explicada-por-los-componentes-principales"><i class="fa fa-check"></i><b>13.4.2</b> Varianza explicada por los componentes principales</a></li>
<li class="chapter" data-level="13.4.3" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#número-de-componentes-a-usar"><i class="fa fa-check"></i><b>13.4.3</b> Número de componentes a usar</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#pca-en-r"><i class="fa fa-check"></i><b>13.5</b> PCA en R</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="análisis-factorial.html"><a href="análisis-factorial.html"><i class="fa fa-check"></i><b>14</b> Análisis factorial</a>
<ul>
<li class="chapter" data-level="14.1" data-path="análisis-factorial.html"><a href="análisis-factorial.html#variables-latentes"><i class="fa fa-check"></i><b>14.1</b> Variables Latentes</a></li>
<li class="chapter" data-level="14.2" data-path="análisis-factorial.html"><a href="análisis-factorial.html#solución-factorial"><i class="fa fa-check"></i><b>14.2</b> Solución factorial</a></li>
<li class="chapter" data-level="14.3" data-path="análisis-factorial.html"><a href="análisis-factorial.html#rotaciones-de-factores"><i class="fa fa-check"></i><b>14.3</b> Rotaciones de factores</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="análisis-factorial.html"><a href="análisis-factorial.html#rotaciones-ortogonales"><i class="fa fa-check"></i><b>14.3.1</b> Rotaciones ortogonales</a></li>
<li class="chapter" data-level="14.3.2" data-path="análisis-factorial.html"><a href="análisis-factorial.html#rotaciones-oblicuas"><i class="fa fa-check"></i><b>14.3.2</b> Rotaciones oblicuas</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="análisis-factorial.html"><a href="análisis-factorial.html#análisis-factorial-en-r"><i class="fa fa-check"></i><b>14.4</b> Análisis factorial en R</a></li>
</ul></li>
<li class="part"><span><b>VIII Clusterización</b></span></li>
<li class="chapter" data-level="15" data-path="qué-son-los-métodos-de-clusterización.html"><a href="qué-son-los-métodos-de-clusterización.html"><i class="fa fa-check"></i><b>15</b> ¿Qué son los métodos de clusterización?</a></li>
<li class="chapter" data-level="16" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html"><i class="fa fa-check"></i><b>16</b> Conceptos teóricos</a>
<ul>
<li class="chapter" data-level="16.1" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#medidas-de-disimilaridad"><i class="fa fa-check"></i><b>16.1</b> Medidas de disimilaridad</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#datos-numéricos"><i class="fa fa-check"></i><b>16.1.1</b> Datos numéricos</a></li>
<li class="chapter" data-level="16.1.2" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#datos-ordinales"><i class="fa fa-check"></i><b>16.1.2</b> Datos ordinales</a></li>
<li class="chapter" data-level="16.1.3" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#datos-categóricos"><i class="fa fa-check"></i><b>16.1.3</b> Datos categóricos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="k-medias.html"><a href="k-medias.html"><i class="fa fa-check"></i><b>17</b> K-medias</a>
<ul>
<li class="chapter" data-level="17.1" data-path="k-medias.html"><a href="k-medias.html#algorítmo-k-medias"><i class="fa fa-check"></i><b>17.1</b> Algorítmo k-medias</a></li>
<li class="chapter" data-level="17.2" data-path="k-medias.html"><a href="k-medias.html#variantes"><i class="fa fa-check"></i><b>17.2</b> Variantes</a></li>
<li class="chapter" data-level="17.3" data-path="k-medias.html"><a href="k-medias.html#desventajas"><i class="fa fa-check"></i><b>17.3</b> Desventajas</a></li>
<li class="chapter" data-level="17.4" data-path="k-medias.html"><a href="k-medias.html#k-medias-en-r"><i class="fa fa-check"></i><b>17.4</b> K-medias en R</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html"><i class="fa fa-check"></i><b>18</b> Clusterización jerárquica</a></li>
<li class="part"><span><b>IX Selección de modelos</b></span></li>
<li class="chapter" data-level="19" data-path="motivación.html"><a href="motivación.html"><i class="fa fa-check"></i><b>19</b> Motivación</a>
<ul>
<li class="chapter" data-level="19.0.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#ejemplo"><i class="fa fa-check"></i><b>19.0.1</b> Ejemplo</a></li>
<li class="chapter" data-level="19.1" data-path="motivación.html"><a href="motivación.html#esquemas-básicos"><i class="fa fa-check"></i><b>19.1</b> Esquemas básicos</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="motivación.html"><a href="motivación.html#akaike-information-criterion."><i class="fa fa-check"></i><b>19.1.1</b> Akaike Information Criterion.</a></li>
<li class="chapter" data-level="19.1.2" data-path="motivación.html"><a href="motivación.html#bayesian-information-criterion."><i class="fa fa-check"></i><b>19.1.2</b> Bayesian Information Criterion.</a></li>
<li class="chapter" data-level="19.1.3" data-path="motivación.html"><a href="motivación.html#cross-validation"><i class="fa fa-check"></i><b>19.1.3</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="motivación.html"><a href="motivación.html#esquemas-adicionales"><i class="fa fa-check"></i><b>19.2</b> Esquemas adicionales</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="motivación.html"><a href="motivación.html#bootstrap"><i class="fa fa-check"></i><b>19.2.1</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="motivación.html"><a href="motivación.html#information-value-woe"><i class="fa fa-check"></i><b>19.3</b> Information Value &amp; WoE</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="motivación.html"><a href="motivación.html#weight-of-evidence-woe"><i class="fa fa-check"></i><b>19.3.1</b> Weight of Evidence (WoE)</a></li>
<li class="chapter" data-level="19.3.2" data-path="motivación.html"><a href="motivación.html#information-value"><i class="fa fa-check"></i><b>19.3.2</b> Information Value</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="motivación.html"><a href="motivación.html#matrices-de-confusión-y-derivados"><i class="fa fa-check"></i><b>19.4</b> Matrices de confusión (y derivados…)</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="motivación.html"><a href="motivación.html#estadísticos-derivados-de-las-matrices-de-confusión"><i class="fa fa-check"></i><b>19.4.1</b> Estadísticos derivados de las matrices de confusión</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="motivación.html"><a href="motivación.html#curvas-auc-roc"><i class="fa fa-check"></i><b>19.5</b> Curvas AUC-ROC</a>
<ul>
<li class="chapter" data-level="19.5.1" data-path="motivación.html"><a href="motivación.html#construcción-de-una-curva-auc-roc"><i class="fa fa-check"></i><b>19.5.1</b> Construcción de una curva AUC-ROC</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Herramientas Estadísticas para Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="qué-es-una-svm" class="section level1" number="8">
<h1><span class="header-section-number">Capítulo 8</span> ¿Qué es una SVM?</h1>
<p>Las maquinas de soporte vectorial son, en principio, un clasificador lineal con las característica de que no asume ninguna dsitribución a los datos y directamente busca el hiperplano óptimo que separa los datos en clases.</p>
<p>De forma inherente una SVM es un clasificador de dos clases, sin embargo para usarlas en problemas con <span class="math inline">\(L&gt;2\)</span> clases bastará con aplicar el algorítmo repetidamente comparando cada clase contra el resto o bien aplicar distintas SVM para cada par de clases y finalmente clasificar un nuevo punto vía <em>mayoría clasificada</em> para las SVMs.</p>
<p>En este capítulo exploraremos brevemente la teoría detras de las SVM para dos clases.</p>
<p>Será conveniente etiquetar las clases como <span class="math inline">\(\mathcal{L}=\{-1,-1\}\)</span>.</p>
<p>Dado cualquier par <span class="math inline">\((\beta,\beta_0) \in \mathbb{S}^{p-1}\times\mathbb{R}\)</span> existe un clasificador lineal que asigna a los puntos <span class="math inline">\(\{x:x^T\beta+\beta_0&gt;0\}\)</span> a 1 y a los puntos <span class="math inline">\(\{x:x^T\beta+\beta_0&lt;0\}\)</span> a -1.</p>
<p>Este clasificador es ambiguo en la decisión para la frontera del hiperplano dado por <span class="math inline">\(H_{\beta_0,\beta}\colon= \{x:x^T\beta+\beta_0=0\}\)</span>.</p>
<p>Para cualquier observación <span class="math inline">\((x_i,y_i)\)</span>, el clasificador funciona correctamente si y solo si <span class="math inline">\(y_i(x_i^T\beta +\beta_0)&gt;0\)</span>. Más aún, el valor <span class="math inline">\(|y_i(x_i^T\beta +\beta_0)|\)</span> indica la distancia de <span class="math inline">\(x_i\)</span> a la frontera de decisión.</p>
<p>Las SMV buscan aquel hiperplano que separa completamente a las dos clases y que maximiza la distancia hacia el punto más cercano. Este plano puede determinarse a través del siguiente problema de optimización:</p>
<p><span class="math display">\[ \begin{equation}  \max_{\beta,\beta_0,||\beta||_2=1} M \ sujeto \ a \ y_i(x_i^T\beta+\beta_0)\geq M,\ i=1,\dots,n \end{equation} \]</span></p>
<p>Donde <span class="math inline">\(M\)</span> lo podemos interpretar como el margen alrededor del hiperplano optimo que no contiene a ninguna observación.</p>
<div id="estimación-de-los-coeficientes" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Estimación de los coeficientes</h2>
<p>El problema de optimización mostrado es complicado de resolver fundamentalmente por la restricción <span class="math inline">\(||\beta||_2=1\)</span>, sin embargo dado que el plano <span class="math inline">\(H_{\beta_0,\beta}\)</span> es invariente bajo escalamiento sobre <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta\)</span> podemos aplicar el factor <span class="math inline">\(1/M\)</span> (de tal forma que <span class="math inline">\(||\beta||_2=1/M\)</span>) y el problema se transformaría en:</p>
<p><span class="math display">\[\begin{equation}  \max_{\beta,\beta_0}\frac{1}{||\beta||_2} \ sujeto\ a \ y_i(x_i^T\beta+\beta_0)\geq 1,\ i=1,\dots,n \end{equation} \]</span></p>
<p>Si además trabajamos con el problema equivalente</p>
<p><span class="math display" id="eq:optim">\[\begin{equation} 
  \min_{\beta,\beta_0}\frac{1}{2}||\beta||_2^2 \ sujeto\ a \ y_i(x_i^T\beta+\beta_0)\geq 1,\ i=1,\dots,n 
  
  \tag{8.1}
\end{equation}\]</span></p>
<p>tendremos ahora un problema de optimización con una función objetivo cuadrática y restricciones lineales que puede resolverse eficientemente usando optimizadores convexos estándar.</p>
<p>Para resolver el problema de optimización se usa el problema dual de Lagrange cuya teoría nos dice que la mejor cota inferior para <span class="math inline">\(||\beta||_2^2/2\)</span> es igual al valor óptimo:</p>
<p><span class="math display">\[\begin{equation} \max_{\lambda\in \mathbb{R_{\geq0}^n}} \min_{\beta,\beta_0} L(\beta,\beta_0;\lambda) \end{equation} \]</span></p>
<p>donde <span class="math inline">\(L(\beta,\beta_0;\lambda)\)</span> es el lagrangiano de <a href="qué-es-una-svm.html#eq:optim">(8.1)</a>.</p>
<p>Los estimadores de los coeficientes son calculados usando a los puntos <em>más cercanos</em> a la frontera de decisión como puntos <em>soporte</em> de ahí que a esos puntos se les llame <em>vectores soporte</em></p>
<p>Sin meternos demasiado en la teoría, el proceso de estimación de los coeficientes de una SVM y la obtención de predicciones usando a dualidad de Lagrange es el que sigue:</p>
<ul>
<li><p>Resolver el problema dual de <a href="qué-es-una-svm.html#eq:optim">(8.1)</a></p></li>
<li><p>Sean <span class="math inline">\(S=\{i:\lambda_i^*\neq0\}\)</span> los índices de los vectores soporte</p></li>
<li><p>Calcular <span class="math inline">\(\beta^*=\sum_{i\in S}\lambda_i^*y_i x_i\)</span> y <span class="math inline">\(\beta_0^*=-\frac{1}{2}\{\min_{i:y_i=1}x_i^T\beta^*+\max_{i:y_i=-1}x_i^T\beta^*\}\)</span></p></li>
<li><p>Para cada punto nuevo <span class="math inline">\(x\)</span>, lo clasificamos con <span class="math display">\[\psi^{SVM}(x)=sgn(x^T\beta^*+\beta_0^*)\]</span></p></li>
</ul>
</div>
<div id="rkhs-y-el-método-kernel" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> RKHS y el método kernel</h2>
<p>Típicamente nos encontraremos con datos para los cuales no será evidente la existencia de un hiperplano que pueda separar a las clases. Esto puede suceder porque la frontera de clasificación es no lineal, o bien porque los datos tienen mucha varianza (<em>ruido</em>) y las densidades de las clases se intersectan.</p>
<p>Existen dos formas de <em>generalizar</em> el hiperplano de separación para resolver ambos problemas.</p>
<p>Una forma de resolver el problema de <em>no separabilidad</em> es llevar los datos a alguna dimensión superior mediante <span class="math inline">\(x \mapsto \phi(x)=(\phi_1(x),\phi_2(x),\dots)\)</span> donde <span class="math inline">\(\phi_1,\phi_2,\dots\)</span> son funciones reales en algún conjunto <span class="math inline">\(\mathcal{X}\)</span>, donde <span class="math inline">\(\mathcal{X}\)</span> no requiere alguna estructura específica.</p>
<p>A la función <span class="math inline">\(\phi\)</span> se le conoce como función característica (<em>feature map</em>) y a sus componentes <span class="math inline">\(\phi_1,\phi_2,\dots\)</span> se les llama características (<em>features</em>)</p>
<p>En dimensiones suficientemente <em>grandes</em> los puntos correspondientes a dos clases son siempre separables.</p>
<p>Una pregunta natural es ¿cómo escoger buenas <em>características</em> y cuántas deberíamos escoger? Antes de responder estas preguntas asumamos que ya tenemos la función <span class="math inline">\(\phi\)</span> y definamos <span class="math inline">\(k(x,x&#39;)=\langle \phi(x),\phi(x&#39;)\rangle\)</span> para cualesquiera <span class="math inline">\(x,x&#39;\in \mathcal{X}\)</span>, donde <span class="math inline">\(\langle \cdot,\cdot \rangle\)</span> es el producto interior en el espacio de características.</p>
<p>Con ello podemos obtener los coeficientes de la SVM como sigue:</p>
<p><strong>1:</strong> Resolver el problema dual en el espacio de características <span class="math display">\[\max_{\lambda\in \mathbb{R}_{\geq 0}^n,\sum_i\lambda_i y_i=0}\sum_{i=1}^n\lambda_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \lambda_i \lambda_j y_i y_j k(x_i,x_j)\]</span> para obtener <span class="math inline">\(\lambda_1^*,\cdots,\lambda_n^*\)</span></p>
<p><strong>2:</strong> Sea <span class="math inline">\(S=\{i:\lambda_i^*\neq 0\}\)</span> el conjunto de índices de los vectores soporte.</p>
<p><strong>3:</strong> Calcular <span class="math inline">\(\beta_0^*=\sum_{i\in S}\lambda_i^*y_i \phi(x_i)\)</span> y <span class="math display">\[\beta_0^*=-\frac{1}{2}\{\min_{i:y_i=1}\sum_{j\in S}\lambda _j^*y_j k(x_i,x_j)+\max_{i:y_i=-1}\sum_{j \in S}\lambda_j^* y_j k(x_i,x_j) \}\]</span></p>
<p><strong>4:</strong> Cada nueva observación la clasificamos de acuerdo a <span class="math display">\[sgn\{ \sum_{i\in S}\lambda_i^* y_i k(x_i,x) + \beta_0^* \}\]</span></p>
<p>Notemos que el procedimiento anterior depende de <span class="math inline">\(\phi\)</span> a través de <span class="math inline">\(k(\cdot,\cdot)\)</span>, excepto por <span class="math inline">\(\beta^*\)</span> pero si solo queremos hacer predicciones entonces no es necesario saber exactamente quién es <span class="math inline">\(\phi\)</span> siempre que la función <span class="math inline">\(k\)</span> esté dada.</p>
<p>El siguiente teorema nos dice que cualquier kernel definido positivo <span class="math inline">\(k\)</span> siempre puede obtenerse como el producto interno en algún espacio de características de dimensión infinita.</p>
<p>Recordemos que un <em>espacio de Hilbert</em> es un espacio producto interior donde toda sucesión de Cauchy tiene límite respecto a la norma asociada al producto interior. Podemos pensar a un espacio de Hilbert como la generalización del espacio Euclideano.</p>
<p><strong>Definición:</strong> Una función simétrica <span class="math inline">\(k:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}\)</span> se llama <em>kernel definido positivo</em> en <span class="math inline">\(\mathcal{X}\)</span> si <span class="math inline">\((k(x_i,x_j))_{i,j=1,\dots,n}\)</span> es una matriz semi-definida positiva para cualquier <span class="math inline">\(n\in \mathbb{N}\)</span> y <span class="math inline">\(x_1,\dots,x_n\in \mathcal{X}\)</span></p>
<p><strong>Teorema:</strong> Si <span class="math inline">\(k:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}\)</span> es un kernel definido positivo, entonces existe un espacio de Hilbert <span class="math inline">\(\mathcal{H}\)</span> y una función <span class="math inline">\(\phi :\mathcal{X}\rightarrow \mathcal{H}\)</span> tal que <span class="math inline">\(k(x_1,x_2)=\langle\phi(x_1),\phi(x_2)\rangle\)</span> donde <span class="math inline">\(\langle\cdot,\cdot \rangle_\mathcal{H}\)</span> es el producto interno en <span class="math inline">\(\mathcal{H}\)</span>.</p>
<p>Esto significa que en lugar de trabajar directamente en espacios de dimensión superior, podemos trabajar con kernels definidos positivos, más aún la <em>calidad</em> de las SVM dependerá solamente de la elección de las funciones kernel <span class="math inline">\(k\)</span>.</p>
<div id="cómo-escoger-un-kernel-k" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> ¿Cómo escoger un kernel k?</h3>
<p>Algunos kernels usados en la practica:</p>
<ul>
<li><p>Kernel lineal: <span class="math inline">\(k(x,x&#39;)=x^Tx&#39;\)</span></p></li>
<li><p>Kernel Polinomial: <span class="math inline">\(k(x,x&#39;)=(c+x^Tx&#39;)^d\)</span>. Típicamente usado si suponemos que la similaridad entre dos observaciones está dada por las covariables e interacciones de ellas.</p></li>
<li><p>Kernel Gaussiano: <span class="math inline">\(k(x,x&#39;)=\exp\{-\frac{1}{2\sigma^2}||x-x&#39;||_2^2\}\)</span>. Es el más popular para trabajar con características no lineales.</p></li>
<li><p>Kernel de Laplace: <span class="math inline">\(k(x,x&#39;)=\exp\{-\frac{1}{\sigma}||x-x&#39;||_2\}\)</span>. Similar al gaussiano, éste mide la similaridad de observaciones basados en la distancia en <span class="math inline">\(\mathcal{X}\)</span></p></li>
</ul>
</div>
</div>
<div id="margen-suave" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> <em>Margen suave</em></h2>
<p>Recordemos que en el contexto de las SVM llamamos <em>margen</em> al espacio entre la frontera de decisión y el punto más cercano de cada clase.</p>
<p>Una segunda forma de lidiar con problemas de no separabilidad es buscar el hiperplano óptimo con el margen <span class="math inline">\(M\)</span>, más amplio tal que el número de observaciones clasificadas erroneamente sea <em>pequeño</em>.</p>
<p>Matemáticamente, lo que hacemos es agregar variables de <em>holgura</em> al problema de optimización original:</p>
<p><span class="math display">\[ \begin{equation}  \max_{\beta,\beta_0,||\beta||_2=1} M \ sujeto \ a \ y_i(x_i^T\beta+\beta_0)\geq M(1-\xi_i),\ i=1,\dots,n \end{equation} \]</span></p>
<p><span class="math display">\[\sum_{i=1}^n \xi_i\leq K\]</span></p>
</div>
<div id="svms-en-r" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> SVMs en R</h2>
<p>En este ejemplo usaremos una máquina de soporte vectorial para clasificar un conjunto que asemeja un tablero de ajedréz.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
<p>Evidentemente, en el espacio original de los datos, resulta imposible encontrar un hiperplano que separe a las dos clases de interés.</p>
<p><strong>Algunos planos posibles</strong></p>
<p><img src="_main_files/figure-html/unnamed-chunk-91-.gif" width="672" /></p>
<p>Es aquí donde se vuelve relevante el uso de los RKHS (<em>reproducing kernel hilbert space</em>) cuya utilidad reside en poder trabajar con representaciones en dimensiones superiores en donde la elección del hiperplano sea más sencilla.</p>
<p>Para construir la svm debemos especificar, al menos, el tipo de algoritmo, el kernel a usar y los parámetros del mismo.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="qué-es-una-svm.html#cb133-1"></a>svm &lt;-<span class="st"> </span><span class="kw">ksvm</span>(z<span class="op">~</span>., <span class="dt">data =</span> chessboard[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], </span>
<span id="cb133-2"><a href="qué-es-una-svm.html#cb133-2"></a>            <span class="dt">type =</span> <span class="st">&quot;C-svc&quot;</span>, </span>
<span id="cb133-3"><a href="qué-es-una-svm.html#cb133-3"></a>            <span class="dt">kernel =</span> <span class="st">&quot;rbfdot&quot;</span>, </span>
<span id="cb133-4"><a href="qué-es-una-svm.html#cb133-4"></a>            <span class="dt">kpar=</span><span class="kw">list</span>(<span class="dt">sigma =</span> <span class="fl">0.1</span>), </span>
<span id="cb133-5"><a href="qué-es-una-svm.html#cb133-5"></a>            <span class="dt">scaled =</span> F, </span>
<span id="cb133-6"><a href="qué-es-una-svm.html#cb133-6"></a>            <span class="dt">cross  =</span> <span class="dv">5</span>)</span></code></pre></div>
<p><strong>Resultados con diferente valor para el parámetro del kernel</strong></p>
<p><img src="_main_files/figure-html/unnamed-chunk-93-.gif" width="672" /></p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="qué-es-una-red-neuronal.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="antecedentes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
