[
["index.html", "Herramientas Estadísticas para Ciencia de Datos Herramientas Estadísticas para Ciencia de Datos 0.1 Objetivos 0.2 Estructura 0.3 Detalles técnicos Licencia", " Herramientas Estadísticas para Ciencia de Datos Sofía Villers Gómez David Alberto Mateos Montes de Oca Herramientas Estadísticas para Ciencia de Datos Primera edición del libro de texto para el curso Herramientas Estadísticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias. 0.1 Objetivos Como el título lo indica, a lo largo de este libro se expondrán diferentes modelos estadísticos y sus aplicaciones con un enfoque a Ciencia de Datos. El objetivo es proveer al lector de las herramientas necesarias para comprender los fundamentos de estos modelos y sus aplicaciones mediante el uso del lenguaje R. 0.2 Estructura El libro se descompone en dos grandes secciones, una de ellas enfocada a presentar diferentes paquetes de R considerados de gran utilidad para la práctica de Ciencia de Datos, la otra se enfoca en la teoría y aplicación de diferentes modelos. La estructura del libro sigue el orden en que se imparte la materia en la Facultad de Ciencias sin embargo hemos diseñado los capítulos suficientemente independientes como para ser consultados en el orden de preferencia del lector. 0.3 Detalles técnicos Este libro fue escrito con bookdown usando RStudio. Esta versión fue escrita con: ## Finding R package dependencies ... Done! ## setting value ## version R version 4.0.2 (2020-06-22) ## os Windows 10 x64 ## system x86_64, mingw32 ## ui RStudio ## language (EN) ## collate Spanish_Mexico.1252 ## ctype Spanish_Mexico.1252 ## tz America/Mexico_City ## date 2020-11-16 Licencia This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],
["notación.html", "Capítulo 1 Notación", " Capítulo 1 Notación A lo largo del libro usaremos la notación típica de estadística pero también haremos uso de la siguiente: \\(x^{(i)}\\): el conjunto de inputs (variables explicativas) \\(y^{(i)}\\): es la variable de output/salida (variable dependiente) que queremos predecir (ajustar) A la pareja \\((x^{(i)},y^{(i)})\\) le llamaremos ejemplo de entrenamiento El conjunto de entrenamiento se denota por: \\(\\{(x^{(i)},y^{(i)})|i\\in N\\}\\) De forma general, denotaremos por \\(\\mathcal{X}\\) al espacio de inputs y por \\(\\mathcal{Y}\\) al espacio de outputs N.B. Omitiremos el uso de indices en donde sea claro a qué nos referimos. "],
["glosario-dscml-estadística.html", "Capítulo 2 Glosario DSc/ML - Estadística", " Capítulo 2 Glosario DSc/ML - Estadística Machine Learning / Ciencia de Datos Estadística red, grafo (network, graphs) modelo (model) pesos (weigths) parámetros (parameters) aprendizaje (learning) ajuste (fiting) prueba, generalización (testing, generalization) ajuste en el conjunto de prueba aprendizaje supervisado (supervised learning) regresión, clasificación aprendizaje no supervisado (unsupervised learning) estimación de densidades, clusterización "],
["entrenamiento-de-modelos.html", "Capítulo 3 Entrenamiento de modelos", " Capítulo 3 Entrenamiento de modelos Se le denomina de esta manera a la acción de ajustar el mejor modelo a los datos. Formalmente se define como sigue: Dado un conjunto de entrenamiento \\((x^{(i)},y^{(i)})\\in(\\mathcal{X} \\times \\mathcal{Y})\\) el objetivo es aprender (ajustar) una función \\(h:\\mathcal{X}\\rightarrow \\mathcal{Y}\\) tal que \\(h(x)\\) sea un buen predictor de \\(y\\). La función \\(h\\) suele llamarse hipótesis. Cuando el conjunto \\(\\mathcal{Y}\\) es continuo, estamos frente a un problema de regresión. Si se trata de un conjunto discreto entonces tenemos un problema de clasificación. "],
["regresión-lineal.html", "Capítulo 4 Regresión Lineal 4.1 Un poco de história 4.2 Objetivos del análisis de regresión 4.3 El algorítmo de regresión lineal 4.4 Regresión lineal simple 4.5 Solución al problema de regresión lineal simple 4.6 Regresión lineal múltiple 4.7 Solución al problema de regresión lineal múltiple. 4.8 Aplicación en R", " Capítulo 4 Regresión Lineal 4.1 Un poco de história Los primeros problemas prácticos tipo regresión iniciaron en el siglo XVIII, relacionados con la navegación basada en la Astronomía. Legendre desarrolló el método de mínimo cuadrados en 1805. Gauss afirma que él desarrolló este método algunos años antes y demuestra, en 1809, que mínimos cuadrados proporciona una solución óptima cuando los errores se distribuyen normal. Francis Galton acuña el término regresión al utilizar el modelo para explicar el fenómeno de que los hijos de padres altos, tienden a ser altos en su generación, pero no tan altos como lo fueron sus padres en la propia, por lo que hay un efecto de regresión. El modelo de regresión lineal es, probablemente, el modelo de su tipo más conocido en estadística. El modelo de regresión se usa para explicar o modelar la relación entre una sola variable, \\(y\\), llamada dependiente o respuesta, y una o más variables predictoras, independientes, covariables, o explicativas, \\(x_1, x_2, ..., x_p\\). Si \\(p = 1\\), se trata de un modelo de regresión simple y si \\(p &gt; 1\\), de un modelo de regresión múltiple. En este modelo se asume que la variable de respuesta, \\(y\\), es aleatoria y las variables explicativas son fijas, es decir, no aleatorias. La variable de respuesta debe ser continua, pero los regresores pueden tener cualquier escala de medición. 4.2 Objetivos del análisis de regresión Existen varios objetivos dentro del análisis de regresión, entre otros: Determinar el efecto, o relación, entre las variables explicativas y la respuesta. Predicción de una observación futura. Describir de manera general la estructura de los datos. 4.3 El algorítmo de regresión lineal Sea \\(\\Phi: \\mathcal{X} \\rightarrow \\mathbb{R}^N\\) y consideremos la familia de hipótesis lineales \\[H=\\{x\\mapsto w \\cdot \\Phi(x)+b | w\\in\\mathbb{R}^N, b\\in\\mathbb{R}\\}\\] La regresión lineal consiste en buscar la hipótesis \\(h\\in H\\) con el menor error cuadrático medio, es decir, se debe resolver el problema de optimización: \\[\\min \\frac{1}{m}\\sum_{i=1}^{m}(h(x_i)-y_i)^2\\] 4.4 Regresión lineal simple Para este modelo supondremos que nuestra respuesta, \\(y\\), es explicada únicamente por una covariable, \\(x\\). Entonces, escribimos nuestro modelo como: \\[y^{(i)}=\\beta_0+\\beta_1x^{(i)}+\\epsilon^{(i)},\\ \\ i=1,2,\\dots,n\\] Como podemos observar, se ha propuesto una relación lineal entre la variable \\(y\\) y la variable explicativa \\(x\\), que es nuestro primer supuesto sobre el modelo: La relación funcional entre \\(x\\) y \\(y\\) es una línea recta. Observamos que la relación no es perfecta, ya que se agrega el término de error, \\(\\epsilon\\). Dado que la parte aleatoria del modelo es la variable \\(y\\), asumimos que al error se le “cargan” los errores de medición de \\(y\\), así como las perturbaciones que le pudieran ocasionar los términos omitidos en el modelo. Gauss desarrolló este modelo a partir de la teoría de errores de medición, que es de donde se desprenden los supuestos sobre este término: \\(\\mathbb{E}(\\epsilon^{(i)})=0\\) \\(\\mathbb{V}ar(\\epsilon^{(i)})=\\sigma^2\\) \\(\\mathbb{C}ov(\\epsilon^{(i)},\\epsilon^{(j)})=0, \\ \\forall i\\neq j\\) N.B. Los errores \\(\\epsilon^{(i)}\\) son variables aleatorias no observables. 4.5 Solución al problema de regresión lineal simple 4.5.1 Mínimos cuadrados ordinarios En una situación real, tenemos \\(n\\) observaciones de la variable de respuesta así como de la variable explicativa, que conforman las parejas de entrenamiento \\((x_i, y_i), \\ i = 1, 2, ..., n\\). Entonces, nuestro objetivo será encontrar la recta que mejor ajuste a los datos observados. Utilizaremos el método de mínimos cuadrados para estimar los parámetros del modelo, que consiste en minimizar la suma de los errores al cuadrado, esto es: \\[\\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n(y_i-(\\beta_0+\\beta_1x^{(i)}))^2\\] Al minimizar la expresión anteriore obtenemos las siguientes expresiones para los estimadores: \\[\\hat{\\beta_1}=\\frac{\\sum_{i=1}^ny_i(x_i-\\bar{x})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\] \\[\\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x}\\] Una desventaja del método de mínimos cuadrados, es que no se pueden hacer procesos de inferencia sobre los parámetros de interés \\(\\beta_0\\) y \\(\\beta_1\\); procesos como intervalos de confianza o pruebas de hipótesis. Para subsanar esta deficiencia, es necesario asumir una distribución para el error, \\(\\epsilon_i\\), que, siguiendo la teoría general de errores, se asume que tiene distribución normal, con media cero y varianza \\(\\sigma^2\\). Este supuesto garantiza que las distribuciones de \\(y_i,\\ \\hat{\\beta_0},\\ \\hat{\\beta_1}\\) sean normales, lo que permite tanto la construcción de intervalos de confianza como de pruebas de hipótesis. N.B El estimador de \\(\\sigma^2\\) está dado por \\(\\hat{\\sigma^2}=\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{n-2}\\) 4.5.1.1 Pruebas de hipótesis En el modelo de regresión lineal simple, la prueba de hipótesis más importante es determinar si estadísticamente existe la dependencia líneal entre \\(x\\) y \\(y\\), y que no sea producto del muestreo (debido al azar). Es decir, realizar la prueba de hipótesis: \\[H_0:\\beta_1=0 \\ vs.\\ H_a:\\beta_1\\neq 0\\] No rechazar la hipótesis nula, implicaría que la variable \\(x\\) no ayuda a explicar a \\(y\\) o bien que, tal vez, la relación entre estas variables no es lineal. En este modelo, esta última explicación es un poco cuestionable, ya que se parte, de inicio, del diagrama de dispersión de los datos. Si rechazamos la hipótesis nula, implicará que \\(x\\) es importante para explicar la respuesta \\(y\\) y que la relación lineal entre ellas puede ser adecuada. Rechazar esta hipótesis nula, también podría implicar que existe una relación lineal entre las variables pero, tal vez, se pueda mejorar el ajuste con algún otro término no lineal. 4.5.1.2 Interpretación de los parámetros Cuando se tiene una recta en el sentido determinista, los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) tienen una interpretación muy clara; \\(\\beta_0\\) se interpreta como el valor de \\(y\\) cuando \\(x\\) es igual a cero y \\(\\beta_1\\) como el cambio que experimenta la variable de respuesta \\(y\\) por unidad de cambio en \\(x\\). La interpretación, desde el punto de vista estadístico, de los parámetros estimados en el modelo de regresión es muy similar: \\(\\hat{\\beta_0}\\) es el promedio esperado de la respuesta \\(y\\) cuando \\(x = 0\\) (este parámetro tendrá una interpretación dentro del modelo, si tiene sentido que \\(x\\) tome el valor cero, de lo contrario, no tiene una interpretación razonable) y \\(\\hat{\\beta_1}\\) es el cambio promedio o cambio esperado en \\(y\\) por unidad de cambio en \\(x\\). 4.6 Regresión lineal múltiple La mayoría de los fenómenos reales son multicausales, por esta razón, un modelo de regresión más acorde a estudios reales es el modelo de regresión lineal múltiple, que es la generalización del modelo simple. En este modelo supondremos que la variable de respuesta, \\(y\\), puede explicarse a través de una colección de \\(k\\) covariables \\(x_1,\\dots,x_k\\). El modelo se escribe de la siguiente manera: \\[y_i = \\beta_0+\\beta_1 x_1^{(i)}+\\beta_2 x_2^{(i)}+\\dots++\\beta_k x_k^{(i)}+\\epsilon_i\\] Al igual que en el caso simple, los parámetros del modelo se pueden estimar por mínimos cuadrados, con el inconveniente de que no se pueden realizar inferencias sobre ellos. Nuevamente, para poder hacer intervalos de confianza y pruebas de hipótesis sobre los verdaderos parámetros hay que suponer que el vector de errores se distribuye normal, en este caso multivariada, es decir: \\[\\epsilon\\sim N_n(0,\\sigma^2\\mathbb{I})\\] Esta estructura del error permite tener las mismas propiedades distribucionales que en regresión simple, es decir, \\(y_i\\) se distribuye normal y \\(\\beta_i\\) tiene distribución normal, facilitando las inferencias sobre cada parámetro y la construcción de intervalos de predicción para las \\(y\\)’s. 4.7 Solución al problema de regresión lineal múltiple. 4.7.1 Ecuaciones normales Las expresiones para estimar los parámetros involucrados en el modelo son: \\[\\hat{\\beta}=(X^TX)^{-1}X^Ty\\] \\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{n-p}\\] donde \\(p=k+1\\) es el número total de parámetros en el modelo. Tanto en el modelo simple como en el múltiple, la variación total de las \\(y\\)’s se puede descomponer en una parte que explica el modelo, i.e., los \\(k\\) regresores o variables explicativas y otra no explicada por estas variables, llamada error. \\[\\sum_{i=1}^n(y_i-\\bar{y})^2=\\sum_{i=1}^n(\\hat{y_i}-\\bar{y})^2+\\sum_{i=1}^n(\\hat{y_i}-y_i)^2\\] 4.7.1.1 Prueba de hipótesis La descomposición anterior ayuda para realizar la importante prueba de hipótesis: \\[H_0:\\beta_1=\\beta_2=\\dots=\\beta_k=0\\ vs.\\ H_a:\\beta_i\\neq0 \\ p.a. \\ i\\] misma que se realiza a través del cociente entre los errores cuadráticos medios: \\[F_0=\\frac{SS_R/k}{SS_E/(n-k-1)}=\\frac{MS_R}{MS_E}\\sim F_{(k,n-k-1)}\\] Esta estadística se desprende de la tabla de análisis de varianza, que es muy similar a la tabla ANOVA que se utiliza para hacer pruebas de hipótesis. En este caso la tabla es: Fuente de variación Grados de libertad Suma de cuadrados Cuadrados medios F Regresión k \\(SS_R\\) \\(MS_R=SS_R/k\\) Error n-k-1 \\(SS_E\\) \\(MS_E=SS_E/(n-k-1)\\) \\(F=\\frac{MS_R}{MS_E}\\) Total n-1 \\(S_{yy}\\) Por lo general, esta estadística rechaza la hipótesis nula, ya que de lo contrario, implicaría que ninguna de las variables contribuye a explicar la respuesta, \\(y\\). Como se puede observar en la hipótesis alternativa, el rechazar \\(H_0\\) solo implica que al menos uno de los regresores contribuye significativamente a explicar \\(y\\). Asimismo, el rechazar \\(H_0\\) no implica que todos contribuyan ni tampoco dice cuál o cuáles contribuyen, por esta razón, una salida estándar de regresión múltiple tiene pruebas individuales sobre la significancia de cada regresor en el modelo. El estadístico para hacer tanto los contrastes de hipótesis como los intervalos de confianza individuales, es: \\[t=\\frac{\\hat{\\beta_i}-\\beta_0^{(i)}}{\\sqrt{\\hat{\\mathbb{V}ar}(\\hat{\\beta_i})}}\\sim t_{(n-p)}\\] Podemos apreciar que los constrastes de hipótesis se pueden hacer contra cualquier valor particular del parámetro \\(\\beta_0^{(i)}\\), en general. No obstante, en las pruebas estándar sobre los parámetros de un modelo, este valor particular es 0, ya que se intenta determinar si la variable asociada al \\(i\\)-ésimo parámetro es estadísticamente significativa para explicar la respuesta. Por lo que el estadístico para este caso es: \\[t=\\frac{\\hat{\\beta_i}}{\\sqrt{\\hat{\\mathbb{V}ar}(\\hat{\\beta_i})}}\\sim t_{(n-p)}\\] De este estadístico se desprenden también los intervalos de confianza para cada parámetro: \\[\\beta_i\\in(\\hat{\\beta_i}\\pm t_{(n-p,1-\\alpha/2)} \\sqrt{\\hat{\\mathbb{V}ar} (\\hat{\\beta_i})})\\] #### Interpretación de parámetros La interpretación de cada parámetro es similar a la del coeficiente de regresión \\(\\hat{\\beta_1}\\) en el modelo simple, anexando la frase: “manteniendo constantes el resto de las variables”. Esto es, \\(\\hat{\\beta_i}\\) es el cambio promedio o cambio esperado en \\(y\\) por unidad de cambio en \\(x_i\\), sin considerar cambio alguno en ninguna de las otras variables dentro del modelo, es decir, suponiendo que estas otras variables permanecen fijas. Esta interpretación es similar a la que se hace de la derivada parcial en un modelo determinista. Nuevamente, la interpretación de \\(\\hat{\\beta_0}\\) estará sujeta a la posibilidad de que, en este caso, todas las variables puedan tomar el valor cero. 4.7.1.2 Predicción de nuevos valores Uno de los usos más frecuentes del modelo de regresión es el de predecir un valor de la respuesta para un valor particular de las covariables en el modelo. Si la predicción se realiza para un valor de las covariables dentro del rango de observación de las mismas, se tratará de una interpolación, y si se realiza para un valor fuera de este rango, hablaremos de una extrapolación. En cualquiera de los dos casos, estaremos interesados en dos tipos de predicciones: Predicción de la respuesta media: \\(y_0=\\mathbb{E}(y|X_0)\\) Predicción de una nueva observación: \\(y_0\\) En ambos casos, la estimación puntual es la misma: \\(\\hat{y_0}=X_0^T\\hat{\\beta}\\) Lo que difiere es el intervalo de predicción. Para la respuesta media es: \\(y_0=(\\hat{y_0}\\pm t_{(n-p,1-\\alpha/2)}\\sqrt{\\hat{\\sigma^2}X_0^T(X^TX)^{-1}X_0})\\) Y para predecir una observación: \\(y_0=(\\hat{y_0}\\pm t_{(n-p,1-\\alpha/2)}\\sqrt{\\hat{\\sigma^2}(1+X_0^T(X^TX)^{-1}X_0)})\\) 4.7.1.3 Coeficiente de determinación Un primer elemento de juicio sobre el modelo de regresión lo constituye el coeficiente de determinación \\(R^2\\), que es la proporción de variabilidad de las \\(y\\)’s que es explicada por las \\(x\\)’s y que se escribe como: \\[R^2=\\frac{SS_R}{S_{yy}}=1-\\frac{SS_E}{S_{yy}}\\] Una \\(R^2\\) cercana a uno implicaría que mucha de la variabilidad de la respuesta es explicada por el conjunto de regresores incluidos en el modelo. Es deseable tener una \\(R^2\\) grande en nuestro modelo, pero esto no significa, como mucha gente piensa, que ya el modelo está bien ajustado. 4.7.2 Evaluación de supuestos Los dos modelos de regresión presentados, el simple y el múltiple, se construyeron sobre los supuestos de: La relación funcional entre la variable de respuesta \\(y\\) y cada regresor \\(x_i\\) es lineal La esperanza de los errores es cero, \\(\\mathbb{E}(\\epsilon_i=0)\\) La varianza de los errores es constante, \\(\\mathbb{V}ar(\\epsilon_i) = \\sigma^2\\) Los errores no están correlacionados, \\(\\mathbb{C}ov(\\epsilon_i, \\epsilon_j) = 0;\\ i\\neq j\\) Los errores tienen distribución normal con media cero y varianza \\(\\sigma^2\\) Entonces, para garantizar que el modelo es adecuado, es indispensable verificar estos supuestos. 4.7.2.1 Residuos Los elementos más importantes para verificar estos supuestos son los residuos, definidos como: \\[e_i=y_i-\\hat{y}_i\\] Estos residuos representan la discrepancia entre la respuesta predicha por el modelo ajustado, \\(\\hat{y}_i\\) y el correspondiente valor observado, \\(y_i\\). En la literatura de regresi ́on lineal existen cuatro tipos de residuos, a saber Residuo crudo: \\(e_i\\) Residuo estandarizado: \\(d_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma}^2}}\\) Residuo estudentizado interno: \\(r_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma}^2(1-h_{ii}})}\\) Residuo estudentizado externo: \\(t_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma_{(-i)}}^2(1-h_{ii})}}\\) Estos residuos se utilizan en los distintos procedimientos para evaluar los supuestos y lo adecuado del ajuste del modelo. La mayoría de las pruebas conocidas para la verificación de los supuestos, son pruebas gráficas. Indudablemente, la prueba más importante es sobre la normalidad de los errores, ya que sobre este supuesto descansan todas la inferencias de este modelo. La manera de verificarlo es a través de la gráfica conocida como QQ-plot o QQ-norm, que grafica los cuantiles teóricos de una distribución normal (eje x) vs. los cuantiles asociados a los residuos. Entonces, si los residuos realmente provienen de una normal, la gráfica debe mostrar la función identidad. Fuertes desviaciones de esta línea darían evidencia de que los errores no se distribuyen normal. 4.7.2.2 Linealidad de los predictores La manera estándar de evaluar la linealidad de las variables explicativas es a través de la gráfica de cada una de ellas contra los residuos. Si la variable en cuestión ingresa al modelo de manera lineal, esta gráfica debe mostrar un patrón totalmente aleatorio entre los puntos dispuestos en ella. Cuando la variable explicativa es politómica, este tipo de gráficas son poco ilustrativas en este sentido. 4.7.2.3 Supuestos sobre los errores Si la gráfica entre los valores ajustados y los residuos estandarizados, muestra un patrón aleatorio, es simétrica alrededor del cero y los puntos están comprendidos entre los valores -2 y 2, entonces se tendrá evidencia de que los errores tienen media cero, varianza constante y no están correlacionados. Los métodos mostrados hasta ahora, permiten evaluar el modelo de manera global y no por cada observación dentro del mismo. Dado que una observación puede resultar determinante sobre alguna(s) característica(s) del modelo, es conveniente verificar el impacto que cada observación pueda tener en los distintos aspectos del modelo. Las estadísticas para evaluar el impacto que tiene una observación sobre todo el vector de parámetros, alguno de los regresores y sobre los valores predichos, se basan en la misma idea, que consiste en cuantificar el cambio en la característica de interés con y sin la observación que se está evaluando. 4.7.2.3.1 Puntos palanca Antes de presentar las estadísticas que servirán para hacer este diagnóstico, introduciremos un elemento que es común a ellas: la llamada palanca (leverage) de una observación. Recordemos que el ajuste del modelo se expresaba como: \\[\\hat{\\beta}=(X^TX)^{-1}X^Ty \\Rightarrow \\hat{y}=X\\hat{\\beta}=Hy\\] Con \\(H\\) conocida como la matriz sombrero. Un resultado fundamental sobre esta matriz sombrero es: \\[\\mathbb{V}ar(e)=(I-H)\\sigma^2 \\Rightarrow \\mathbb{V}ar(e_i)=(1-h_i)\\sigma^2\\] Con \\(h_i\\) el i-ésimo elemento de la diagonal de la matriz \\(H\\). Observemos que esta palanca sólo depende de \\(X\\), entonces, una observación con una palanca, \\(h_i\\), grande, es aquella con valores extremos en alguna(s) de su(s) covariable(s). Ya que el promedio de las \\(h_i&#39;s\\) es \\(p/n\\), consideraremos una observación con palanca grande si su palanca es mayor a \\(2p/n\\). En este sentido, \\(h_i\\) corresponde a la distancia de Mahalanobis de \\(X\\) definida como \\((X-\\bar{X})^T\\hat{\\Sigma}^{-1}(X-\\bar{X})\\). La dependencia de las estadísticas para el diagnóstico de las observaciones, estriba en que sus cálculos dependen de los valores de la palanca de cada individuo. Estas estadísticas son: Distancia de Cook Dfbetas Dffits Distancia de Cook: Sirve para determinar si una observación es influyente en todo el vector de parámetros. Una observación se considera influyente, si su distancia de Cook sobrepasa el valor uno. Dfbetas: Sirven para determinar si una observación es influyente en alguno de los coeficientes de regresión. Hay un dfbeta por cada parámetro dentro del modelo, incluido, por supuesto, el de la ordenada al origen. La regla de dedo es que la observación \\(i\\) es influyente en el j-ésimo coeficiente de regresión si: \\[|Dfbetas_{j,i}|&gt;\\frac{2}{\\sqrt{n}}\\] Dffits: Se utilizan para determinar si una observación es influyente en la predicción de \\(y\\). Se dice que la i-ésima observación es influyente para predecir \\(y\\), si: \\[|Dffits_i|&gt;2\\sqrt{\\frac{p}{n}}\\] 4.7.2.4 Multicolinealidad El modelo de regresión lineal múltiple, se construye bajo el supuesto de que los regresores son ortogonales, i.e., son independientes. Desafortunadamente, en la mayoría de las aplicaciones el conjunto de regresores no es ortogonal. Algunas veces, esta falta de ortogonalidad no es seria; sin embargo, en algunas otras los regresores están muy cerca de una perfecta relación lineal, en tales casos las inferencias realizadas a través del modelo de regresión lineal pueden ser erróneas. Cuando hay una cercana dependencia lineal entre los regresores, se dice que estamos en presencia de un problema de multicolinealidad. Efectos de la multicolinealidad: Varianzas de los coeficientes estimados son muy grandes. Los estimadores calculados de distintas sub muestras de la misma población, pueden ser muy diferentes. La significancia de algún regresor se puede ver afectada (volverse no significativo) por que su varianza es más grande de lo que debería ser en realidad o por la correlación de la variable con el resto dentro del modelo. Es común que algún signo de un parámetro cambie, haciendo ilógica su interpretación dentro del modelo. 4.7.2.4.1 ¿Cómo detectar multicolinealidad? Matriz de correlación. Examinar las correlaciones entre pares de variables: \\[r_{ij}\\ \\ \\ i, j = 1, 2, \\dots, k\\ \\ i\\neq j\\] Pero, si dos o más regresores están linealmente relacionados, es posible que ninguna de las correlaciones entre cada par de variables, sea grande. Factor de inflación de la varianza. \\[VIF_j=(1-R_j^2)^{-1}\\] Con \\(R_j^2\\) el coeficiente de determinación del modelo de regresión entre el j-ésimo regresor, \\(x_j\\) (tomado como variable de respuesta) y el resto de los regresores \\(x_i\\), \\(i\\neq j\\). Experiencias prácticas indican que si algunos de los VIF’s excede a 10, su coeficiente asociado es pobremente estimado por el modelo debido a multicolinealidad. Análisis del eigensistema. Basado en los eigenvalores de la matriz \\(X^TX\\). Número de condición. \\[K=\\frac{\\lambda_{max}}{\\lambda_{min}}\\] Si el número de condición es menor que 100, no existen problemas serios de multicolinealidad. Si está entre 100 y 1000 existe de moderada a fuerte multicolinealidad y si excede a 1000, hay severa multicolinealidad. Índice de condición. \\[k_j=\\frac{\\lambda_{max}}{\\lambda_j}\\] Si el índice de condición es menor que 10, no hay ningún problema. Si está entre 10 y 30, hay moderada multicolinealidad, y si es mayor que 30, existe una fuerte colinealidad en la j-ésima variable en el modelo. N.B. En algunos paquetes estos índices se presentan aplicando la raíz cuadrada a su expresión, entonces hay que extraer raíz a los puntos de corte de los criterios correspondientes. 4.7.2.5 Relación funcional Un supuesto importante en el modelo de regresión es el que considera que debe existir una relación funcional lineal entre cada regresor y la variable de respuestas. Pero, ¿qué debemos hacer si no se cumple esta relación lineal de la respuesta con alguno(s) de los regresor(es)? Primero, ya dijimos que este supuesto se evalúa realizando la gráfica de dispersión entre los residuos del modelo y los valores de la variable en cuestión. Cuando no hay una asociación lineal entre la respuesta y la covariable, generalmente este diagrama de dispersión muestra un patrón (tendencia) que sugiere qué tipo de transformación se debería hacer a la covariable para lograr linealidad con la respuesta. Debe quedar claro que la transformación puede realizarse a la variable explicativa o a la variable de respuesta. A muchos investigadores no les gusta transformar la respuesta porque argumentan que pierden interpretabilidad del modelo. Aunque esto puede ser cierto, existen transformaciones de la respuesta que pueden regresarse para interpretar el modelo con la respuesta original. Un problema asociado a esta identificación por parte del usuario, es que debe tener experiencia para asociar estas formas a una función analítica específica; hecho no necesariamente cierto. Por lo tanto, requiere de alguna herramienta técnica que pudiera auxiliarlo en esta labor. Un buen auxiliar, en el caso de que se crea que es necesario transformar la respuesta, es usar la llamada trasformación Box-Cox. 4.7.2.5.1 Transformación Box-Cox La transformación Box-Cox de la respuesta, es una función que sirve para normalizar la distribución del error, estabilizar la varianza de este error y mejorar la relación lineal entre \\(y\\) y las \\(X’s\\). Se define como: \\[y_i^{\\lambda} = \\left\\{ \\begin{array}{ll} \\frac{y_i^{\\lambda-1}}{\\lambda}, &amp; \\lambda \\neq 0;\\\\ ln(y_i), &amp; \\lambda=0 .\\end{array} \\right.\\] La siguiente tabla muestra el rango de valores de \\(\\lambda\\) que estarían asociados a una transformación analítica común. Rango \\(\\lambda\\) Transformación Asociada (-2.5, -1.5] \\(\\frac{1}{y^2}\\) (-1.5, -0.75] \\(\\frac{1}{y}\\) (-0.75, -0.25] \\(\\frac{1}{\\sqrt{y}}\\) (-0.25, 0.25] \\(ln(y)\\) (0.25, 0.75] \\(\\sqrt{y}\\) (0.75, 1.25] \\(y\\) (1.25, 2.5) \\(y^2\\) 4.7.2.5.2 Transformación Box-Tidwell Box y Tidwell implementan un proceso iterativo para encontrar la mejor transformación de las variables predictoras en el modelo de regresión lineal. Definiendo como \\(X_j^{\\gamma_j}\\) la correspondiente transformación Box-Tidwell de la variable \\(j\\). La tabla anterior para las transfomaciones analíticas de la respuesta, también aplican para estas transformaciones de los predictores. 4.8 Aplicación en R "],
["modelos-lineales-generalizados.html", "Capítulo 5 Modelos lineales generalizados 5.1 Regresión logística 5.2 Modelo de regresión logísitica simple 5.3 Modelo de regresión logísitica multiple 5.4 Regresión Multinomial 5.5 Modelos para conteos 5.6 GLM binomial negativa 5.7 Exponencial 5.8 Gamma", " Capítulo 5 Modelos lineales generalizados En el capítulo anterior exploramos el modelo básico que nos permite responder a la pregunta: ¿puede ser la variable de interés predicha por un conjunto de variables explicativas? Sin embargo, para poder utilizar dicho modelo, es necesario que la variable respuesta sea continua y cumpla las hipótesis estándar del modelo lineal (datos normales, varianza constante, etc.) Si la variable de interés es, por ejemplo binaria podemos ajustar un modelo de regresión logística en donde lo que predecimos son las probabilidades de la ocurrencia del evento medido con la variable binaria. En 1944, Berkson utilizó por primera vez la regresión logística como una forma de solucionar el problema de explicar una variable dicotómica a través de una variable continua. En este caso, la función logit hace que en lugar de trabajar con valores de la variable respuesta entre \\((0, 1)\\), trabajemos con una variable respuesta que puede tomar cualquier valor. No fue hasta 1972 cuando John Nelder introdujo los modelos lineales generalizados (GLM por sus siglas en inglés), de ahí que en general se considere a la regresión logística como algo distinto a los GLM, cuando lo que ocurre es que tanto la regresión múltiple como la logística, de Poisson, ordinal, etcétera, son casos particulares de un GLM. Para entender lo que es un GLM, volvamos al modelo de regresión múltiple, en este modelos suponemos que: \\[Y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}+\\epsilon\\] \\[E[Y]=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] Es decir, que existe una relación lineal entre las \\(X\\) y \\(E[Y]\\) (el valor medio de Y dado un cierto valor de las variables explicativas). Si las observaciones son binarias, entonces: \\[P(Y = 1) = p\\] \\[P(Y = 0) = 1-p\\] Y \\(E[Y] = 0\\times P(Y = 0) + 1 \\times P(Y = 1) = p\\), por lo tanto un modelo de regresión múltiple relacionará directamente la probabilidad de que ocurra un suceso con las variables explicativas, lo cual no es lo que se busca al ajustar un modelo de regresión lineal. Lo que hacen los GLM es establecer esa relación lineal no entre la media de la variable respuesta y los predictores, sino entre una función de la media de variable respuesta y los predictores, es decir: \\[g(E[Y])=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] Según de qué tipo sea la varible \\(Y\\), así será la función \\(g(\\cdot)\\). Entonces se puede decir que un GLM tiene 3 componentes: Componente aleatorio: La variable respuesta \\(Y\\) . Para poder utilizar un GLM, la distribución de \\(Y\\) ha de pertenecer a la familia exponencial, es decir, su función de densidad ha de poder escribirse como: \\[f(y;\\theta,\\phi)=exp\\{\\frac{y\\theta-b(\\theta)}{a(\\phi)}+c(y,\\phi)\\}\\] donde \\(a(\\cdot)\\), \\(b(\\cdot)\\) y \\(c(\\cdot)\\) son funciones específicas. El parámetro \\(\\theta\\) es lo que se llama parámetro canónico de localización y \\(\\phi\\) es un parámetro de dispersión. Pertenecen a la familia exponecial la distribución Normal, Bernouilli, Binomial, Poisson, Exponecial, Gamma, entre otras. Componente sistemático: Las variables predictoras \\(X_i \\ \\ i = 1,...,k\\) Función liga: La función que relaciona la media, \\(E[Y]\\), con las variables predictoras \\(X\\). En el caso del modelo de regresión ordinaria, \\(\\mu = \\nu\\), por lo tanto la función liga es la identidad. Hay muchas opciones par la función liga. La función liga canonica es una función que transforma la media en el parámetro canónico \\(\\theta\\) \\[g(E[Y])=\\theta\\] Entoces \\(g(\\cdot)\\) es la función liga canónica. La siguiente tabla muestra las funciones link canónicas para las distribuciones más comunes usadas en los GLMs: Distribución Liga canónica Normal \\(X \\beta = E[Y]\\) (identidad) Binomial \\(X \\beta = ln(\\frac{P}{1-P})\\) (logística) Poisson \\(X \\beta = ln(E[Y])\\) (logarítmica) Exponencial \\(X \\beta = \\frac{1}{E[Y]})\\) (recíproca) Gamma \\(X \\beta = \\frac{1}{E[Y]})\\) (recíproca) La diferencia que hay entre usar la función liga y usar una transformación, es que la función liga transforma la media, \\(E[Y]\\), y no los datos, \\(Y\\). Los GLM generalizan la regresión ordinaria de dos modos: permitiendo que la variable de respuesta \\(Y\\) tenga distribuciones diferentes a la normal y, por otro lado, incluyendo distintas funciones liga de la media, lo cual resulta muy útil para datos categóricos. 5.1 Regresión logística El modelo de regresión logistica es un GLM donde la distribución de probabilidad es Bernoulli o Binomial, y la función liga es el logit (ya que relaciona a la media, que en una Bernouilli es la probabilidad con el predictor lineal). Por lo tanto la estimación de los parámetros y los contrastes de hipótesis utilizan la teoría desarrollada para los GLMs. Estos modelos se utilizan cuando se desea conocer la relación entre Una variable dependiente cualitativa, dicotómica. Una o más variables explicativas independientes, llamadas covariables ya sean cualitativas o cuantitativas Por tanto, el objetivo de la regresión logística no es, como en regresión lineal, predecir el valor de la variable \\(Y\\) a partir de una o varias variables predictoras, sino que queremos predecir la probabilidad de que ocurra \\(Y\\) conocidos los valores de las variables \\(X_i&#39;s\\). Recordemos que las covariables cualitativas deben transformarse en las covariables cualitativas dicotómicas ficticias necesarias (variables dummy). De manera que al hacer esta transformación cada categoría de la variable entrará en el modelo de forma individual. 5.2 Modelo de regresión logísitica simple Para este modelo supondremos que nuestra respuesta, \\(Y\\), es explicada únicamente por una covariable, \\(X\\). Asumimos que la variable independiente \\(Y\\) está codificada como un 0 o un 1. Entonces, escribimos nuestro modelo como: \\[ln(\\frac{p}{1-p})=\\beta_{0}+\\beta_{1}X\\] \\[\\frac{p}{1-p}=e^{\\beta_{0}+\\beta_{1}X}\\] \\[p=e^{\\beta_{0}+\\beta_{1}X}-p\\times e^{\\beta_{0}+\\beta_{1}X}\\] \\[p(1+e^{\\beta_{0}+\\beta_{1}X})=e^{\\beta_{0}+\\beta_{1}X}\\] \\[p=\\frac{e^{\\beta_{0}+\\beta_{1}X}}{1+e^{\\beta_{0}+\\beta_{1}X}}\\] Y: \\[1-p=\\frac{1}{1+e^{\\beta_{0}+\\beta_{1}X}}\\] Los valores posibles de estas ecuaciones varían entre 0 y 1. Un valor cercano a 0 significa que es muy improbable que \\(Y\\) haya ocurrido, y un valor cercano a 1 significa que es muy probable que tuviese lugar. Similar a regresión lineal los valores de los parámetros se estiman utilizando el método de máxima verosimilitud que selecciona los coeficientes que hacen más probable que los valores observados ocurran. Para este análisis tenemos la razón de momios (odds ratio), que corresponde a la razón entre las posibilidades de respuesta. \\[OR=\\frac{\\frac{P(Y=1|X=1)}{1-P(Y=1|X=1)}}{\\frac{P(Y=1|X=0)}{1-P(Y=1|X=0)}}\\] El valor nulo para la razón de momios es el 1. Un \\(OR = 1\\) implica que las dos categorías comparadas son iguales. El valor mínimo posible es 0 y el máximo teóricamente posible es infinito. Un OR inferior a la unidad se interpreta como que el desenlace es menos frecuente en la categoría o grupo que se ha elegido como de interés con respecto al otro grupo o categoría de referencia. Un OR = 3 se interpreta como una ventaja 3 veces superior de una de las categorías \\(X = 1\\) relativamente a la otra categoría \\(X=0\\). 5.3 Modelo de regresión logísitica multiple Análogo a lo que observamos en los modelos de regresión lineal, el modelo de regresión logística se puede facilmente generalizar de un modelo simple a un múltiple. \\[ln(\\frac{p}{1-p})=\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_{2}+...\\beta_{k}X_{k}\\] \\[p=P(Y)=\\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_{2}+...\\beta_{k}X_{k})}}\\] De nuevo los valores posibles de estas ecuaciones varían entre 0 y 1. El propósito del análisis es predecir la probabilidad de que un evento \\(Y\\) ocurra para el \\(i-eismo\\) individuo. Para dicha \\(i-ésima\\) persona, \\(Y\\) será 0 (la respuesta no ocurre) o 1 (la respuesta ocurre), y el valor predicho, \\(\\mathbb{P}(Y)\\), tendrá un valor 0 (no hay probabilidad de que el resultado ocurra) o 1 (el resultado seguro que ocurre). 5.3.1 Regresión logística simple en R En el siguiente ejercicio se busca analizar si los productos salen o no defectuosos de acuerdo de la temperatura de la máquina que los produce. Los datos son los siguientes: temperatura &lt;-c(66,70,69,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58) defecto &lt;-c( 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1) Con ambos vectores construimos un dataframe: datos &lt;- data.frame(temperatura = temperatura, defecto = defecto) rm(temperatura, defecto) Resumen visual de los datos: colores &lt;- NULL colores[datos$defecto == 0] &lt;- &quot;green&quot; colores[datos$defecto == 1] &lt;- &quot;red&quot; plot(datos$temperatura, datos$defecto, pch = 21, bg = colores, xlab = &quot;Temperatura&quot;, ylab = &quot;Prob.defecto&quot;) legend(&quot;bottomleft&quot;, c(&quot;No defecto&quot;, &quot;Si defecto&quot;), pch = 21, col = c(&quot;green&quot;, &quot;red&quot;)) Creamos el modelo de regresión logística (modelo de regresión lineal generalizado y parametrizamos por familia binomial). reg &lt;- glm(defecto ~ temperatura, data = datos, family = binomial) Tabla resumen: summary(reg) ## ## Call: ## glm(formula = defecto ~ temperatura, family = binomial, data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.84513 -0.38010 -0.09632 -0.02831 2.41364 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 32.3381 17.6301 1.834 0.0666 . ## temperatura -0.5028 0.2643 -1.902 0.0571 . ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 24.0850 on 22 degrees of freedom ## Residual deviance: 9.8032 on 21 degrees of freedom ## AIC: 13.803 ## ## Number of Fisher Scoring iterations: 7 Creamos una nueva variable al dataframe de nuestros datos con las probabilidades de pertenencia a la clase 1 predichas por el modelo. datos$predict&lt;-reg$fitted.values Dibujamos la recta de probabilidad para cada una de las temperaturas: datos_probab &lt;- data.frame(temperatura = seq(50, 85, 0.1)) datos.predict &lt;- predict(reg, datos_probab, type = &quot;response&quot;) plot(datos$temperatura, datos$defecto, pch = 21, bg = colores, xlab = &quot;Temperatura&quot;, ylab = &quot;Prob.defecto&quot;) legend(&quot;bottomleft&quot;, c(&quot;No defecto&quot;, &quot;Si defecto&quot;), pch = 21, col = c(&quot;green&quot;, &quot;red&quot;)) lines(datos_probab$temperatura, datos.predict, col = &quot;blue&quot;, lwd = 2) Bondad de Ajuste: dev &lt;- reg$deviance nullDev &lt;- reg$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 14.28173 chidf &lt;- reg$df.null - reg$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0.0001573848 chisq.prob es el p-value de la estadística “modelChi”, para valores pequeños se dice que el modelo es estadisticamente significativo. 5.3.2 Ejercicio. Se tiene la siguiente tabla donde se eligen varios niveles de ronquidos y se ponen en relación con una enfermedad cardíaca. Se toman como puntuaciones relativas de ronquidos los valores \\(\\{0, 2, 4, 5\\}\\). Ronquido Presencia de enfermedad cardiaca Ausencia de enfermedad cardiaca Nunca 24 1355 Ocasional 35 603 Casi cada noche 21 192 Cada noche 30 224 Fijamos los niveles de manera ordinal: roncas &lt;- c(0, 2, 4, 5) frecuencia &lt;- cbind (SI=c(24 , 35, 21, 30) , NO=c (1355 ,603 , 192 , 224)) logit.irls &lt;- glm( frecuencia~roncas , family = binomial ( link = logit )) summary ( logit.irls )$ coefficients ## Estimate Std. Error z value ## (Intercept) -3.8662481 0.16621436 -23.260614 ## roncas 0.3973366 0.05001066 7.945039 ## Pr(&gt;|z|) ## (Intercept) 1.110885e-119 ## roncas 1.941304e-15 El modelo queda de la siguiente forma: \\[ln(\\frac{p}{1-p})=-3.87+.40X\\] Como \\(\\beta = 0.40 &gt; 0\\) entonces la probabilidad de ataque cardíaco aumenta cuando los niveles de ronquidos se incrementan. Con un nivel de ronquido \\(X=0\\) obtenemos: \\[ln(\\frac{p}{1-p})=-3.87\\] y p &lt;- exp(-3.87)/(1+exp(-3.87)) print(p) ## [1] 0.02043219 La probabilidad de tener la enfermedad es 2.04%. Mientras que si \\(X=5\\) obtenemos: \\[ln(\\frac{p}{1-p})=-3.87+.40*5=-1.87\\] y p &lt;- exp(-1.87)/(1+exp(-1.87)) print(p) ## [1] 0.1335417 La probabilidad de tener la enfermedad aumenta a 13.35%. Calcular la probabilidad de presentar la enfermedad cardíaca cuando el nivel de ronquido es Ocasional. ¿Cuántas veces más probable es la ocurrencia de la enfermedad cardíaca cuando el nivel de ronquidos es cada noche en comparación con ocasional? 5.3.3 Regresión logística múltiple en R Los datos corresponden a información sobre la respuesta a anuncios en redes sociales, se tienen datos de género, edad, salario de los individuos asi como la información de si se realizó o no la compra del producto anunciado. datos &lt;- read.csv(&quot;example_data/social_network_ads.csv&quot;, header = TRUE) Descriptivos básicos de los datos: str(datos) ## &#39;data.frame&#39;: 400 obs. of 5 variables: ## $ User.ID : int 15624510 15810944 15668575 15603246 15804002 15728773 15598044 15694829 15600575 15727311 ... ## $ Gender : chr &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; ... ## $ Age : int 19 35 26 27 19 27 27 32 25 35 ... ## $ EstimatedSalary: int 19000 20000 43000 57000 76000 58000 84000 150000 33000 65000 ... ## $ Purchased : int 0 0 0 0 0 0 0 1 0 0 ... summary(datos) ## User.ID Gender Age ## Min. :15566689 Length:400 Min. :18.00 ## 1st Qu.:15626764 Class :character 1st Qu.:29.75 ## Median :15694342 Mode :character Median :37.00 ## Mean :15691540 Mean :37.66 ## 3rd Qu.:15750363 3rd Qu.:46.00 ## Max. :15815236 Max. :60.00 ## EstimatedSalary Purchased ## Min. : 15000 Min. :0.0000 ## 1st Qu.: 43000 1st Qu.:0.0000 ## Median : 70000 Median :0.0000 ## Mean : 69743 Mean :0.3575 ## 3rd Qu.: 88000 3rd Qu.:1.0000 ## Max. :150000 Max. :1.0000 Resumen de cuantos elementos hay en cada caso para la variable de compra del producto: table(datos$Purchased) ## ## 0 1 ## 257 143 Creación del modelo de regresión logística(modelo de regresión lineal generalizado y parametrizamos por binomial) aplicando a datos[,-1] estamos dejando fuera la variable “User.ID”: modelo &lt;- glm(Purchased ~ ., data = datos[,-1], family = binomial) summary(modelo) ## ## Call: ## glm(formula = Purchased ~ ., family = binomial, data = datos[, ## -1]) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9109 -0.5218 -0.1406 0.3662 2.4254 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.278e+01 1.359e+00 -9.405 &lt; 2e-16 ## GenderMale 3.338e-01 3.052e-01 1.094 0.274 ## Age 2.370e-01 2.638e-02 8.984 &lt; 2e-16 ## EstimatedSalary 3.644e-05 5.473e-06 6.659 2.77e-11 ## ## (Intercept) *** ## GenderMale ## Age *** ## EstimatedSalary *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 521.57 on 399 degrees of freedom ## Residual deviance: 275.84 on 396 degrees of freedom ## AIC: 283.84 ## ## Number of Fisher Scoring iterations: 6 Bondad de ajuste: dev &lt;- modelo$deviance nullDev &lt;- modelo$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 245.7297 chidf &lt;- modelo$df.null - modelo$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0 El modelo queda de la siguiente forma: \\[ln(\\frac{p}{1-p})=-12.78+.33*Genero+.237*Edad+0.000036*Salario\\] La prueba Ji-Cuadrada para la significancia del modelo da un p-value de cero, por lo tanto el modelo propuesto con las variables de género, edad y salario resulta ser significativo. Agregamos a los datos la columna de probabilidad de compra calculada con nuestro modelo: datos &lt;- cbind(datos, modelo$fitted.values) plot(modelo$fitted.values, col = as.factor(datos$Gender), main=&quot;Probabilidad de compra por género&quot;, ylab=&quot;probabilidad de compra&quot;) plot(modelo$fitted.values, datos$Age, col = as.factor(datos$Gender), main=&quot;Probabilidad de compra vs edad coloreado por edad&quot;, xlab=&quot;probabilidad de compra&quot;,ylab=&quot;edad&quot;) plot(modelo$fitted.values, datos$EstimatedSalary, main=&quot;Probabilidad de compra vs salario&quot;, xlab=&quot;probabilidad de compra&quot;,ylab=&quot;salario&quot;) Observamos que de las 3 variables consideradas, la que parece tener más efecto en la probabilidad de compra es la edad. Veamos ahora algunas probabilidades. Según nuestro modelo ¿cuál es la probabilidad de compra de una mujer 55 años con salario de 80,000? 1/(1+exp(-(-12.78+.33*0+.237*55+0.000036*80000))) ## [1] 0.9583136 ¿Y el de una mujer de 35 años con el mismo salario? 1/(1+exp(-(-12.78+.33*0+.237*35+0.000036*80000))) ## [1] 0.167284 Mientrás que la probabilidad de compra de una mujer de 55 años con salario de 80,000 es \\(96\\%\\) una mujer con el mismo salario pero 20 años mas joven tendrá una probabilidad de sólo \\(17\\%\\) de comprar el producto anunciado. Calcular la probailidad de que un hombre de 45 años con salario de 50,000 compre el producto anunciado. ¿Cuántas veces más probable es que un hombre de 45 años con salario de 100,000 compre el producto que un hombre de la misma edad pero con salario de 50,000? 5.4 Regresión Multinomial Hasta ahora hemos revisado el caso en el que la variable respuesta era dicotómica. Ahora nos centramos en el caso en el que la variable de interés tiene más de dos categorías, por ejemplo, afiliación política; resultado de un partido de fútbol; marcas de teléfonos celulares, etc. Por simplicidad, se ilustrará la metodología para el caso de tres categorías, ya que la generalización a más de tres es inmediata. Supongamos que codificamos las tres categorías de la variable respuesta como 0, 1 y 2. En el caso de regresión logística, el logit es: \\[ln(\\frac{p}{1-p})=ln(\\frac{P[Y=1]}{P[Y=0]})\\] Ahora el modelo necesita dos funciones logit ya que tenemos tres categorías, y necesitamos decidir que categorías queremos comparar. Lo más general es utilizar \\(Y = 0\\) como referencia y formar logits comparándola con \\(Y = 1\\) y \\(Y = 2\\). Supongamos que tenemos k variables explicativas, entonces: \\[ln(\\frac{P[Y=1]}{P[Y=0]})=\\beta_{10}+\\beta_{11}X_1+\\beta_{12}X_{2}+...\\beta_{1k}X_{k}\\] \\[ln(\\frac{P[Y=2]}{P[Y=0]})=\\beta_{20}+\\beta_{21}X_1+\\beta_{22}X_{2}+...\\beta_{2k}X_{k}\\] Y ahora tenemos el doble de coeficientes que en el caso de regresión logística. Las probabilidades se calcularán como: \\[P[Y=0|X]=\\frac{1}{1+e^{g_1(X)}+e^{g_2(X)}}\\] \\[P[Y=1|X]=\\frac{e^{g_1(X)}}{1+e^{g_1(X)}+e^{g_2(X)}}\\] \\[P[Y=2|X]=\\frac{e^{g_2(X)}}{1+e^{g_1(X)}+e^{g_2(X)}}\\] \\[g_1(X)=\\beta_{10}+\\beta_{11}X_1+\\beta_{12}X_{2}+...\\beta_{1k}X_{k}\\] \\[g_2(X)=\\beta_{20}+\\beta_{21}X_1+\\beta_{22}X_{2}+...\\beta_{2k}X_{k}\\] 5.4.1 Regresión Multinomial en R Datos “rh_satisfaction” corresponde a información del área de recursos humanos de una empresa que mide el nivel de satisfacción de sus empleados (en escala de 1 a 5). También se tiene algunas características de los empleados (edad, area, salario, etc). El primer análisis a realizar que queremos analizar es la satisfacción de los empleados a través del salario y la edad. Para ello aplicaremos primero una regresión para cada variable por separado y posteriormente un modelo con ambas. La función para realizar la regresión multinomial es multinom, la cual es similar a la de los comandos para regresión logística. Esta función está en la librería nnet. Cargamos la libreria y los datos: library(nnet) datos &lt;- read.csv(&quot;example_data/hr_satisfaction.csv&quot;, header=TRUE) Ajustemos el modelo de regresión multinominal para la satisfacción de los empleados con la variable de salario. multi1 &lt;- multinom(satisfied~salary, data = datos) ## # weights: 15 (8 variable) ## initial value 804.718956 ## iter 10 value 801.134736 ## final value 800.932439 ## converged print(multi1) ## Call: ## multinom(formula = satisfied ~ salary, data = datos) ## ## Coefficients: ## (Intercept) salary ## 2 -0.1080096 3.069240e-06 ## 3 -0.2193869 2.708006e-06 ## 4 -0.1515928 6.556397e-06 ## 5 -0.4102788 1.026791e-05 ## ## Residual Deviance: 1601.865 ## AIC: 1617.865 Tenemos entonces 4 ecuaciones y el análisis está tomando la categoría 1 como la base para construir los modelos logit como vimos en la teoría. Uno de los usos de este modelo puede ser el calcular probabilidades especifícas para ciertos niveles de satisfacción que sean de interés. Por ejemplo la probabilidad de que un empleado con salario de 50,000 (la media es 50,416.06) esté muy satisfecho en la empresa es \\(21\\%\\) y se calcula de la siguiente forma: s &lt;- 50000 denom &lt;- 1+exp(-0.108+.0000031*s)+exp(-0.219+.0000027*s) + exp(-0.152+.0000066*s)+exp(-0.410+.0000103*s) exp(-0.410+.0000103*s)/denom ## [1] 0.2106376 Mientras que la probabilidad de que este empleado esté completamente insatisfecho será: 1/denom ## [1] 0.1896422 Ahora ajustemos el modelo de regresión multinominal para la satisfacción de los empleados con la variable de edad. multi2 &lt;- multinom(satisfied~age, data = datos) ## # weights: 15 (8 variable) ## initial value 804.718956 ## iter 10 value 798.978325 ## final value 798.956315 ## converged multi2 ## Call: ## multinom(formula = satisfied ~ age, data = datos) ## ## Coefficients: ## (Intercept) age ## 2 -0.0680823 0.002780933 ## 3 -0.4380887 0.008846122 ## 4 0.6359941 -0.011962449 ## 5 1.1775667 -0.028143304 ## ## Residual Deviance: 1597.913 ## AIC: 1613.913 La probabilidad de que un empleado de edad 40 años esté muy satisfecho en la empresa es 20% y se calcula de la siguiente forma: e &lt;- 40 denom &lt;- 1+exp(-0.068+.0028*e)+exp(-0.438+.0088*e) + exp(0.636-.0120*e)+exp(1.178-.0281*e) exp(1.178-.0281*e)/denom ## [1] 0.2034909 Finalmente ajustamos el modelo de regresión con las variables explicativas de salario y edad. multi3 &lt;- multinom(satisfied~salary+age, data = datos) ## # weights: 20 (12 variable) ## initial value 804.718956 ## iter 10 value 799.192997 ## final value 797.372424 ## converged multi3 ## Call: ## multinom(formula = satisfied ~ salary + age, data = datos) ## ## Coefficients: ## (Intercept) salary age ## 2 -0.2315138 3.109419e-06 0.003091909 ## 3 -0.5872992 2.836453e-06 0.009138518 ## 4 0.2954069 6.368488e-06 -0.011320961 ## 5 0.6390753 9.838360e-06 -0.027078238 ## ## Residual Deviance: 1594.745 ## AIC: 1618.745 Con este modelo entonces podemos calcular la probabilidad de que un empleado de edad 25 años esté muy satisfecho en la empresa es \\(27\\%\\) y se calcula de la siguiente forma: e &lt;- 25 s &lt;- 50000 denom &lt;- 1+exp(-0.232+.0000031*s+0.003*e)+exp(-0.587+.0000028*s+0.009*e) + exp(0.295+.0000063*s-0.011*e)+exp(0.640+.0000098*s-0.027*e) exp(0.640+.0000098*s-0.027*e)/denom ## [1] 0.2730235 Mientras que la probabilidad de estar completamente insatisfecho será: 1/denom ## [1] 0.1732192 Calcular la probabilidad de un empleado de 50 años con salario de 65,000 asigne un nivel de satisfacción de 3. Ajustar un modelo con las variables de salario y educación. Calcular la probabilidad de que un empleado con estudios de postgrado con salario de 65,000 este completamente satisfecho con la empresa. Si quisiera analizar la satisfacción por departamento ¿qué procedimiento propondría usar? 5.5 Modelos para conteos En muchos casos las variables respuesta son conteos, y en ocasiones estos recuentos aparecen al resumir en tablas de contingencia otras variables. Hay cuatro razones por las que sería erroneo utlizar un modelo de regresión normal para datos de conteo : Puede dar lugar a predicciones negativas. La varianza de la variable respuesta no es independiente de la media. Los errores no siguen una distribución Normal. Los ceros que aparecen en la variable respuesta dan problemas a la hora de transformar la variables. Sin embargo, si la variable es de conteo pero los datos toman valores elevados, entonces si podría ser posible utilizar la distribución Normal. El modelo más simple para cuando la variable de respuesta son recuentos es asumir que el componente aleatorio \\(Y\\) sigue una distribución de Poisson. Esta distribución es unimodal y su propiedad más destacada es que la media y la varianza coinciden. \\[E(Y)=Var(Y)=\\mu\\] De modo que cuando el número de recuentos es mayor en media, también tienden a tener mayor variabilidad. La principal diferencia entre la distribución de Poisson y la Binomial, es que, aunque ambas cuentan el número de veces que ocurre algo, en la distribución de Poisson no sabemos cuántas veces no ocurrio, y en la Binomial sí lo sabemos. Supongamos que estamos haciendo un estudio sobre cuantas larvas de insectos hay en ciertos árboles, los datos de los que disponemos corresponden al número de larvas por hoja \\((Y)\\). Habrá hojas que no tengan ninguna, y otras que tenga hasta 5 ó 6. Si el número medio de larvas por hoja es \\(\\mu\\), la probabilidad de observar \\(y_0\\) larvas por hoja viene dada por la siguiente ecuación: \\[P(y_0)=\\frac{e^{\\mu}\\mu^{y_0}}{y_0!}\\] Donde \\(\\mu\\) se puede aproximar con \\(\\mu=np\\), para \\(n\\) grande y \\(p\\) pequeño. Es decir, que una distribución de Poisson se obtiene a partir de una Binomial con \\(p\\) pequeño y \\(n\\). Entonces el modelo GLM para los conteos se basará en modelar la relación entre la media muestral \\(\\mu\\) y las variables explicativas. \\[\\mu=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] Por las características de la variable (son conteos) buscamos que la parte derecha de la ecuación sólo tome valores positivos. Por esta razón habitualmente se usa el logaritmo de la media como la función liga, de modo que el modelo log-lineal se puede expresar como: \\[log(\\mu) = log(E[Y])=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] de modo que al despejar \\(\\mu\\) obtenemos: \\[\\mu = e^{\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}}\\] 5.5.1 Sobredispersión en GLM Poisson En una distribución de Poisson, la media y la varianza son iguales, pero en la práctica, los datos de conteo muestran mayor variabilidad de la que se espera en un modelo binomial o Poisson. En el caso de este último, es común que la varianza sea mucho mayor que la media \\(\\mathbb{V}\\left(Y\\right)&gt;&gt;\\mathbb{E}\\left(Y\\right)=\\mu\\), este fenómeno se conoce como sobredispersión. Por ejemplo, podemos suponer que cada individuo tienen igual probabilidad de padecer cierta enfermedad; no obstante, siendo más realistas, es claro que que estas probabilidades varían debido a factores genéticos, de salubridad y de localización geográfica, entre otros, propiciando mayor variabilidad sobre el número de sujetos enfermos en un periodo determinado, que los que puede predecir el modelo Poisson asociado. Una forma de medir la sobredispersión en los datos es ajustando una distribución quasipoisson, la cual ajustará el modelo con distribución Poisson pero no asumirá varianza igual a la media y calculará el parámetro de dispersión. 5.5.2 GLM Poisson R Entre los cangrejos cacerola se sabe que cada hembra tiene un macho en su nido, pero puede tener más machos concubinos. Considerando que deseamos relacionar la variable respuesta, el número de concubinos, con las variables explicativas son: color, estado de la espina central, peso y anchura del caparazón, procederemos a ajustar un modelo lineal a los datos. En un primer análisis sólo consideramos la anchura del caparazón como variable explicativa. tabla &lt;- read.csv ( &quot;http://www.hofroe.net/stat557/data/crab.txt&quot; , header =TRUE, sep = &quot;\\t&quot; ) plot.tabla &lt;- aggregate (rep (1,nrow (tabla)), list (Sa= tabla$Satellite , W=tabla$Width), sum) plot(tabla$Width,tabla$Satellite, xlab=&quot;Ancho de caparazón&quot;, ylab=&quot;Número de Concubinos&quot;, bty=&quot;L&quot;, axes = FALSE, type =&quot;n&quot;) axis (2, at =1:15) axis (1, at=seq (20 , 34, 2)) text (y= plot.tabla$Sa , x= plot.tabla$W, labels = plot.tabla$x) Ahora agruparemos los datos según cortes específicos en la variable de ancho de caparazón (Width) para poder ver visualmente el ajuste del modelo de regresión. Discretizamos el ancho del caparazón tabla$W.fac = cut( tabla$Width , breaks =c(0, seq (23.25 , 29.25), Inf )) Calculamos el numero medio de concubinos para cada categoria según el ancho del caparazón: plot.y &lt;- aggregate ( tabla$Satellite , by= list (W= tabla$W.fac), mean )$x Determinamos la media del ancho del caparazon por categoría: plot.x &lt;- aggregate ( tabla$Width , by = list (W= tabla$W.fac ), mean )$x Representamos las medias de anchura y la media del numero de concubinos: plot (x = plot.x , y = plot.y , ylab = &quot; Numero de Concubinos &quot;, xlab = &quot; Anchura (cm) &quot;, bty = &quot;L&quot;, axes = FALSE, type = &quot;p&quot;, pch = 16) axis(2, at =0:5) axis(1, at=seq (20 , 34, 2)) En este caso podemos observar una relación lineal entre la media del número de concubinos y la media del ancho del caparazón por categoría. Ahora ajustamos el modelo lineal entre el número de concubinos y el ancho del caparazón definiendo una función de distribución Poisson. m1 &lt;- glm(Satellite~Width , family = poisson , data = tabla ) summary( m1 ) ## ## Call: ## glm(formula = Satellite ~ Width, family = poisson, data = tabla) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8526 -1.9884 -0.4933 1.0970 4.9221 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.30476 0.54224 -6.095 1.1e-09 *** ## Width 0.16405 0.01997 8.216 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 632.79 on 172 degrees of freedom ## Residual deviance: 567.88 on 171 degrees of freedom ## AIC: 927.18 ## ## Number of Fisher Scoring iterations: 6 Recordando que para medir el ajuste del modelo utilizamos la devianza, en donde la devianza nula es la desviación para el modelo que no depende de ninguna variable. Miéntras que la devianza residual es la diferencia entre la desviación del modelo que no depende de ninguna variable menos la del modelo que incluye a la variable “Width”. La diferencia entre ambas se distribuirá como una distribución Ji-cuadrada con 1 grado de libertad. Bondad de Ajuste: dev &lt;- m1$deviance nullDev &lt;- m1$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 64.91309 chisq.prob &lt;- 1 - pchisq(modelChi, 1) chisq.prob ## [1] 7.771561e-16 Se puede rechazar claramente la hipótesis nula. Por lo que concluimos que hay un aportación significativa de la variable de ancho del caparazón al modelo del número de concubinos. El modelo queda de la siguiente forma: \\[ln(\\mu)=-3.30+0.16*Width\\] Y los valores estimados por el modelo se pueden encontrar en m1$fitted.values. Y se puede predecir la media del números de concubinos para un valor de ancho de caparazón dado, por ejemplo, para una anchura igual a 26.3: predict.glm ( m1 , type = &quot;response&quot;, newdata = data.frame( Width = 26.3 )) ## 1 ## 2.744581 Entonces siguiendo este modelo, decimos que los cangrejos hembra con ancho de caparazón del 26.3cm tienen en promedio 2.7 parejas. Podriamos ajustar ahora el modelo con las variables de peso, color y estado de la espina. Se deja al lector el ejercicio de utilizando el algoritmo stepAIC encontrar el mejor modelo para el número de concubinos. 5.6 GLM binomial negativa Una distribución que puede usarse como alternativa a una Poisson es la . Dado que su varianza es más grande que su media, constituye una excelente alternativa para modelar datos de conteo sobredispersos, que son muy comunes en aplicaciones reales. Si una variable aleatoria \\(Y\\) se distribuye como una binomial negativa, entonces la función de probabilidad es: \\[P(y|k,\\mu)=\\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}\\left( \\frac{k}{\\mu+k} \\right)^k\\left( 1-\\frac{k}{\\mu+k} \\right)^y\\] con \\(y=0,1,2,...\\) donde \\(k\\) y \\(\\mu\\) son los parámetros de la distribución y se tiene que \\[E(Y)=\\mu\\] \\[Var(Y)=\\mu+\\frac{\\mu^2}{k}\\] El parámetro \\(\\frac{1}{k}\\) es un parámetro de dispersión, de modo que si \\(\\frac{1}{k} \\rightarrow 0\\) entonces \\(Var(Y)\\rightarrow \\mu\\) y la distribución binomial negativa converge a una distribución Poisson. Por otro lado, para un valor fijo de \\(k\\) esta distribución pertenece a la familia exponencial natural, de modo que se puede definir un modelo GLM binomial negativo. En general, se usa una función liga de tipo logaritmo. La regresión binomial negativa se puede utilizar para datos sobredispersos de recuentos, es decir cuando la varianza condicional es mayor que la media condicional. Se puede considerar como una generalización de la regresión de Poisson, ya que tiene su misma estructura de medias y además un parámetro adicional para el modelo de sobredispersión. Si la distribución condicional de la variable observada es más dispersa, los intervalos de confianza para la regresión binomial negativa es probable que sean más estrechos que los correspondientes a un modelo de regresión de Poisson. 5.6.1 GLM Binomial Negativa en R Usando los mismos datos de los cangrejos ahora ajustaremos un modelo con distribución binomial negativa: require( MASS ) m2 &lt;- glm.nb(Satellite~Width , data = tabla ) summary( m2 ) ## ## Call: ## glm.nb(formula = Satellite ~ Width, data = tabla, init.theta = 0.90456808, ## link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7798 -1.4110 -0.2502 0.4770 2.0177 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.05251 1.17143 -3.459 0.000541 *** ## Width 0.19207 0.04406 4.360 1.3e-05 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(0.9046) family taken to be 1) ## ## Null deviance: 213.05 on 172 degrees of freedom ## Residual deviance: 195.81 on 171 degrees of freedom ## AIC: 757.29 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 0.905 ## Std. Err.: 0.161 ## ## 2 x log-likelihood: -751.291 En este caso el modelo queda de la siguiente forma: \\[ln(\\mu)=-4.05+0.19*Width\\] Observamos que tanto la devianza como el coeficiente de Akaike son menores en comparación con el modelo que usa una distribución Poisson. Adicionalmente habiamos visto que los datos indicaban que la varianza era poco mas de tres veces la media. Por estas razones para este ejemplo se puede decir que un modelo con distribución binomial negativa es mas adecuado. 5.6.2 Ejercicio La base de datos “productos.csv” contiene información sobre el número de productos financieros que posee cada cliente, adicional a esa información se tiene la edad, género, número de ofertas, antigüedad y el número de créditos que tiene. Deseamos obtener información respecto a la relación entre el número de productos y el resto de las variables que nos permitan incrementar la venta de productos. Ajustar un modelo al número de productos financieros en función de la edad del cliente. Asumiendo una distribución Poisson Calcular con la distribución quasiPoisson el valor del parámetro de dispersión para estos datos. ¿Qué puede concluir de este modelo? ¿Valdrá la pena cambiar a una distribución binomial negativa? Encontrar el mejor modelo que explique el número de productos financieros a partir de todas las variables disponibles. ¿Qué recomendación puede dar al área de mercadotecnia que esta buscando incrementar el número de productos por cliente? 5.7 Exponencial Se dice que la variable respuesta \\(Y\\) es de tipo Exponencial cuando hemos observado el tiempo transcurrido hasta que ocurre un evento de interés como resultado de un conjunto de variables predictoras que pueden ser de tipo numérico o categórico. En este caso similar al caso de conteos también se asume que \\(Y\\) sólo toma valores positivos y es continua. En este caso la función liga utilizada es el recíproco, de modo que el modelo lineal se puede expresar como: \\[\\frac{1}{E[Y]}=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] 5.7.1 GLM Exponencial en R Los datos de este ejemplo corresponden a la información de un banco qué busca entender la transición de sus clientes hacia un estado de alto riesgo de crédito. Con la finalidad de entender mejor el momento en que el cliente pasa a un estado de alto riesgo crediticio por falta de pago, el banco hizo un seguimiento a una muestra representativa de sus clientes. Deseamos identificar cuáles de las características de los clientes influyen en la transición de un estado de bajo riesgo a alto riesgo. datos &lt;- read.csv(&quot;example_data/default.csv&quot;, header=TRUE) Breve análisis exploratorio: str(datos) ## &#39;data.frame&#39;: 1000 obs. of 17 variables: ## $ duration : int 6 48 12 42 24 36 24 36 12 30 ... ## $ credit_history : chr &quot;critical/other existing credit&quot; &quot;existing paid&quot; &quot;critical/other existing credit&quot; &quot;existing paid&quot; ... ## $ purpose : chr &quot;radio/tv&quot; &quot;radio/tv&quot; &quot;education&quot; &quot;furniture/equipment&quot; ... ## $ credit_amount : int 1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ... ## $ installment_commitment: int 4 2 2 2 3 2 3 2 2 4 ... ## $ personal_status : chr &quot;male single&quot; &quot;female div/dep/mar&quot; &quot;male single&quot; &quot;male single&quot; ... ## $ residence_since : int 4 2 3 4 4 4 4 2 4 2 ... ## $ property_magnitude : chr &quot;real estate&quot; &quot;real estate&quot; &quot;real estate&quot; &quot;life insurance&quot; ... ## $ age : int 67 22 49 45 53 35 53 35 61 28 ... ## $ other_payment_plans : chr &quot;none&quot; &quot;none&quot; &quot;none&quot; &quot;none&quot; ... ## $ housing : chr &quot;own&quot; &quot;own&quot; &quot;own&quot; &quot;for free&quot; ... ## $ existing_credits : int 2 1 1 1 2 1 1 1 1 2 ... ## $ job : chr &quot;skilled&quot; &quot;skilled&quot; &quot;unskilled resident&quot; &quot;skilled&quot; ... ## $ num_dependents : int 1 1 2 2 2 2 1 1 1 1 ... ## $ own_telephone : chr &quot;yes&quot; &quot;none&quot; &quot;none&quot; &quot;none&quot; ... ## $ foreign_worker : chr &quot;yes&quot; &quot;yes&quot; &quot;yes&quot; &quot;yes&quot; ... ## $ default : int 0 1 0 0 1 0 0 0 0 1 ... summary(datos) ## duration credit_history purpose ## Min. : 4.0 Length:1000 Length:1000 ## 1st Qu.:12.0 Class :character Class :character ## Median :18.0 Mode :character Mode :character ## Mean :20.9 ## 3rd Qu.:24.0 ## Max. :72.0 ## credit_amount installment_commitment ## Min. : 250 Min. :1.000 ## 1st Qu.: 1366 1st Qu.:2.000 ## Median : 2320 Median :3.000 ## Mean : 3271 Mean :2.973 ## 3rd Qu.: 3972 3rd Qu.:4.000 ## Max. :18424 Max. :4.000 ## personal_status residence_since property_magnitude ## Length:1000 Min. :1.000 Length:1000 ## Class :character 1st Qu.:2.000 Class :character ## Mode :character Median :3.000 Mode :character ## Mean :2.845 ## 3rd Qu.:4.000 ## Max. :4.000 ## age other_payment_plans ## Min. :19.00 Length:1000 ## 1st Qu.:27.00 Class :character ## Median :33.00 Mode :character ## Mean :35.55 ## 3rd Qu.:42.00 ## Max. :75.00 ## housing existing_credits ## Length:1000 Min. :1.000 ## Class :character 1st Qu.:1.000 ## Mode :character Median :1.000 ## Mean :1.407 ## 3rd Qu.:2.000 ## Max. :4.000 ## job num_dependents own_telephone ## Length:1000 Min. :1.000 Length:1000 ## Class :character 1st Qu.:1.000 Class :character ## Mode :character Median :1.000 Mode :character ## Mean :1.155 ## 3rd Qu.:1.000 ## Max. :2.000 ## foreign_worker default ## Length:1000 Min. :0.0 ## Class :character 1st Qu.:0.0 ## Mode :character Median :0.0 ## Mean :0.3 ## 3rd Qu.:1.0 ## Max. :1.0 Selección de observaciones en donde si hubo falta de pago: datos &lt;- datos[datos$default==1,] Construcción de la variable del tiempo transcurrido al default: datos$meses_default &lt;- datos$duration-datos$installment_commitment hist(datos$meses_default) Lo primero será ajustar un modelo que explique el tiempo en que occure la falta de pago en función de la variable tenencia de teléfono propio. N.B. La distribución exponencial es un caso particular de la distribución gamma y por esa razón en la función glm sólo aparece la familia Gamma m1 &lt;- glm(meses_default ~ own_telephone, data = datos, family = Gamma) summary(m1) ## ## Call: ## glm(formula = meses_default ~ own_telephone, family = Gamma, ## data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7845 -0.6226 -0.1556 0.2734 1.5773 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.049933 0.002235 22.34 &lt; 2e-16 ## own_telephoneyes -0.009344 0.003234 -2.89 0.00414 ## ## (Intercept) *** ## own_telephoneyes ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.374624) ## ## Null deviance: 123.08 on 299 degrees of freedom ## Residual deviance: 120.01 on 298 degrees of freedom ## AIC: 2327.4 ## ## Number of Fisher Scoring iterations: 5 Bondad de ajuste: dev &lt;- m1$deviance nullDev &lt;- m1$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 3.07052 chidf &lt;- m1$df.null - m1$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0.07972398 La prueba de bondad de ajuste da una probabilidad de \\(7\\%\\) que es menor al \\(10\\%\\) y puede considerarse suficiente para decir que el modelo con esta variable si aporta información estadísticamente significativa. El modelo queda de la siguiente forma: \\[\\frac{1}{E(Y)}=0.050-0.009*TeléfonoPropio\\] Con este modelo podriamos calcular, en promedio, en cuantos meses de iniciado el crédito se presentará el no pago en función de si el teléfono es propio o no. table(m1$fitted.values) %&gt;% knitr::kable() Var1 Freq 20.0267380435668 187 24.6371681425079 113 Recordando que el objetivo del ejercicio es entender mejor la relación entre las caracteristicas de los clientes y su riesgo crediticio nos podemos plantear las siguientes preguntas: ¿A qué se debe que sólo tenemos 2 estimaciones de tiempo? ¿Con estos datos un modelo en donde la variable explicativa es la edad, es mejor? ¿Cómo podemos mejorar el modelo y/o las estimaciones de los tiempo de falta de pago? 5.8 Gamma Como se mencionó en la sección anterior las distribución exponencial es un caso particular de la distribución gamma. En general cuando la variable de respuesta es de tipo numérico, pero sólo puede tomar valores positivos de forma asimétrica, es decir, se encuentra concentrada en un conjunto de valores y su frecuencia disminuye cuando aumenta el valor de la respuesta, se dice que la variable se distribuye gamma. La distribución gamma sólo está definida para valores mayores a cero, por lo que si la variable de respuesta \\(Y\\) toma valores negativos o cero, para poder utilizar esta distribución será necesario realizar una transformación de los datos, sumando una constante lo suficientemente grande que haga todas las obervaciones positivas. Análogo al caso exponencial, la función liga utilizada es el recíproco, de modo que el modelo lineal se expresa como: \\[\\frac{1}{E[Y]}=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] 5.8.1 GLM Gamma en R La base de datos que utilizaremos para este ejemplo, corresponde a las reclamaciones recibidas por cierta aseguradora para su producto de seguro de automóviles. Los siniestros corresponden al primer trimestre de 2015 y se registraron además del monto total reclamado, características del asegurado asi como del siniestro. La aseguradora busca modelar el monto total reclamado en función de las otras variables medidas para con ello mejorar su cálculo de primas e identificación de grupos que deberian tener sobreprima. Cargamos los datos y realizamos un breve análisis exploratorio de los datos. datos &lt;- read.csv(&quot;example_data/insurance_claims.csv&quot;, header = TRUE) str(datos) ## &#39;data.frame&#39;: 1000 obs. of 22 variables: ## $ months_as_customer : int 328 228 134 256 228 256 137 165 27 212 ... ## $ age : int 48 42 29 41 44 39 34 37 33 42 ... ## $ policy_number : int 521585 342868 687698 227811 367455 104594 413978 429027 485665 636550 ... ## $ policy_state : chr &quot;OH&quot; &quot;IN&quot; &quot;OH&quot; &quot;IL&quot; ... ## $ policy_deductable : int 1000 2000 2000 2000 1000 1000 1000 1000 500 500 ... ## $ policy_annual_premium : num 1407 1197 1413 1416 1584 ... ## $ insured_sex : chr &quot;MALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; &quot;FEMALE&quot; ... ## $ insured_education_level : chr &quot;MD&quot; &quot;MD&quot; &quot;PhD&quot; &quot;PhD&quot; ... ## $ insured_occupation : chr &quot;craft-repair&quot; &quot;machine-op-inspct&quot; &quot;sales&quot; &quot;armed-forces&quot; ... ## $ insured_hobbies : chr &quot;sleeping&quot; &quot;reading&quot; &quot;board-games&quot; &quot;board-games&quot; ... ## $ insured_relationship : chr &quot;husband&quot; &quot;other-relative&quot; &quot;own-child&quot; &quot;unmarried&quot; ... ## $ incident_date : chr &quot;25/01/2015&quot; &quot;21/01/2015&quot; &quot;22/02/2015&quot; &quot;10/01/2015&quot; ... ## $ incident_type : chr &quot;Single Vehicle Collision&quot; &quot;Vehicle Theft&quot; &quot;Multi-vehicle Collision&quot; &quot;Single Vehicle Collision&quot; ... ## $ collision_type : chr &quot;Side Collision&quot; &quot;?&quot; &quot;Rear Collision&quot; &quot;Front Collision&quot; ... ## $ incident_severity : chr &quot;Major Damage&quot; &quot;Minor Damage&quot; &quot;Minor Damage&quot; &quot;Major Damage&quot; ... ## $ authorities_contacted : chr &quot;Police&quot; &quot;Police&quot; &quot;Police&quot; &quot;Police&quot; ... ## $ incident_hour_of_the_day : int 5 8 7 5 20 19 0 23 21 14 ... ## $ number_of_vehicles_involved: int 1 1 3 1 1 3 3 3 1 1 ... ## $ auto_make : chr &quot;Saab&quot; &quot;Mercedes&quot; &quot;Dodge&quot; &quot;Chevrolet&quot; ... ## $ auto_model : chr &quot;92x&quot; &quot;E400&quot; &quot;RAM&quot; &quot;Tahoe&quot; ... ## $ auto_year : int 2004 2007 2007 2014 2009 2003 2012 2015 2012 1996 ... ## $ total_claim_amount : int 71610 5070 34650 63400 6500 64100 78650 51590 27700 42300 ... summary(datos) ## months_as_customer age policy_number ## Min. : 0.0 Min. :19.00 Min. :100804 ## 1st Qu.:115.8 1st Qu.:32.00 1st Qu.:335980 ## Median :199.5 Median :38.00 Median :533135 ## Mean :204.0 Mean :38.95 Mean :546239 ## 3rd Qu.:276.2 3rd Qu.:44.00 3rd Qu.:759100 ## Max. :479.0 Max. :64.00 Max. :999435 ## policy_state policy_deductable ## Length:1000 Min. : 500 ## Class :character 1st Qu.: 500 ## Mode :character Median :1000 ## Mean :1136 ## 3rd Qu.:2000 ## Max. :2000 ## policy_annual_premium insured_sex ## Min. : 433.3 Length:1000 ## 1st Qu.:1089.6 Class :character ## Median :1257.2 Mode :character ## Mean :1256.4 ## 3rd Qu.:1415.7 ## Max. :2047.6 ## insured_education_level insured_occupation ## Length:1000 Length:1000 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## insured_hobbies insured_relationship ## Length:1000 Length:1000 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## incident_date incident_type ## Length:1000 Length:1000 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## collision_type incident_severity ## Length:1000 Length:1000 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## authorities_contacted incident_hour_of_the_day ## Length:1000 Min. : 0.00 ## Class :character 1st Qu.: 6.00 ## Mode :character Median :12.00 ## Mean :11.64 ## 3rd Qu.:17.00 ## Max. :23.00 ## number_of_vehicles_involved auto_make ## Min. :1.000 Length:1000 ## 1st Qu.:1.000 Class :character ## Median :1.000 Mode :character ## Mean :1.839 ## 3rd Qu.:3.000 ## Max. :4.000 ## auto_model auto_year total_claim_amount ## Length:1000 Min. :1995 Min. : 100 ## Class :character 1st Qu.:2000 1st Qu.: 41813 ## Mode :character Median :2005 Median : 58055 ## Mean :2005 Mean : 52762 ## 3rd Qu.:2010 3rd Qu.: 70593 ## Max. :2015 Max. :114920 Eliminar las columnas que contienen información específica: datos &lt;- datos[,c(-3,-12)] Ajustaremos un primer modelo con la variable de género del asegurado: m1 &lt;- glm(total_claim_amount ~ insured_sex, data = datos, family = Gamma) summary(m1) ## ## Call: ## glm(formula = total_claim_amount ~ insured_sex, family = Gamma, ## data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.2500 -0.2192 0.0987 0.3053 0.8796 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.875e-05 4.049e-07 46.30 &lt;2e-16 ## insured_sexMALE 4.519e-07 6.027e-07 0.75 0.454 ## ## (Intercept) *** ## insured_sexMALE ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.2504562) ## ## Null deviance: 599.52 on 999 degrees of freedom ## Residual deviance: 599.38 on 998 degrees of freedom ## AIC: 23578 ## ## Number of Fisher Scoring iterations: 5 Bondad de ajuste: dev &lt;- m1$deviance nullDev &lt;- m1$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 0.1409954 chidf &lt;- m1$df.null - m1$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0.7072934 La prueba de bondad de ajuste nos dice que el modelo es malo ya que la explicación de la varianza ganada con el modelo no es estadísticamente significativa. Intentemos ahora con la variable de edad m2 &lt;- glm(total_claim_amount ~ age, data = datos, family = Gamma) summary(m2) ## ## Call: ## glm(formula = total_claim_amount ~ age, family = Gamma, data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.2383 -0.2181 0.0940 0.3057 0.8563 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.175e-05 1.313e-06 16.571 &lt;2e-16 *** ## age -7.125e-08 3.228e-08 -2.208 0.0275 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.2506544) ## ## Null deviance: 599.52 on 999 degrees of freedom ## Residual deviance: 598.31 on 998 degrees of freedom ## AIC: 23576 ## ## Number of Fisher Scoring iterations: 5 Bondad de ajuste: dev &lt;- m2$deviance nullDev &lt;- m2$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 1.207078 chidf &lt;- m2$df.null - m2$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0.2719116 ¿Es un buen modelo?, ¿Es mejor que el modelo de género? Probar con la variable de tipo de siniestro. m3 &lt;- glm( total_claim_amount ~ incident_type, data = datos, family = Gamma) summary(m3) ## ## Call: ## glm(formula = total_claim_amount ~ incident_type, family = Gamma, ## data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.44568 -0.18206 -0.01135 0.17050 0.80416 ## ## Coefficients: ## Estimate ## (Intercept) 1.622e-05 ## incident_typeParked Car 1.722e-04 ## incident_typeSingle Vehicle Collision -7.070e-07 ## incident_typeVehicle Theft 1.650e-04 ## Std. Error ## (Intercept) 2.108e-07 ## incident_typeParked Car 5.471e-06 ## incident_typeSingle Vehicle Collision 2.944e-07 ## incident_typeVehicle Theft 4.976e-06 ## t value Pr(&gt;|t|) ## (Intercept) 76.967 &lt;2e-16 ## incident_typeParked Car 31.470 &lt;2e-16 ## incident_typeSingle Vehicle Collision -2.401 0.0165 ## incident_typeVehicle Theft 33.162 &lt;2e-16 ## ## (Intercept) *** ## incident_typeParked Car *** ## incident_typeSingle Vehicle Collision * ## incident_typeVehicle Theft *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.07073018) ## ## Null deviance: 599.520 on 999 degrees of freedom ## Residual deviance: 80.724 on 996 degrees of freedom ## AIC: 21492 ## ## Number of Fisher Scoring iterations: 5 Bondad de ajuste: dev &lt;- m3$deviance nullDev &lt;- m3$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 518.7961 chidf &lt;- m3$df.null - m3$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0 ¿Qué podemos decir de este modelo?, ¿Nos sirve este modelo para clasificar la sobreprima? ¿Podemos encontrar un mejor modelo? "],
["modelos-lineales-generalizados-construcción-y-evaluación.html", "Capítulo 6 Modelos Lineales Generalizados (Construcción y Evaluación) 6.1 Exploración de los datos. 6.2 Elección de la estructura de errores y función liga. 6.3 Bondad de ajuste. 6.4 Simplificación del modelo. 6.5 Criterios de evaluación de modelos. 6.6 Análisis de los residuos. 6.7 Evaluación de GLMs en R", " Capítulo 6 Modelos Lineales Generalizados (Construcción y Evaluación) En la construcción de modelos lineales generalizados es importante tener en cuenta que NO existe un único modelo que sea válido. En la mayoría de los casos, habrá un número variable de modelos plausibles que puedan ajustarse a un conjunto determinado de datos. Parte del trabajo de construcción y evaluación del modelo es determinar cuál de todos estos modelos son adecuados, y entre todos los modelos adecuados, cuál es el que explica la mayor proporción de la varianza sujeto a la restricción de que todos los parámetros del modelo deberán ser estadísticamente significativos. En algunos casos habrá más de un modelo que ajuste igual de bien a los datos y en esos casos queda a criterio del modelador elegir uno u otro. Los pasos que hay que seguir en la construcción y evaluación de un GLM son muy similares a los de cualquier modelo estadístico. 6.1 Exploración de los datos. Siempre es conveniente conocer los datos con los que se esta trabajando. Puede resultar interesante obtener gráficos que nos muestren la relación entre la variable de respuesta y cada una de las variables explicativas, gráficos de caja (box-plot) para variables categóricas, o matrices de correlación entre las variables explicativas. El objetivo de este análisis exploratorio es: Buscar posibles relaciones de la variable respuesta/dependiente con la(s) variable(s) explicativa(s). Considerar la necesidad de aplicar transformaciones de las variables. Eliminar variables explicativas que estén altamente correlacionadas. 6.2 Elección de la estructura de errores y función liga. A veces resultará fácil elegir estas propiedades del modelo basandose en las características de la variable de respuesta. Pero en otras ocasiones resultará tremendamente difícil, y será a posteriori cuando comprobemos, analizando los residuos, la idoneidad de la distribución de errores elegida. Por otro lado, puede ser una práctica recomendable el comparar modelos con distintas funciones liga para ver cuál se ajusta mejor a nuestros datos. 6.3 Bondad de ajuste. Los tests de significación para los estimadores del modelo. (p-values de los estimadores) La cantidad de varianza explicada por el modelo. Esto en GLM se conoce como devianza. La devianza nos da una idea de la variabilidad del los datos. Por ello, para obtener una medida de la variabilidad explicada por el modelo, hemos de comparar la devianza del modelo nulo (Null deviance) con la devianza residual (Residual deviance), esto es, una medida de cuánto de la variabilidad de la variable respuesta no es explicado por el modelo. (Prueba de la Chi-cuadrada sobre la devianza) 6.4 Simplificación del modelo. El principio de parsimonia requiere que el modelo sea tan simple como sea posible. Esto significa que no debe contener parámetros o niveles de un factor que sean redundantes. La simplificación del modelo implica por tanto: La eliminación de las variables explicativas que no sean significativas. La agrupación de los niveles de factores (variables categóricas) que no difieran entre sí. La simplificación del modelo tiene que tener, además, una cierta lógica para el analista y no debe incrementar de manera significativa la devianza residual. 6.5 Criterios de evaluación de modelos. Podemos utilizar la reducción de la devianza como una medida del ajuste del modelo a los datos. Los tests de significación para los parámetros del modelo son también útiles para ayudarnos a simplificar el modelo. Un criterio comúnmente utilizado es el llamado Criterio de Información de Akaike (AIC del inglés Akaike Information Criterion). Este índice evalúa tanto el ajuste del modelo a los datos como la complejidad del modelo. Cuanto más pequeño es el AIC mejor es el ajuste. El AIC es muy útil para comparar modelos similares con distintos grados de complejidad o modelos iguales (mismas variables) pero con funciones liga distintas. 6.6 Análisis de los residuos. Los residuos son las diferencias entre los valores estimados por el modelo y los valores observados. Sin embargo, muchas veces se utilizan los residuos estandarizados, que tienen que seguir una distribución normal. Conviene analizar los siguientes gráficos: Histograma de los residuos. Gráfico de residuos frente a valores estimados. Estos gráficos pueden indicar falta de linealidad, heterocedasticidad (varianza no constante) y valores atípicos. Gráficos de valores atípicos. Existen tests que permiten detectar valores atípicos. Los índices más comunes son el índice de Cook y el de apalancamiento o leverage. Estos gráficos ayudan la evaluación del modelo utilizado. En caso necesario, sería preciso volver a plantear el modelo, tal vez utilizando una estructura de errores más adecuada, otra función liga o incluso eliminando ciertos datos que pueden estar desviando el análisis. 6.7 Evaluación de GLMs en R Ejemplo de meses para default crediticio usando una distribución exponencial. datos &lt;- read.csv(&quot;example_data/default.csv&quot;, header = TRUE) datos &lt;- datos[datos$default==1,] Construcción de la variable del tiempo transcurrido al default: datos$meses_default &lt;- datos$duration-datos$installment_commitment m1 &lt;- glm(meses_default ~ own_telephone, data = datos, family = Gamma) Análisis de puntos influyentes. Distancia de Cook: plot(m1, 4) Puntos palanca para betas: plot(dfbetas(m1)[,1],type=&quot;h&quot;, ylab=&quot;Beta0&quot;) plot(dfbetas(m1)[,2],type=&quot;h&quot;, ylab=&quot;Beta1&quot;) Puntos palanca para los datos ajustados: plot(dffits(m1),type=&quot;h&quot;) "],
["qué-es-una-red-neuronal.html", "Capítulo 7 ¿Qué es una red neuronal? 7.1 Teorema de Universalidad 7.2 Entrenamiento de una red neuronal 7.3 Redes Neuronales en R", " Capítulo 7 ¿Qué es una red neuronal? Las redes neuronales artificiales están inspiradas en la forma en la que las neuronas de nuestro cerebro trabajan en conjunto para resolver una tarea compleja. Éstas modelan una variable respuesta, ya sea de tipo continua o categórica, como función de las covariables a través de la composición de funciones no lineales. De manera general, una red neuronal consiste de una arquitectura, una regla de activación y una regla de salida. Arquitectura: Puede ser descrita vía un gráfo dirigido cuyo nodos son llamados neuronas. Existen una grán cantidad de arquitecturas dependiendo de la naturaleza del grafo i.e. de las relaciones entre los nodos. Una descripción bastante completa de las distintas arquitecturas y sus nombres puede ser consultada en este sitio. Regla de activación: Típicamente el valor en cada nodo \\(v\\) puede ser calculado como \\[x_v = f(\\sum_{u \\rightarrow v}\\beta_{uv}x_u)\\] donde la suma se obtiene sobre los nodos predecesores de \\(v\\) y \\(\\beta_{uv}\\) son los coeficientes (desconocidos) de la red. A la función \\(f(\\cdot)\\) se le conoce como función de activación. Las más comúnmente usadas son la sigmoide logística \\(f(x) = \\frac{e^y}{1+e^y}\\) y la función rectificadora \\(f(x) = \\max\\{{y,0}\\}\\) 7.0.1 Ejemplo Dada la siguiente arquitectura calcularemos el valor de salida dado por el nodo \\(x_6\\). \\[\\begin{align*} x_{6} &amp; =f\\left(\\beta_{3,6}\\cdot x_{3}+\\beta_{4,6}\\cdot x_{4}+\\beta_{5,6}\\cdot x_{5}\\right)\\\\ &amp; \\hookrightarrow x_{3}=f\\left(\\beta_{1,3}\\cdot x_{1}+\\beta_{2,3}\\cdot x_{2}\\right)\\\\ &amp; \\hookrightarrow x_{4}=f\\left(\\beta_{1,4}\\cdot x_{1}+\\beta_{2,4}\\cdot x_{2}\\right)\\\\ &amp; \\hookrightarrow x_{5}=f\\left(\\beta_{1,5}\\cdot x_{1}+\\beta_{2,5}\\cdot x_{2}\\right)\\\\ &amp; \\vdots \\end{align*}\\] donde \\(f(\\cdot)\\) es la función de activación. N.B. Si suponemos que \\(f(x) = \\mathbb{I}(x)\\) y que la red consiste de una sola capa entonces el problema a resolver será el de regresión lineal. Nodos de sesgo: Dependiendo de la utilidad, podríamos estar interesados, como en el problema de regresión lineal, en un término de intercepto o nivel _base-, para ello puede simplemente crearse un nodo extra cuyo valor sea la constante 1. Regla de salida: En el nodo de salida \\(v\\), se calcula: \\[s_v = \\sum_{u\\rightarrow v}\\beta_{uv}x_u\\] y en lugar de aplicar la función de activación \\(f(\\cdot)\\), se usa una regla de salida para obtener el valor o vector de ajuste/predicción. Por ejemplo, en un problema de clasificación de \\(N\\) clases, la capa de salida consitirá de \\(N\\) nodos, cada uno asociado a cada clase; la predicción/ajuste \\(\\hat{y}\\in \\mathbb{R}^p\\) está dada por \\[\\hat{y}=\\arg max\\{s_{vn}\\}\\] Notemos que para este ejemplo, si la arquitectura de la red carece de capas intermedias/ocultas y con función de activación sigmoide logística entonces el modelo corresponde al de regresión logística múltiple. Al igual que en los modelos logísticos, podemos hacer que la red neuronal tenga como salida un vector de probabilidades \\(z=(z_1,\\dots,z_N)\\) dado por \\[z_n=\\frac{e^{s_{v_n}}}{e^{s_{v_1}}+\\dots+e^{s_{v_N}}}\\] Al proceso de normalización de los valores exponenciados se le conoce como la operación softmax. De manera particular una red neuronal cuya arquitectura está representada por un grafo dirigido no cíclico, (feedforward) con función de activación sigmoide logística y regla de salida softmax es un modelo paramétrico descrito por: \\[y \\sim Multinomial(1, (p_1,\\dots,p_N)\\] \\[(p_1,\\dots,p_N)=g^{nnet}(x_v; \\{\\beta_{uv}:u\\rightarrow v\\})\\] 7.1 Teorema de Universalidad La flexibilidad y el poder de las redes neuronales radica en la estructura de composición de funciones de activación no lineales. El siguiente teorema implica que una red neuronal puede aproximar cualquier función; en particular, es relevante el poder aproximar cualquier modelo no paramétrico de regresión. Teorema. Sea \\(\\Delta^{L}=\\left\\{ \\left(y_{1},\\ldots,y_{L}\\right)\\in\\mathbb{R}_{\\geq0}^{L}\\,|\\,y_{1}+\\dots+y_{L}=1\\right\\} \\in \\mathbb{R}^{L}\\), y \\(B\\subseteq\\mathbb{R}^{p}\\) un conjunto acotado que contiene al origen y \\(\\mu\\) una medida de probabilidad en \\(B\\). Para \\(g:\\,B\\rightarrow\\Delta\\) una función medible arbitraria existe una red neuronal totalmente conectada (feedforward), de una capa oculta que tiene \\(m\\) neuronas, activación sigmoide y regla output softmax tal que su correspondiente función de probabilidades \\(g^{NNET}\\) satisface: \\[\\int_{B}\\left\\lVert g-g^{NNET}\\right\\lVert _{2}^{2}d\\mu\\leq\\frac{Clog^{2}\\left(n\\right)}{n}\\] para alguna constante universal \\(C\\). 7.2 Entrenamiento de una red neuronal En el ámbito del machine learning se le conoce como entrenamiento al proceso de encontrar buenos estimadores de los parámetros del modelo usado, en este caso, de la red neuronal. Sea \\(\\beta = (\\beta_{uv}; u\\rightarrow v)\\) el vector de parámetros de una red neuronal. Dados los datos de entrenamiento \\((x^1,y^1),\\dots,(x^n,y^n)\\), la log-verosimilitud de \\(\\beta\\) está dada por: \\[{\\it l}\\left(\\beta\\right)=\\sum_{i=1}^{n}\\sum_{{\\it l}=1}^{L}\\mathbb{I}_{\\left\\{ y_{i}={\\it l}\\right\\} }log\\left(\\frac{e^{s_{v_{{\\it l}}}\\left(x_{i},\\beta\\right)}}{e^{s_{v_{1}}\\left(x_{1},\\beta\\right)}+\\dots+e^{s_{v_{L}}\\left(x_{L},\\beta\\right)}}\\right) =\\colon \\sum_{i=1}^{n}{\\it l}_{i}\\left(\\beta\\right)\\] A continuación mostraremos cómo podemos maximizar la log-verosimilitud para obtener los parámetros. 7.2.1 Back-propagation Gracias a la estructura de composición de funciones del modelo, la derivada puede encontrarse fácilmente usando la regla de la cadena. 7.2.1.1 Ejemplo (back-propagation). Consideremos la siguiente red neuronal: Dado, el ejemplo de entrenamiento \\(x^i=(x_1^i,x_2^i)\\) con etiqueta/valor \\(y^i\\) la red neuronal calcula la probabilidad de salida \\(z_1=\\mathbb{P}[Y=1|X = x^i]\\) de la siguiente forma: \\[\\begin{align*} s_{3}=\\beta_{1,3}x_{1}+\\beta_{2,3}x_{2} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,x_{3}\\leftarrow f\\left(s_{3}\\right)\\\\ s_{4}=\\beta_{1,4}x_{1}+\\beta_{2,4}x_{2} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,x_{4}\\leftarrow f\\left(s_{4}\\right)\\\\ s_{5}=\\beta_{1,5}x_{1}+\\beta_{2,5}x_{2} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,x_{5}\\leftarrow f\\left(s_{5}\\right)\\\\ s_{6}=\\beta_{3,6}x_{3}+\\beta_{4,6}x_{4}+\\beta_{5,6}x_{5} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,z_{1}\\leftarrow f\\left(s_{6}\\right) \\end{align*}\\] Donde, por facilidad, estamos considerando que \\(f(x)=\\frac{e^x}{1+e^x}\\). El valor de la log-verosimilitud para este ejemplo de entrenamiento se calcula como: \\[{\\it l}_{i}\\left(\\beta;\\,\\left(x^{i},\\,y^{i}\\right)\\right)=\\mathbb{I}_{\\left\\{ y^{i}=1\\right\\} }log\\left(z_{1}\\right)+\\mathbb{I}_{\\left\\{ y^{i}=2\\right\\} }log\\left(1-z_{1}\\right)\\] Y el gradiente estocástico como: \\(\\frac{\\partial l}{\\partial z_1} = \\frac{\\mathbb{I}_{\\{y^1=1\\}}}{z_1}-\\frac{\\mathbb{I}_{\\{y^1=2\\}}}{1-z_1}\\) \\(\\frac{\\partial l}{\\partial s_6} = \\frac{\\partial l}{\\partial z_1}\\cdot f&#39;(s_6)\\) \\(\\frac{\\partial l}{\\partial \\beta_{3,6}} = \\frac{\\partial l}{\\partial s_6}\\cdot x_3\\) \\(\\frac{\\partial l}{\\partial \\beta_{4,6}} = \\frac{\\partial l}{\\partial s_6}\\cdot x_4\\) \\(\\frac{\\partial l}{\\partial \\beta_{5,6}} = \\frac{\\partial l}{\\partial s_6}\\cdot x_5\\) \\(\\frac{\\partial l}{\\partial x_3} = \\frac{\\partial l}{\\partial s_6}\\cdot \\beta_{3,6}\\) \\(\\frac{\\partial l}{\\partial x_4} = \\frac{\\partial l}{\\partial s_6}\\cdot \\beta_{4,6}\\) \\(\\frac{\\partial l}{\\partial x_5} = \\frac{\\partial l}{\\partial s_6}\\cdot \\beta_{5,6}\\) \\(\\frac{\\partial l}{\\partial s_3} = \\frac{\\partial l}{\\partial x_3}\\cdot f&#39;(s_3)\\) \\(\\frac{\\partial l}{\\partial s_4} = \\frac{\\partial l}{\\partial x_4}\\cdot f&#39;(s_4)\\) \\(\\frac{\\partial l}{\\partial s_4} = \\frac{\\partial l}{\\partial x_5}\\cdot f&#39;(s_5)\\) \\(\\frac{\\partial l}{\\partial \\beta_{1,3}} = \\frac{\\partial l}{\\partial s_3}\\cdot x_1\\) \\(\\frac{\\partial l}{\\partial \\beta_{2,3}} = \\frac{\\partial l}{\\partial s_3}\\cdot x_2\\) \\(\\frac{\\partial l}{\\partial \\beta_{1,4}} = \\frac{\\partial l}{\\partial s_4}\\cdot x_1\\) \\(\\frac{\\partial l}{\\partial \\beta_{2,4}} = \\frac{\\partial l}{\\partial s_4}\\cdot x_2\\) \\(\\frac{\\partial l}{\\partial \\beta_{1,5}} = \\frac{\\partial l}{\\partial s_5}\\cdot x_1\\) \\(\\frac{\\partial l}{\\partial \\beta_{2,5}} = \\frac{\\partial l}{\\partial s_5}\\cdot x_2\\) \\(\\frac{\\partial l}{\\partial x_1} = \\frac{\\partial l}{\\partial s_3}\\cdot \\beta_{1,3}+\\frac{\\partial l}{\\partial s_4}\\cdot \\beta_{1,4}+\\frac{\\partial l}{\\partial s_5}\\cdot \\beta_{1,5}\\) \\(\\frac{\\partial l}{\\partial x_2} = \\frac{\\partial l}{\\partial s_3}\\cdot \\beta_{2,3}+\\frac{\\partial l}{\\partial s_4}\\cdot \\beta_{2,4}+\\frac{\\partial l}{\\partial s_5}\\cdot \\beta_{2,5}\\) En resúmen, para optimizar la verosimilitud por gradiente estocástico debemos seguir lo siguientes pasos (llamados back-propagation): 1. Inicializar los parámetros \\(\\hat{\\beta}^{(0,n)}=\\{\\hat{\\beta_{u,v}}^{(0,n)}:u\\rightarrow v\\}\\) 2. Para cada fase de entrenamiento \\(t=1,2,\\dots\\) Para \\(i = 1,2,\\dots,n\\) - Obtener el vector de probabilidades \\(z = (z_1,\\dots,z_N)\\) usando como input el ejemplo \\(x^i\\) y los parámteros \\(\\hat{\\beta}^{(t, i-1)}\\) Obtener las derivadas parciales del la log-verosimilitud con respecto a: \\[\\{z_l:1\\leq l\\leq N\\} \\ \\{s_v:v\\in S_M\\} \\ \\{\\beta_{uv}:u\\in S_{M-1}, v\\in S_M\\}\\] \\[\\{x_v:v\\in S_{M-1}\\} \\ \\{s_v:v\\in S_{M-1}\\} \\ \\{\\beta_{uv}:u\\in S_{M-2}, v\\in S_{M-1}\\}\\] \\[\\{x_v:v\\in S_{M-2}\\} \\ \\{s_v:v\\in S_{M-2}\\} \\ \\{\\beta_{uv}:u\\in S_{M-3}, v\\in S_{M-1}\\}\\] \\[\\vdots\\] \\[\\{x_v:v\\in S_2\\} \\ \\{s_v:v\\in S_2\\} \\ \\{\\beta_{uv}:u\\in S_1, v\\in S_2\\}\\] en ese orden. 3. Actualizar los parámetros \\[\\hat{\\beta_{uv}}^{(t,i)} \\leftarrow \\hat{\\beta_{uv}}^{(t,i-1)}+\\alpha \\cdot \\frac{\\partial l_i}{\\partial \\beta_{uv}}\\] En general, a este procedimiento de optimización se le conoce como descenso gradiente. 7.2.2 Saturación Si durante el proceso de entrenamiento, algún nodo \\(v\\) tiene una valor \\(|s_v|\\) muy grande entonces el valor de \\(f&#39;(s_v)\\) será muy cercano a cero para funciones de activación sigmoides. Debido a la regla de la cadena, los valores de \\(\\beta_{uv}\\) se moverán muy lentamente hacia el óptimo, en este caso se dice que el nodo \\(v\\) está saturado. Para evitar problemas de saturación al inicio del entrenamiento comúnmente debemos estandarizar las covariables para que tengan media cero y varianza unitaria. También es conveniente inicializar los parámetos \\(\\hat{\\beta}^{(0,n)}\\) cercanos a cero, comúnmente elegidos uniformemente entre \\([-c,c]\\) para \\(c \\in (0,1)\\). Sin embargo debemos tener cuidado pues si \\(\\hat{\\beta}^{(0,n)}=0\\) entonces \\(\\frac{\\partial l}{\\partial x_v}=0 \\ \\forall \\ v\\in S_{M-1}\\) lo que hará que el algoritmo no se mueva. 7.2.3 Regularización Un problema típico en las redes neuronales es que suelen sobre ajustar los datos de entrenamiento dado que suelen haber más parámetros que variables. Una forma de regularizar es imponer una penalización equivalente al cuadrado de los parámetros, también llamada ridge o L2, a la log-verosimilitud y maximizar respecto a esta log-verosimilitud penalizada. \\[{\\it l}\\left(\\beta\\right)^{ridge}={\\it l}\\left(\\beta\\right)+\\frac{\\lambda}{2}||\\beta||_2^2 = \\sum_{i=1}^{n}\\sum_{{\\it l}=1}^{L}\\mathbb{I}_{\\left\\{ y_{i}={\\it l}\\right\\} }log\\left(\\frac{e^{s_{v_{{\\it l}}}\\left(x_{i},\\beta\\right)}}{e^{s_{v_{1}}\\left(x_{1},\\beta\\right)}+\\dots+e^{s_{v_{L}}\\left(x_{L},\\beta\\right)}}\\right)+\\frac{\\lambda}{2}||\\beta||_2^2\\] 7.3 Redes Neuronales en R Función para calcular tamaño de la capa get_layer_size &lt;- function(x, y, hidden_neurons) { n_x &lt;- dim(x)[1] n_h &lt;- hidden_neurons n_y &lt;- dim(y)[1] size &lt;- list(&quot;n_x&quot; = n_x, &quot;n_h&quot; = n_h, &quot;n_y&quot; = n_y) return(size) } Función para inicializar los parámetros aleatoriamente random_params &lt;- function(x, layer_size){ m &lt;- dim(data.matrix(x))[2] n_x &lt;- layer_size$n_x n_h &lt;- layer_size$n_h n_y &lt;- layer_size$n_y w1 &lt;- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01 b1 &lt;- matrix(rep(0, n_h), nrow = n_h) w2 &lt;- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.01 b2 &lt;- matrix(rep(0, n_y), nrow = n_y) params &lt;- list(&quot;w1&quot; = w1, &quot;b1&quot; = b1, &quot;w2&quot; = w2, &quot;b2&quot; = b2) return(params) } Función de activación sigmoide sigmoid_activation &lt;- function(x){ return(1/(1+exp(-x))) } Función de propagación forward forward_prop &lt;- function(x, params, layer_size){ m &lt;- dim(x)[2] n_h &lt;- layer_size$n_h n_y &lt;- layer_size$n_y w1 &lt;- params$w1 b1 &lt;- params$b1 w2 &lt;- params$w2 b2 &lt;- params$b2 b1_new &lt;- matrix(rep(b1, m), nrow = n_h) b2_new &lt;- matrix(rep(b2, m), nrow = n_y) z1 &lt;- w1 %*% x + b1_new a1 &lt;- sigmoid_activation(z1) z2 &lt;- w2 %*% a1 + b2_new a2 &lt;- sigmoid_activation(z2) cache &lt;- list(&quot;z1&quot; = z1, &quot;a1&quot; = a1, &quot;z2&quot; = z2, &quot;a2&quot; = a2) return(cache) } Función para obtener el costo get_cost &lt;- function(x, y, cache) { m &lt;- dim(x)[2] a2 &lt;- cache$a2 logprobs &lt;- (log(a2) * y) + (log(1-a2) * (1-y)) cost &lt;- -sum(logprobs/m) return(cost) } Función para aplicar backward propagation backward_prop &lt;- function(x, y, cache, params, layer_size){ m &lt;- dim(x)[2] n_x &lt;- layer_size$n_x n_h &lt;- layer_size$n_h n_y &lt;- layer_size$n_y a2 &lt;- cache$a2 a1 &lt;- cache$a1 w2 &lt;- params$w2 dz2 &lt;- a2 - y dw2 &lt;- 1/m * (dz2 %*% t(a1)) db2 &lt;- matrix(1/m * sum(dz2), nrow = n_y) db2_new &lt;- matrix(rep(db2, m), nrow = n_y) dz1 &lt;- (t(w2) %*% dz2) * (1 - a1^2) dw1 &lt;- 1/m * (dz1 %*% t(x)) db1 &lt;- matrix(1/m * sum(dz1), nrow = n_h) db1_new &lt;- matrix(rep(db1, m), nrow = n_h) grads &lt;- list(&quot;dw1&quot; = dw1, &quot;db1&quot; = db1, &quot;dw2&quot; = dw2, &quot;db2&quot; = db2) return(grads) } Función para actualizar los parámetros update_params &lt;- function(grads, params, learning_rate){ w1 &lt;- params$w1 b1 &lt;- params$b1 w2 &lt;- params$w2 b2 &lt;- params$b2 dw1 &lt;- grads$dw1 db1 &lt;- grads$db1 dw2 &lt;- grads$dw2 db2 &lt;- grads$db2 w1 &lt;- w1 - learning_rate * dw1 b1 &lt;- b1 - learning_rate * db1 w2 &lt;- w2 - learning_rate * dw2 b2 &lt;- b2 - learning_rate * db2 updated_params &lt;- list(&quot;w1&quot; = w1, &quot;b1&quot; = b1, &quot;w2&quot; = w2, &quot;b2&quot; = b2) return (updated_params) } Función de entrenamiento de la red neuronal train_nn &lt;- function(x, y, iters, hidden_neurons, learning_rate){ layer_size &lt;- get_layer_size(x, y, hidden_neurons) initial_params &lt;- random_params(x, layer_size) cost_history &lt;- c() for (i in 1:iters) { fwd_prop &lt;- forward_prop(x, initial_params, layer_size) cost &lt;- get_cost(x, y, fwd_prop) back_prop &lt;- backward_prop(x, y, fwd_prop, initial_params, layer_size) updated_params &lt;- update_params(back_prop, initial_params, learning_rate = learning_rate) initial_params &lt;- updated_params cost_history &lt;- c(cost_history, cost) if (i %% 1000 == 0) cat(&quot;Iteration&quot;, i, &quot; | Cost: &quot;, cost, &quot;\\n&quot;) } model &lt;- list(&quot;updated_params&quot; = updated_params, &quot;layer_size&quot; = layer_size, &quot;cost_hist&quot; = cost_history) return(model) } Función para hacer predicciones predict_nn &lt;- function(x, trained_model){ fwd_prop &lt;- forward_prop(x, trained_model$updated_params, trained_model$layer_size) pred &lt;- fwd_prop$a2 return(pred) } Aplicación a datos de default crediticio library(dplyr) library(magrittr) library(fastDummies) datos &lt;- read.csv(url(&quot;https://raw.githubusercontent.com/alberto-mateos-mo/seminario-est-fciencias/master/03_linear_models/logistic/data/Default.csv&quot;)) datos %&lt;&gt;% select(-purpose, -personal_status, -property_magnitude, -other_payment_plans, -housing, -job, -num_dependents, -own_telephone, -foreign_worker) datos &lt;- dummy_cols(datos) x_vars &lt;- t(scale(datos[,c(1,3:7,9:13)])) y_var &lt;- t(as.matrix(datos[,8])) model &lt;- train_nn(x_vars, y_var, hidden_neurons = 10, iters = 10000, learning_rate = 0.9) probas &lt;- predict_nn(x_vars, model) clase &lt;- round(probas) caret::confusionMatrix(as.factor(y_var), as.factor(clase)) "],
["qué-es-una-svm.html", "Capítulo 8 ¿Qué es una SVM? 8.1 Estimación de los coeficientes 8.2 RKHS y el método kernel 8.3 Margen suave 8.4 SVMs en R", " Capítulo 8 ¿Qué es una SVM? Las maquinas de soporte vectorial son, en principio, un clasificador lineal con las característica de que no asume ninguna dsitribución a los datos y directamente busca el hiperplano óptimo que separa los datos en clases. De forma inherente una SVM es un clasificador de dos clases, sin embargo para usarlas en problemas con \\(L&gt;2\\) clases bastará con aplicar el algorítmo repetidamente comparando cada clase contra el resto o bien aplicar distintas SVM para cada par de clases y finalmente clasificar un nuevo punto vía mayoría clasificada para las SVMs. En este capítulo exploraremos brevemente la teoría detras de las SVM para dos clases. Será conveniente etiquetar las clases como \\(\\mathcal{L}=\\{-1,-1\\}\\). Dado cualquier par \\((\\beta,\\beta_0) \\in \\mathbb{S}^{p-1}\\times\\mathbb{R}\\) existe un clasificador lineal que asigna a los puntos \\(\\{x:x^T\\beta+\\beta_0&gt;0\\}\\) a 1 y a los puntos \\(\\{x:x^T\\beta+\\beta_0&lt;0\\}\\) a -1. Este clasificador es ambiguo en la decisión para la frontera del hiperplano dado por \\(H_{\\beta_0,\\beta}\\colon= \\{x:x^T\\beta+\\beta_0=0\\}\\). Para cualquier observación \\((x_i,y_i)\\), el clasificador funciona correctamente si y solo si \\(y_i(x_i^T\\beta +\\beta_0)&gt;0\\). Más aún, el valor \\(|y_i(x_i^T\\beta +\\beta_0)|\\) indica la distancia de \\(x_i\\) a la frontera de decisión. Las SMV buscan aquel hiperplano que separa completamente a las dos clases y que maximiza la distancia hacia el punto más cercano. Este plano puede determinarse a través del siguiente problema de optimización: \\[ \\begin{equation} \\max_{\\beta,\\beta_0,||\\beta||_2=1} M \\ sujeto \\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq M,\\ i=1,\\dots,n \\end{equation} \\] Donde \\(M\\) lo podemos interpretar como el margen alrededor del hiperplano optimo que no contiene a ninguna observación. 8.1 Estimación de los coeficientes El problema de optimización mostrado es complicado de resolver fundamentalmente por la restricción \\(||\\beta||_2=1\\), sin embargo dado que el plano \\(H_{\\beta_0,\\beta}\\) es invariente bajo escalamiento sobre \\(\\beta_0\\) y \\(\\beta\\) podemos aplicar el factor \\(1/M\\) (de tal forma que \\(||\\beta||_2=1/M\\)) y el problema se transformaría en: \\[\\begin{equation} \\max_{\\beta,\\beta_0}\\frac{1}{||\\beta||_2} \\ sujeto\\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq 1,\\ i=1,\\dots,n \\end{equation} \\] Si además trabajamos con el problema equivalente \\[\\begin{equation} \\min_{\\beta,\\beta_0}\\frac{1}{2}||\\beta||_2^2 \\ sujeto\\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq 1,\\ i=1,\\dots,n \\tag{8.1} \\end{equation}\\] tendremos ahora un problema de optimización con una función objetivo cuadrática y restricciones lineales que puede resolverse eficientemente usando optimizadores convexos estándar. Para resolver el problema de optimización se usa el problema dual de Lagrange cuya teoría nos dice que la mejor cota inferior para \\(||\\beta||_2^2/2\\) es igual al valor óptimo: \\[\\begin{equation} \\max_{\\lambda\\in \\mathbb{R_{\\geq0}^n}} \\min_{\\beta,\\beta_0} L(\\beta,\\beta_0;\\lambda) \\end{equation} \\] donde \\(L(\\beta,\\beta_0;\\lambda)\\) es el lagrangiano de (8.1). Los estimadores de los coeficientes son calculados usando a los puntos más cercanos a la frontera de decisión como puntos soporte de ahí que a esos puntos se les llame vectores soporte Sin meternos demasiado en la teoría, el proceso de estimación de los coeficientes de una SVM y la obtención de predicciones usando a dualidad de Lagrange es el que sigue: Resolver el problema dual de (8.1) Sean \\(S=\\{i:\\lambda_i^*\\neq0\\}\\) los índices de los vectores soporte Calcular \\(\\beta^*=\\sum_{i\\in S}\\lambda_i^*y_i x_i\\) y \\(\\beta_0^*=-\\frac{1}{2}\\{\\min_{i:y_i=1}x_i^T\\beta^*+\\max_{i:y_i=-1}x_i^T\\beta^*\\}\\) Para cada punto nuevo \\(x\\), lo clasificamos con \\[\\psi^{SVM}(x)=sgn(x^T\\beta^*+\\beta_0^*)\\] 8.2 RKHS y el método kernel Típicamente nos encontraremos con datos para los cuales no será evidente la existencia de un hiperplano que pueda separar a las clases. Esto puede suceder porque la frontera de clasificación es no lineal, o bien porque los datos tienen mucha varianza (ruido) y las densidades de las clases se intersectan. Existen dos formas de generalizar el hiperplano de separación para resolver ambos problemas. Una forma de resolver el problema de no separabilidad es llevar los datos a alguna dimensión superior mediante \\(x \\mapsto \\phi(x)=(\\phi_1(x),\\phi_2(x),\\dots)\\) donde \\(\\phi_1,\\phi_2,\\dots\\) son funciones reales en algún conjunto \\(\\mathcal{X}\\), donde \\(\\mathcal{X}\\) no requiere alguna estructura específica. A la función \\(\\phi\\) se le conoce como función característica (feature map) y a sus componentes \\(\\phi_1,\\phi_2,\\dots\\) se les llama características (features) En dimensiones suficientemente grandes los puntos correspondientes a dos clases son siempre separables. Una pregunta natural es ¿cómo escoger buenas características y cuántas deberíamos escoger? Antes de responder estas preguntas asumamos que ya tenemos la función \\(\\phi\\) y definamos \\(k(x,x&#39;)=\\langle \\phi(x),\\phi(x&#39;)\\rangle\\) para cualesquiera \\(x,x&#39;\\in \\mathcal{X}\\), donde \\(\\langle \\cdot,\\cdot \\rangle\\) es el producto interior en el espacio de características. Con ello podemos obtener los coeficientes de la SVM como sigue: 1: Resolver el problema dual en el espacio de características \\[\\max_{\\lambda\\in \\mathbb{R}_{\\geq 0}^n,\\sum_i\\lambda_i y_i=0}\\sum_{i=1}^n\\lambda_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\lambda_i \\lambda_j y_i y_j k(x_i,x_j)\\] para obtener \\(\\lambda_1^*,\\cdots,\\lambda_n^*\\) 2: Sea \\(S=\\{i:\\lambda_i^*\\neq 0\\}\\) el conjunto de índices de los vectores soporte. 3: Calcular \\(\\beta_0^*=\\sum_{i\\in S}\\lambda_i^*y_i \\phi(x_i)\\) y \\[\\beta_0^*=-\\frac{1}{2}\\{\\min_{i:y_i=1}\\sum_{j\\in S}\\lambda _j^*y_j k(x_i,x_j)+\\max_{i:y_i=-1}\\sum_{j \\in S}\\lambda_j^* y_j k(x_i,x_j) \\}\\] 4: Cada nueva observación la clasificamos de acuerdo a \\[sgn\\{ \\sum_{i\\in S}\\lambda_i^* y_i k(x_i,x) + \\beta_0^* \\}\\] Notemos que el procedimiento anterior depende de \\(\\phi\\) a través de \\(k(\\cdot,\\cdot)\\), excepto por \\(\\beta^*\\) pero si solo queremos hacer predicciones entonces no es necesario saber exactamente quién es \\(\\phi\\) siempre que la función \\(k\\) esté dada. El siguiente teorema nos dice que cualquier kernel definido positivo \\(k\\) siempre puede obtenerse como el producto interno en algún espacio de características de dimensión infinita. Recordemos que un espacio de Hilbert es un espacio producto interior donde toda sucesión de Cauchy tiene límite respecto a la norma asociada al producto interior. Podemos pensar a un espacio de Hilbert como la generalización del espacio Euclideano. Definición: Una función simétrica \\(k:\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}\\) se llama kernel definido positivo en \\(\\mathcal{X}\\) si \\((k(x_i,x_j))_{i,j=1,\\dots,n}\\) es una matriz semi-definida positiva para cualquier \\(n\\in \\mathbb{N}\\) y \\(x_1,\\dots,x_n\\in \\mathcal{X}\\) Teorema: Si \\(k:\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}\\) es un kernel definido positivo, entonces existe un espacio de Hilbert \\(\\mathcal{H}\\) y una función \\(\\phi :\\mathcal{X}\\rightarrow \\mathcal{H}\\) tal que \\(k(x_1,x_2)=\\langle\\phi(x_1),\\phi(x_2)\\rangle\\) donde \\(\\langle\\cdot,\\cdot \\rangle_\\mathcal{H}\\) es el producto interno en \\(\\mathcal{H}\\). Esto significa que en lugar de trabajar directamente en espacios de dimensión superior, podemos trabajar con kernels definidos positivos, más aún la calidad de las SVM dependerá solamente de la elección de las funciones kernel \\(k\\). 8.2.1 ¿Cómo escoger un kernel k? Algunos kernels usados en la practica: Kernel lineal: \\(k(x,x&#39;)=x^Tx&#39;\\) Kernel Polinomial: \\(k(x,x&#39;)=(c+x^Tx&#39;)^d\\). Típicamente usado si suponemos que la similaridad entre dos observaciones está dada por las covariables e interacciones de ellas. Kernel Gaussiano: \\(k(x,x&#39;)=\\exp\\{-\\frac{1}{2\\sigma^2}||x-x&#39;||_2^2\\}\\). Es el más popular para trabajar con características no lineales. Kernel de Laplace: \\(k(x,x&#39;)=\\exp\\{-\\frac{1}{\\sigma}||x-x&#39;||_2\\}\\). Similar al gaussiano, éste mide la similaridad de observaciones basados en la distancia en \\(\\mathcal{X}\\) 8.3 Margen suave Recordemos que en el contexto de las SVM llamamos margen al espacio entre la frontera de decisión y el punto más cercano de cada clase. Una segunda forma de lidiar con problemas de no separabilidad es buscar el hiperplano óptimo con el margen \\(M\\), más amplio tal que el número de observaciones clasificadas erroneamente sea pequeño. Matemáticamente, lo que hacemos es agregar variables de holgura al problema de optimización original: \\[ \\begin{equation} \\max_{\\beta,\\beta_0,||\\beta||_2=1} M \\ sujeto \\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq M(1-\\xi_i),\\ i=1,\\dots,n \\end{equation} \\] \\[\\sum_{i=1}^n \\xi_i\\leq K\\] 8.4 SVMs en R En este ejemplo usaremos una máquina de soporte vectorial para clasificar un conjunto que asemeja un tablero de ajedréz. chessboard &lt;- read.csv(&quot;example_data/chessboard.csv&quot;) ggplot(chessboard, aes(x, y, colour = as.factor(z)))+ geom_point()+ scale_colour_unam(name = &quot;Clase&quot;, palette = &quot;secondary&quot;)+ labs(x = &quot;&quot;, y = &quot;&quot;)+ theme_light() Evidentemente, en el espacio original de los datos, resulta imposible encontrar un hiperplano que separe a las dos clases de interés. Algunos planos posibles Es aquí donde se vuelve relevante el uso de los RKHS (reproducing kernel hilbert space) cuya utilidad reside en poder trabajar con representaciones en dimensiones superiores en donde la elección del hiperplano sea más sencilla. Para construir la svm debemos especificar, al menos, el tipo de algoritmo, el kernel a usar y los parámetros del mismo. svm &lt;- ksvm(z~., data = chessboard[,1:3], type = &quot;C-svc&quot;, kernel = &quot;rbfdot&quot;, kpar=list(sigma = 0.1), scaled = F, cross = 5) Resultados con diferente valor para el parámetro del kernel "],
["antecedentes.html", "Capítulo 9 Antecedentes 9.1 Árboles de regresión 9.2 Árboles de clasificación 9.3 Algunos problemas en los árboles 9.4 Árboles en R", " Capítulo 9 Antecedentes También llamados métodos basados en árboles, son algorítmos que hacen particiones al espacio de covariables y a cada una de ellas le ajustan un modelo simple, por ejemplo una constante. Para hacer la tarea sencilla pensemos las particiones como binarias, es decir, el primer pasó será partir el espacio de covariables en dos, luego cada parte se divide en dos regiones más y continuamos el proceso hasta cumplir alguna regla de paro. Una parte muy importante de este tipo de modelos binarios es la interpretabilidad pues el árbol estratifica la población de acuerdo a sus características. 9.1 Árboles de regresión Supongamos que tenemos \\(p\\) covariables, una variable de respuesta/objetivo y \\(N\\) observaciones. El argoritmo debe poder decidir las variables a particionar, los puntos de quiebre y la topología del árbol. Supongamos que tenemos una partición de en \\(M\\) regiones \\(R_1,R_2,\\dots,R_M\\) y modelamos la respuesta como la constante \\(c_m\\) en cada región: \\[f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)\\] Si tomamos como criterio minimizar la suma de cuadrados \\(\\sum(y_i-f(x_i))^2\\), es sencillo obtener que la mejor \\(\\hat{c}_m\\) es el promedio de \\(y_i\\) en la región \\(R_m\\): \\[\\hat{c}_m=prom(y_i|x_i\\in R_m)\\] Ahora para encontrar la mejor partición binaria en términos de la suma de cuadrados seguimos lo siguiente pasos: Consideremos la variable de partición \\(j\\) y el punto de quiebre \\(s\\) definimos el par de semiplanos \\[R_1(j,s)=\\{X|X_j\\leq s\\}\\] \\[R_2(j,s)=\\{X|X_j &gt;s\\}\\] Entonces buscamos la variabe \\(j\\) y el punto de quiebre \\(s\\) que resuelve \\[\\min_{j,s}[\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]\\] Para cualquier elección de \\(j\\) y \\(s\\) la minimización dentro de los corchetes se resuelve como \\[\\hat{c}_1=prom(y_i|x_i \\in R_1(j,s))\\] y \\[\\hat{c}_2=prom(y_i|x_i \\in R_2(j,s))\\] Para cada variable de partición, encontrar el punto de quiebre \\(s\\) se puede hacer muy rápidamente pasando por todos los puntos. Una vez encontrada la mejor partición, volvemos a hacer particiones siguiendo el mismo método. ¿Qué tanto debemos dejar que un árbol crezca? El crecimiento de un árbol hace referencia a la cantidad de particiones que contiene. En este sentido un árbol muy grande podría sobre ajustar los datos mientras que uno muy pequeño podría no capturar la estructura de ellos. El tamaño del árbol es un parámetro que incide directamente en la complejidad del modelo y el tamaño óptimo debe ser elegido de forma adaptativa según los datos. La solución usada es dejar crecer el árbol (\\(T_0\\)) hasta tener un tamaño de nodo específico y entonces podar el árbol usando una estrategia costo-complejidad como mostramos en seguida. Definimos un sub-árbol \\(T \\subset T_0\\) como cualquier árbol que pueda ser obtenido de podar el árbol \\(T_0\\), es decir, colapsar el número de sus nodos internos. Indexemos los nodos terminales con \\(m\\), siendo el nodo \\(m\\) representante de la región \\(R_m\\). Sea \\(|T|\\) el número de nodos terminales en el árbol \\(T\\) y sean \\[N_m=\\#\\{x_i\\in R_m\\}\\] \\[\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m}y_i\\] \\[Q_m(T)=\\frac{1}{N_m}\\sum_{x_i\\in R_m}(y_i-\\hat{c}_m)^2\\] definimos entonces el criterio costo-complejidad como: \\[C_\\alpha(T)=\\sum_{m=1}^{|T|}N_mQ_m(T)+\\alpha |T|\\] La idea es encontrar, para cada \\(\\alpha\\), el sub-árbol \\(T_\\alpha \\subseteq T_0\\) que minimice \\(C_\\alpha(T)\\). El parámetro \\(\\alpha \\geq 0\\) controla el tradeoff entre el tamaño del árbol y la bondad de ajuste a los datos. Valores grandes de \\(\\alpha\\) resultará en árboles pequeños y viceversa. 9.2 Árboles de clasificación Si el objetivo es clasificar los cambios necesarios al algoritmo ocurren en los criterios de partición y de poda. Para árboles de regresión usamos la medida de impureza del nodo, error cuadrático, \\(Q_m(T)\\) pero ésta no es útil para clasificación. Para cada nodo \\(m\\), representando una región \\(R_m\\) con \\(N_m\\) observaciones sea \\[\\hat{p}_{mk}=\\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i=k)\\] la proporción de observaciones de la clase \\(k\\) en el nodo \\(m\\). Clasificamos, entonces, las observaciones en el nodo \\(m\\) a la clase \\(k(m)=\\arg \\max_k \\ \\hat{p}_{mk}\\), es decir, la clase mayoritaria en el nodo \\(m\\). Algunas medidas \\(Q_m(T)\\) de impureza del nodo son: Error de clasificación: \\(\\frac{1}{N_m}\\sum_{i \\in R_m}I(y_i \\neq k(m))=1-\\hat{p}_{mk(m)}\\) Índice GINI: \\(\\sum_{k \\ne k&#39;}\\hat{p}_{mk}\\hat{p}_{mk&#39;}=\\sum_{k=1}^K\\hat{p}_{mk}(1-\\hat{p}_{mk})\\) Entropía cruzada: \\(-\\sum_{k=1}^K\\hat{p}_{mk}\\log\\hat{p}_{mk}\\) En el caso de dos clases, si \\(p\\) es la proporción de la segunda clase, las medidas son: Error de clasificación: \\(1-max(p,1-p)\\) Índice GINI: \\(2p(1-p)\\) Entropía: \\(-p\\log p-(1-p) \\log (1-p)\\) 9.3 Algunos problemas en los árboles 9.3.1 Covariables categóricas Al trabajar con una covariable/predictora categórica con \\(q\\) posibles valores existen \\(2^{q-1}-1\\) posibles particiones binarias y el tiempo de computación se vuelve enorme para valores grandes de \\(q\\). Sin embargo si tenemos un problema de clasificación de dos clases \\((0,1)\\) podemos simplificar el problema ordenando los valores de la variable categórica de acuerdo a la propoción en la clase 1, entonces hacemos las particiones como si la variable fuera ordinal. Es posible demostrar que este procedimiento logra la partición óptima en términos de entropía o índice Gini de entre las \\(2^{q-1}-1\\) posibles. 9.3.2 La matriz de pérdida En problemas de clasificación, las consecuencias de hacerlo incorrectamente pueden ser más importantes para alguna clase que para otras. Para resolver esta situación definimos una matriz de pérdida \\(L\\) de \\(K\\times K\\), con \\(L_{kk&#39;}\\) siendo la pérdida incurrida por clasificar erroneamente una clase \\(k\\) como una \\(k&#39;\\). Típicamente \\(L_{kk}=0\\ \\forall k\\). Para incorporar las pérdidas en el modelo podemos modificar el índice Gini como \\(\\sum_{k \\neq k&#39;}L_{kk&#39;}\\hat{p}_{mk}\\hat{p}_{mk&#39;}\\). Aunque en el caso de \\(k&gt;2\\) clases es muy útil, no lo es para el caso \\(k=2\\), ¿por qué? 9.4 Árboles en R "],
["qué-es-un-random-forest.html", "Capítulo 10 ¿Qué es un random forest?", " Capítulo 10 ¿Qué es un random forest? Random Forest, al igual que el árbol de decisión, es un modelo de aprendizaje supervisado para clasificación (aunque también puede usarse para problemas de regresión). Es una técnica de aprendizaje automático muy popular. Los Random Forests tienen una capacidad de generalización muy alta para muchos problemas. Los Random Forests se construyen a través de árboles de decisión. Los árboles tienden a aprender muy bien los datos de entrenamiento pero su generalización no es tan buena, por lo tanto tienen la tendencia de sobre-ajustar (overfit). La solución para evitar esto es la de crear muchos árboles y que trabajen en conjunto y estos son los Random Forests. Un Random Forest es un conjunto (ensemble) de árboles de decisión combinados con bagging. Al usar bagging, lo que en realidad está pasando, es que distintos árboles ven distintas porciones de los datos. Ningún árbol ve todos los datos de entrenamiento. Esto hace que cada árbol se entrene con distintas muestras de datos para un mismo problema. De esta forma, al combinar sus resultados, unos errores se compensan con otros y tenemos una predicción que generaliza mejor. La idea de random forest "],
["cómo-funciona-random-forest.html", "Capítulo 11 ¿Cómo funciona Random Forest? 11.1 Ventajas y Desventajas de Random Forest 11.2 Hiper-parámetros más útiles del Random Forest: 11.3 Otros parámetros también disponibles para árboles:", " Capítulo 11 ¿Cómo funciona Random Forest? Random Forest funciona así: Seleccionamos k variables (columnas) de las m totales (siendo k menor a m) y creamos un árbol de decisión con esas k características. Creamos n árboles variando siempre la cantidad de k variables y también podríamos variar la cantidad de muestras que pasamos a esos árboles (esto es conocido como “bootstrap sample”) Tomamos cada uno de los n árboles y le pedimos que hagan una misma clasificación. Guardamos el resultado de cada árbol obteniendo n salidas. Calculamos los votos obtenidos para cada “clase” seleccionada y consideraremos a la más votada como la clasificación final de nuestro “bosque”. En este algoritmo la aleatoriedad le brinda flexibilidad suficiente como para poder obtener gran variedad de árboles y de muestras que en su conjunto aparentemente caótico, producen una salida concreta. 11.1 Ventajas y Desventajas de Random Forest Vemos algunas de sus ventajas son: Funciona bien -aún- sin ajuste de hiperparámetros Funciona bien para problemas de clasificación y también de regresión. Al utilizar múltiples árboles se reduce considerablemente el riesgo de overfiting Se mantiene estable con nuevas muestras puesto que al utilizar cientos de árboles sigue prevaleciendo el promedio de sus votaciones. Y sus desventajas: En algunos datos de entrada “particulares” random forest también puede caer en overfitting Es mucho más “costo” de crear y ejecutar que “un sólo árbol” de decisión. Puede requerir muchísimo tiempo de entrenamiento Random Forest no funciona bien con datasets pequeños. Es muy difícil poder interpretar los ¿cientos? de árboles creados en el bosque, si quisiéramos comprender y explicar a un cliente su comportamiento. 11.2 Hiper-parámetros más útiles del Random Forest: n_estimators: número de árboles que va a tener el bosque. Normalmente cuantos más mejor, pero a partir de cierto punto deja de mejorar y sólo hace que vaya más lento. Un buen valor por defecto puede ser el uso de 100 árboles. n_jobs: número de cores que se pueden usar para entrenar los árboles. Cada árbol es independiente del resto, así que entrenar un bosque aleatorio es una tarea muy paralelizable. Por defecto sólo utiliza 1 core de la CPU. Para mejorar el rendimiento puedes usar tantos cores como estimes necesario. Si usas n_jobs = -1, estás indicando que quieres usar tantos cores como tenga tu máquina. max_features: usa forma de garantizar que los árboles son diferentes, es que todos se entrenan con una muestra aleatoria de los datos. Si queremos que todavía sean más diferentes, podemos hacer que distintos árboles usen distintos atributos. Esto puede ser útil especialmente cuando algunos atributos están relacionados entre sí. 11.3 Otros parámetros también disponibles para árboles: max_depth: la profundidad máxima del árbol. min_samples_split: número mínimo de muestras necesarias antes de dividir este nodo. También se puede expresar en porcentaje. min_samples_leaf: número mínimo de muestras que debe haber en un nodo final (hoja). También se puede expresar en porcentaje. max_leaf_nodes: número máximo de nodos finales "],
["random-forest-en-r.html", "Capítulo 12 Random forest en R", " Capítulo 12 Random forest en R # Carga el paquete específico del método Random Forest library(randomForest) # Carga de datos inicial, tipos de flores con diferentes caracter?sticas data(iris) # Selección de una submuestra del 70% de los datos set.seed(456) training_sample &lt;- sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.7,0.3)) train &lt;- iris[training_sample, ] test &lt;- iris[!training_sample, ] # Ajustar modelo modelo &lt;- randomForest(Species~., data=train) # Resumen del ajuste del modelo modelo ## ## Call: ## randomForest(formula = Species ~ ., data = train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 2.13% ## Confusion matrix: ## setosa versicolor virginica class.error ## setosa 31 0 0 0.00000000 ## versicolor 0 34 1 0.02857143 ## virginica 0 1 27 0.03571429 # Importancia de las variables modelo$importance ## MeanDecreaseGini ## Sepal.Length 4.547454 ## Sepal.Width 2.383503 ## Petal.Length 25.823796 ## Petal.Width 29.080949 # Hacer predicciones predicciones &lt;- predict(modelo, test) # Matriz de confusión (mc &lt;- with(test,table(predicciones, Species))) ## Species ## predicciones setosa versicolor virginica ## setosa 19 0 0 ## versicolor 0 13 3 ## virginica 0 2 19 # % correcto 100 * sum(diag(mc)) / sum(mc) ## [1] 91.07143 Referencias \\(\\href{https://www.iartificial.net/random-forest-bosque-aleatorio/}{https://www.iartificial.net/random-forest-bosque-aleatorio/}\\) \\(\\href{https://www.aprendemachinelearning.com/random-forest-el-poder-del-ensamble/}{https://www.aprendemachinelearning.com/random-forest-el-poder-del-ensamble/}\\) "],
["análisis-de-componentes-principales.html", "Capítulo 13 Análisis de Componentes Principales 13.1 Introducción 13.2 Los componentes principales 13.3 Interpretación geométrica 13.4 Consideraciones 13.5 PCA en R", " Capítulo 13 Análisis de Componentes Principales 13.1 Introducción Supongamos que tenemos un conjunto de datos en \\(\\mathbb{R}^p\\) y que queremos visualizarlos. Si \\(p\\) es muy grande no podremos visulizar todo el conjunto con un solo gráfico, una manera de visualizar los datos es hacer gráficos de dsipersión de dos dimensiones. Sin embargo la cantidad de gráficos de dos dimensiones que podemos hacer son \\(p(1-p)/2\\). Lo cual vuelve la tarea compleja conforme el valor de \\(p\\) aumenta. Lo que nos gustaría es poder encontrar una representación de los datos con menos dimensiones pero que conserva la mayor cantidad de información posible. El análisis de componentes principales es una herramienta que nos permitirá lograr eso, enocntrar representaciones en menos dimensiones que conserven la mayor cantidad de varianza. 13.2 Los componentes principales Los componentes principales de un conjunto de datos en \\(\\mathbb{R}^p\\) son una serie de combinaciones lineales de rango \\(q \\leq p\\) que, colectivamente, explican la mayoría de la varianza original. Si denotamos las observaciones por \\(x_1, x_2, \\dots, x_n\\), sea \\[f(\\lambda) = \\mu+V_q\\lambda\\] el modelo lineal de rango \\(q\\) que las representa; esta ecuación es la representación paramétrica de un hiperplano afín de rango \\(q\\). Si resolvemos el problema que nos permita encontrar el plano afín que minimice el error de reconstrucción: \\[min_{\\mu,\\lambda_i,V_q} \\sum_{i=1}^N ||x_i-\\mu -V_q\\lamda_i||^2\\] obtenemos \\[\\hat{\\mu}=\\bar{x}\\] \\[\\hat{\\lambda}_i=V_q^T(x_i-\\bar{x})\\] Con lo cual debemos encontrar la matriz ortogonal \\(V_q\\) tal que: \\[min_{V_q}\\sum_{i=1}^N||x_i-\\bar{x}-V_q V_q^T(x_i-\\bar{x})||^2\\] Por facilidad, asumimos que \\(\\bar{x}=0\\). En caso contrario centramos las variables para tener media cero. Entoncesla matriz \\(H_q = V_q V_q^T\\) es una matriz de proyección que mapea cada punto \\(x_i\\) en su reconstrucción \\(H_q x_i\\) que es la proyección ortogonal de \\(x_i\\) en el subespacio generado por las columnas de \\(V_q\\) La solución puede ser expresada como: \\[\\begin{equation} X = UDV^T \\tag{13.1} \\end{equation}\\] esto es, la descomposicion en valores singulares de \\(X\\) donde: \\(U\\) es una matriz ortogonal de \\(N\\times p\\) cuyas columnas llamamos valores singulares izquierdos \\(V\\) es una matriz ortogonal de \\(p\\times p\\) cuyas columnas llamamos valores singulares derechos \\(D\\) es una matriz diagonal de \\(p \\times p\\) cuyos elementos cumplen \\(d_1\\geq d_2\\geq \\dots \\geq d_p\\geq 0\\) llamados valores singulares A las columnas de \\(UD\\) las llamamos componentes principales de \\(X\\) El primer componente principal está dado por la combinación lineal normalizada de las columnas de los datos originales cuya varianza es máxima: \\[Z_1 = \\phi_{11}X_1+\\phi_{21}X_2+ \\dots+\\phi_{p1}X_p\\] N.B. \\(\\sum_{j=1}^p\\phi_{j1}^2=1\\) A los valores \\(\\phi_{j1}\\) los llamamos pesos del primer componente principal. Éstos tienen la restricción de que su suma debe ser igual a uno para evitar que el primer componente tenga varianza arbitraria. 13.3 Interpretación geométrica Para el primer componente principal, \\(\\phi_1\\) define la dirección en el espacio para la cual los datos presentan mayor variación. El segundo componente principal es aquella combinación lineal ortogonal al primer componente que tenga máxima varianzay aspi sucesivamente con el resto de componentes. Los scores de los componentes principales son las proyecciones de los datos en las direcciones descritas por \\(\\phi_i\\) 13.4 Consideraciones 13.4.1 Escalamiento de variables Los resultados del análisis de componentes principales serán distintos si las variables no se centran (media cero) e incluso serán distintos si éstas se escalan individualmente. Las diferencias en los resultados están ligadas directamente con la escala en la que estén medidas las diferentes variables del conjunto de datos. Consideremos por ejemplo los datos USArrests, en este conjunto las variables Murder, Rape y Assault están medidas como ocurrencia por cada 100,000 habitantes y UrbanPop como porcentaje de población, en este sentido, si analizamos las varianzas: lapply(USArrests, var) ## $Murder ## [1] 18.97047 ## ## $Assault ## [1] 6945.166 ## ## $UrbanPop ## [1] 209.5188 ## ## $Rape ## [1] 87.72916 veremos que Assault tiene la varianza más grande lo cual causará si no escalaramos los datos, que el primer componente tengo un peso muy grande para esta variable. Para evitar que los componentes principales dependan de la escala o de la elección de algún factor de escala deberemos transformar las variables para que tengan desviación estandar unitaria antes de aplicar el algoritmo de componentes principales. N.B. Si las variables fueron medidas en la misma escala entonces no es necesario aplicar ninguna transformación. 13.4.2 Varianza explicada por los componentes principales La varianza total del conjunto de datos está dada por \\[\\sum_{j=1}^pVar(X_j)=\\sum_{j=1}^p\\frac{1}{n}\\sum_{i=1}^{n}x_{ij}^2\\] La varianza explicada por el m-ésimo componente principal es \\[\\frac{1}{n}\\sum_{i=1}^n(\\sum_{j=1}^p\\phi_{jm}x_{ij})^2\\] Y la proporción de varianza explicada por el m-ésimo componente es \\[\\frac{\\sum_{i=1}^n(\\sum_{j=1}^p\\phi_{jm}x_{ij})^2}{\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2}\\] 13.4.3 Número de componentes a usar De manera general, un conjunto de datos n-dimensional tiene \\(min(n-1,p)\\) componentes principales distintos. Sin embargo, dado que no típicamente usaremos este análisis para reducir la dimensión de los datos no estamos interesados en usarlos todos. De hecho lo que buscamos es el menor número de componentes que nos permitan capturar una buena cantidad de información. Para encontrar este número de componentes no existe una respuesta única. Comúnmente usaremos como ayuda el screeplot o gráfica de codo de la varianza del m-ésimo componente principal y escogeremos la cantidad que nos permita capturar ya sea cierta cantidad de varianza e.g. el 70% de ella o bien aquel número de componentes a partir del cual el incremento de varianza por cada componente adicional sea marginal. 13.5 PCA en R WIP "],
["análisis-factorial.html", "Capítulo 14 Análisis factorial 14.1 Variables Latentes 14.2 Solución factorial 14.3 Rotaciones de factores 14.4 Análisis factorial en R", " Capítulo 14 Análisis factorial 14.1 Variables Latentes En muchas ocasiones los datos multivariados son vistos como mediciones indirectas provenientes de una fuente subyacente que típicamente no puede ser medida directamente. Por ejemplo: Tests psicológicos y pedagógicos se usan como una forma de medir la inteligencia u otras habilidades mentales. Los electroencefalogramas miden la actividad cerebral de forma indirecta usando señales electromagnéticas grabadas por sensores colocados en diferentes partes de la cabeza. Los precios de las acciones en el mercado financiero reflejan factores no medidos como la confianza en el mercado o influencias externas que serían dificiles de medir o identificar. El análisis factorial es una herramienta que permite identificar las fuentes latentes subyacentes a un conjunto de datos. 14.2 Solución factorial La descomposición en valores singulares (13.1) tiene una representación en variables latentes. Escribiendo \\(S = \\sqrt{N}U\\), \\(A^T = \\frac{DV^T}{\\sqrt{N}}\\), tenemos \\[X = SA^T\\] Y por tanto cada columna de \\(X\\) es una combinación lineal de las columnas de \\(S\\). Dado que \\(U\\) es una matriz ortogonal y asumiendo que las columnas de \\(X\\) (y de \\(U\\)) tienen media cero tenemos que las columnas de \\(S\\) también tienen media cero, no están correlacionadas y tienen varianza unitaria. En términos de variables aleatorias podemos interpretar a los componentes principales como una estimación del modelo de variables latentes: \\[X_1 = a_{11}S_1+a_{12}S_2+\\dots +a_{1p}S_p\\] \\[X_2 = a_{21}S_1+a_{22}S_2+\\dots +a_{2p}S_p\\] \\[\\vdots\\] \\[X_p = a_{p1}S_1+a_{p2}S_2+\\dots +a_{pp}S_p\\] o simplemente \\(X = AS\\). Las variables \\(X_j\\), que están correlacionadas son representadas como una expasión lineal de las variables \\(S_l\\), no correlacionadas y de varianza unitaria. Sin embargo, esto no es demasiado satisfactorio puesto que dada cualquier matriz ortogonal de \\(p\\times p\\) en \\(\\mathbb{R}\\) podemos escribir: \\[X = AS\\] \\[X = AR^TRS\\] \\[X = A^*S^*\\] Y \\(Cov(S^*) = R Cov(S)R^T = I\\) Por lo tanto ha una infinidad de tales descomposiciones haciendo imposible identificar las variables latentes como fuentes subyacentes únicas. El modelo clásico de factores resuelve este problema escribiendo el modelo como \\(X = AS+\\epsilon\\). Donde \\(A\\) es la matriz de pesos de los factores, \\(S\\) un vector de \\(q\\) variables latentes y \\(\\epsilon_j\\) son errores no correlacionados de media cero. Comúnmente tanto \\(S_j\\) y \\(\\epsilon_j\\) se modelan como variables aleatorias normales y el modelo factoria se ajusta por máxima verosimilitud. Los parámetros dependen de la matriz de covarianzas \\[\\Sigma = AA^T+D_{\\epsilon}\\] Donde \\(D_{\\epsilon} = diag[Var(\\epsilon_1),\\dots , Var(\\epsilon_p)]\\). Y dado que \\(S_j\\) son variables normales no correlacionadas entonces son variables aleatorias independientes. La presencia de los errores \\(\\epsilon_j\\) hace que el análisis de factores pueda verse como un modelo de la estructura de correlación de \\(X_j\\) en lugar de la estructura de covarianza (la usada en el análisis de componentes principales). 14.3 Rotaciones de factores Como se menciona anteriormente las soluciones al modelo factorial podrían no ser únicas para encontrar la unicidad se introducen restricciones meramente arbitrarias. Diferentes tipos de restricciones derivan en soluciones distintas al modelo. Al proceso de cambiar de una solución a otra se le llama rotación y proviene de la representación geométrica del procedimiento. La razón principal para rotar una solución es dar claridad a las cargas factoriales. Si la solución inicial es confusa una rotación puede proporcionar una estructura más fácil de interpretar. 14.3.1 Rotaciones ortogonales Uno de los patrones de cargas factoriales más usuales y de hecho más deseables es la llamada estructura simple de pesos factoriales. Se dice que los pesps factoriales presentan una estructura simple si cada variable tiene un gran peso en un solo factor, con pesos cercanos a cero en el resto de los factores. Una de las rotaciones que procura generar una estructura de pesos simple son las rotaciones ortogonales (los nuevos ejes después de la rotación siguen siendo ortogonales). 14.3.2 Rotaciones oblicuas Contrario a las rotaciones ortogonales, las rotaciones oblicuas permiten relajar la restricción de ortogonalidad con el fin de ganar simplicidad en la interpretación de los factores. Con este método los factores resultan correlacionados, aunque generalmente esta correlación es pequeña. El uso de rotaciones oblicuas se justifica porque en muchos contextos es lógico suponer que los factores están correlacionados. 14.4 Análisis factorial en R WIP "],
["qué-son-los-métodos-de-clusterización.html", "Capítulo 15 ¿Qué son los métodos de clusterización?", " Capítulo 15 ¿Qué son los métodos de clusterización? Son un serie de técnicas que permiten encontrar subgrupos/clústers dentro de un conjunto de datos Estos subgrupos tienen la característica de que las observaciones dentro de cada uno similares mientras que los subgrupos son distintos entre ellos. Estas técnicas nos permitirán indentificar la estructura subyacente a los datos. Por ejemplo, supongamos que un club de precios cuenta con información sobre las compras de sus clientes, podemos estar interesados en segmentar a los clientes de acuerdo a sus patrones de consumo con la intención de generar estrategias de marketing específicas para cada segmento. El análisis de clústers nos sería útil para identificar estos grupos de consumo. Las técnicas de clusterización son muy usadas y existen una gran variedad de ellas; en esta sección nos enfocaremos en las dos más conocidas k-medias y clusterización jerárquica. "],
["conceptos-teóricos.html", "Capítulo 16 Conceptos teóricos 16.1 Medidas de disimilaridad", " Capítulo 16 Conceptos teóricos Definition 16.1 \\(C_1, C_2, \\dots,C_k\\) es una partición de \\(C\\) de tamaño \\(k\\) si \\(\\cup_{i=1}^k C_i = C\\), y \\(C_i \\cap C_j = \\emptyset\\) si \\(i \\neq j\\). Una clusterización de \\(X\\) es una partición de \\(X\\). Definition 16.2 Una medida de disimilaridad en un conjunto finito \\(X\\) es una función \\(d: X\\times X \\rightarrow \\mathbb{R}\\) simétrica. En particular una métrica es medida de disimilaridad pero una medida de disimilaridad no es necesariamente una métrica. 16.1 Medidas de disimilaridad 16.1.1 Datos numéricos Si \\(X \\subset \\mathbb{R}^p\\) es un conjunto de \\(N\\) datos, entonces tenemos las siguientes medidas de disimilaridad: \\(d_{euc}: X\\times X \\rightarrow \\mathbb{R}^+\\cup\\{0\\}\\); \\(d_e(x_i,x_j)=||x_i-xj||\\) \\(d_{abs}: X\\times X \\rightarrow \\mathbb{R}^+\\cup\\{0\\}\\); \\(d_a(x_i,x_j)=\\sum_{l=1}^p|x_{il}-x_{jl}|\\) \\(d_{cor}: X\\times X \\rightarrow \\mathbb{R}\\); \\(d_c(x_i,x_j)=\\rho(x_i,x_j)\\) 16.1.2 Datos ordinales Si \\(X=\\{x_1,\\dots,x_N\\}\\) representa un conjunto de \\(N\\) datos univariados ordinales, podemos definir la métrica de valor absoluto en \\(X\\) guiándonos por el ordenamiento de los datos, entonces podemos definir \\(f:\\ Rango(X)\\rightarrow \\mathbb{N}\\) de tal manera que \\(f\\) preserve el orden y definir \\[d(x_i,x_j) = |f(x_i)-f(x_j)|\\] 16.1.3 Datos categóricos Si \\(X\\) representa un conjunto de datos categóricos podemos definir como medida de disimilaridad a la delta de Kronocker \\[d(x_i,x_j) = \\left\\{\\begin{matrix} 0 &amp; x_i=x_j\\\\ 1 &amp; e.o.c. \\end{matrix}\\right.\\] Cuando \\(X_{N\\times p}\\) representa un conjunto de datos arbitrario podemos definir \\[d(x_i,x_j) = \\sum_{l=1}^p\\alpha_l d_l(x_{il},x_{jl})\\] donde \\(0\\leq \\alpha_l\\) y \\(sum_{l=1}^p\\alpha_l=1\\), es decir, \\(d\\) es una combinación lineal convexa de las \\(d_l\\) "],
["k-medias.html", "Capítulo 17 K-medias 17.1 Algorítmo k-medias 17.2 Variantes 17.3 Desventajas 17.4 K-medias en R", " Capítulo 17 K-medias Es un algorítmo iterativo descendiente cuyo uso está limitado a conjuntos de datos numéricos. La idea detrás del algorítmo es encontrar clústers cuya variación interna sea tan pequeña como sea posible. La variación interna de un clúster \\(C_k\\) es una medida \\(W(C_k)\\) de qué tanto difieren las observaciones pertenecientes a un clúster, por lo que buscamos resolver el problema: \\[\\begin{equation} \\min_{C_1,\\dots,C_k}\\{\\sum_{k=1}^K W(C_k)\\} \\tag{17.1} \\end{equation}\\] Comúnmnete se utiliza la distancia euclideana como medida de disimilaridad. No se ha probado que exista algún algorítmo que resuelva el problema (17.1), sin embargo existen heurísticas que intentan resolverlo parcialmente. 17.1 Algorítmo k-medias Tomar una partición de tamaño \\(k\\) de manera aleatoria. Para \\(x\\in X\\), se asigna a \\(C_j\\), donde \\[||x-\\bar{x}_{C_j}|| = inf||x-\\bar{x}_{C_l}||\\] Repetir el paso 2 hasta que ningún \\(C_j\\) cambie. Lemma 17.1 (Suma de variaciones internas) En el algorítmo k-medias, el paso 2 nunca incrementa la suma de variaciones internas Es decir, el algoritmo k-medias es un proceso iterativo que divide un conjunto de datos en \\(K\\) grupos excluyentes. Este método es usado extensamente en la literatura, porque es un método sencillo de implementar que utiliza centroides (prototipos) para la representación de los clusters. La calidad de los clusters es mediada por el criterio de variación interna. Este algoritmo produce clusters compactos (de poca dispersión en el interior), pero no toma en consideración la distancia entre clusters, y además el uso de la norma dos hace que el algoritmo sea sensible en presencia de valores atípicos. 17.2 Variantes En términos generales las variantes del algoritmo k-medias difieren en el momento del algoritmo en que se hace la asignación de clusters, entre las variantes más utilizadas en la práctica están las siguientes: Algoritmo de Forgy-Lloyd: los centroides son recalculados después de que todos los individuos fueron asignados. El algoritmo realiza iteraciones hasta que se obtiene la convergencia. Algoritmo de McQueen: Los centroides son recalculados inmediatamente después de cada asignación, y al final de todas las asignaciones. El algoritmo da un único barrido a los datos. Algoritmo de Hartigan: Se selecciona un elemento de cada partición, después se recalculan los centroides sin considerar los elementos seleccionados. Por último se asignan los elementos seleccionados al cluster cuyo centroide sea el más cercano. 17.3 Desventajas Sensibilidad a la partición inicial: como el algoritmo consiste de una búsqueda local, este es muy sensible a la selección inicial de clusters. Falta de robustéz: esta desventaje se hereda del hecho de que la media y varianza son sensibles ante valores atípicos. Número de clusters desconocido: el algoritmo no proporciona información alguna del número de clusters. No es adecuado para variables nominales: no hay una definición de media muestral para tales variables. 17.4 K-medias en R WIP "],
["clusterización-jerárquica.html", "Capítulo 18 Clusterización jerárquica", " Capítulo 18 Clusterización jerárquica A diferencia del método k-medias, la clústerización jerárquica no requiere de una previa especificación del número de clústers a encontrar ni de la configuración inicial de ellos. Como el nombre lo dice, estos clusterización producen representaciones jerárquicas en las cuales los clústers de cada nivel jerárquico son creados a través de la unión de clústers en el nivel inmediato inferior. En el nivel más bajo todos los elementos del conjunto forman un clúster "],
["motivación.html", "Capítulo 19 Motivación 19.1 Esquemas básicos 19.2 Esquemas adicionales 19.3 Information Value &amp; WoE 19.4 Matrices de confusión (y derivados…) 19.5 Curvas AUC-ROC", " Capítulo 19 Motivación En estadística nos enfrentaremos a una número importante de modelos que pueden usarse para explicar el fenómeno presente en nuestros datos. Y además para cada uno de ellos siempre existirá la pregunta ¿qué variables deberíamos incluir? Para dar respuesta a esta necesidad es importante contar con un esquema de selección de modelos que nos permita llegar al más adecuado de manera óptima. Un esquema de selección debe combinar, por un lado, una estrategia de busqueda en el espacio de modelos posibles y, por otro lado, un criterio de comparación que nos permita evaluar la calidad de cada modelo. En ocasiones será sencillo escoger al grupo de modelos candidatos sin embargo para otros problemas las psoibilidades pueden ser enormes. Cuando el número de posibles modelos es muy grande generalmente se emplea una técnica ambiciosa que parte de un modelo inicial y en cada paso se explora el espacio de modelos posibles escogiendo aquel que sea mejor de entre los cercanos al último explorado. 19.0.1 Ejemplo Para un modelo lineal cuyo un espacio de inputs de tamaño p hay \\(2^p\\) posibles sub-modelos. Si p fuera suficientemente pequeño podríamos listar todos los modelos sin embargo en la práctica p suele ser grande. En este caso, para llegar a un modelo adecuado suele utilizarse alguna de las siguientes dos técnicas: Forward Selection: Se empieza con el modelo sin variables y éstas se agregan una por una escogiendo aquella que cumple cierto criterio e.g. que la variable pase la prueba de significancia o que al agregarla al modelo el accuracy mejore en cierto grado. Backward Selection: Empezamos con el modelo de todas las variables y vamos quitando aquellas menos importantes de acuerdo a algún otro criterio e.g. nivel de significancia para el modelo. 19.1 Esquemas básicos Akaike Information Criterion Bayesian Information Criterion Cross-validation El esquema Akaike Information Criterion (AIC) busca maximizar la probabilidad de seleccionar el mejor modelo bajo el supuesto de que éste estuvo dentro del espacio de modelos evaluados. Por otro lado, los esquemas Bayesian Information Criterion (BIC) y validación cruzada buscan optimizar el desempeño predictivo del modelo elegido. 19.1.1 Akaike Information Criterion. Es una aproximación asíntótica a la divergencia Kullback-Leibler entre el modelo de interés y la verdad. También llamada entropía relativa, se define como la esperanza del logaritmo de las diferencias entre P y Q. Supongamos una colección de modelos \\(\\mathbb{M} = \\{\\mathcal{M_1},\\dots,\\mathcal{M_K}\\}\\) donde \\(\\mathcal{M_k}:=\\{f(y|\\theta_k):\\theta_k\\in\\Theta_k\\}\\). Para cada \\(\\mathcal{M}_k\\) sea \\(\\hat{\\theta}_k\\) el estimador máximo verosímil y \\(\\hat{f}_k=f(.|\\hat{\\theta}_k)\\). Usando la divergencia Kullback-Leibler podemos calcular \\[D(f_0||\\hat{f}_k)=\\int f_0\\ log(f_0)-\\int f_0\\ log(\\hat{f}_k)\\] Dado que en realidad observamos la distribución empírica podemos estimar el término negativo como \\[\\hat{H}_k=\\frac{l_k(\\hat{\\theta}_k)}{n}\\] Akaike propuse el siguiente estimador: \\[\\hat{H}_k=\\frac{l_k(\\hat{\\theta}_k)-dim(\\Theta_k)}{n}\\] De donde se deriva el número AIC como: \\[AIC(\\mathcal{M}_k)=-2n \\hat{H}_k\\] Un error común es pensar que el AIC solo puede usarse en modelos anidados sin embargo puede usarse entre modelos distintos siempre que la verosimilitud se calcule con los mismos datos. 19.1.2 Bayesian Information Criterion. BIC es una aproximación para la selección de modelos Bayesiana a posteriori, máxima, dados los datos. Supongamos que establecemos una probabilidad a priori \\(p_k\\) para el modelo \\(\\mathcal{M}_k\\) y una a priori para \\(\\theta_k|\\mathcal{M}_K\\) de \\(\\pi_k\\). Buscamos elegir el modelo con la mayor probabilidad a posteriori; del teorema de Bayes la log-probabilidad a posteriori es: \\[log(\\mathbb{P[\\mathcal{M}|_1,...,Y_n]})=const+log(p_k)+log(\\int exp(l_k(\\theta_k))\\pi_k(\\theta_k) d\\theta_k\\] De donde derivamos que el mejor modelo se obtiene al minimizar \\[BIC(\\mathcal{M}_k)=-2l_k(\\hat{\\theta}_k)+dim(\\Theta_k)log(n)\\] Comparado con el AIC, se impone una mayor penalización por cada parámetro adicional, por lo que el BIC tenderá a seleccionar modelos más simples. 19.1.3 Cross-validation Si buscamos elegir el modelo cuyo desempeño predictivo sea el mejor, lo ideal es contar con un conjunto de prueba aislado. En ausencia de esto podemos probar el modelo con una parte de los datos de entrenamiento. Esto puede hacerse repetidamente escogiendo porciones distintas cada vez. 19.1.3.1 V-fold cross-validation Los datos se dividen en \\(V\\) subconjuntos del mismo tamaño. En cada paso usamos \\(V-1\\) subconjuntos para estimar los parámetros (entrenar el modelo) y probamos en el subconjunto restante. Repertimos \\(V\\) ocasiones y se reporta el error promedio. Elecciones comunes del valor \\(V\\) son: 5, 10 y n. Para n se define leave-one-out cross validation. Donde se usan todos los datos salvo una observación y se predice para ella. Para el caso continuo se utiliza MSE y para clasificación el número de observaciones mal clasificadas. 19.2 Esquemas adicionales Bootstrap. Matrices de confusión. Information Value. Curvas CAP y ROC. 19.2.1 Bootstrap Dado que nuestros modelos están construidos con la información que pudimos observar y no con la información de la población, surgen las siguientes preguntas: ¿Hasta qué grado podemos confiar en que nuestros resultados serán ciertos para toda la población? ¿Qué tanto podrían variar bajo distintos sesgos en la información usada? La técnica de bootstrapping trata de resolver estas preguntas y con ello evaluar la calidad de un modelo a través del resampleo de estadísticos de un modelo. 19.2.1.1 Ejemplo Utilizando la técnica de bootstrapping evaluaremos las variaciones en la \\(R^2\\) de un modelo de regresión lineal. En primer llugar debemos definir una función que extraiga la(s) métrica(s) de interés, en este caso la \\(R^2\\): library(boot) r2 &lt;- function(formula, data, index){ d &lt;- data[index,] mod &lt;- lm(formula, data = d) return(summary(mod)$r.square) } Posteriormente usamos el paquete boot para aplicar la técnica y evaluar las variaciones de la \\(R^2\\) bajo diferentes escenarios muestrales: results &lt;- boot(data = mtcars, statistic = r2, R = 1000, formula = mpg~wt+disp) Podemos imprimir los resultados: results ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = mtcars, statistic = r2, R = 1000, formula = mpg ~ ## wt + disp) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 0.7809306 0.01184656 0.05052901 O bien, podemos graficarlos para leerlos más facilmente: plot(results) 19.2.1.2 Ejercicios Aplique la técnica de bootstrapping para evaluar variaciones en la \\(R^2\\) de los siguientes modelos al mismo tiempo: mpg~data$wt+disp mpg~data$wt+disp+cyl mpg~data$wt+disp+cyl+hp Aplique la técnica de bootstrapping para otro estadístico que le parezca relevante. 19.3 Information Value &amp; WoE Derivaron del uso de regresión logística particularmente en problemas de riesgo de crédito. Se utilizan para medir qué tan bien una variable logra distinguir una respuesta binaria. 19.3.1 Weight of Evidence (WoE) Se calcula de la siguiente forma: \\[WoE_{x=i} = log(\\frac{P[y=1 |x=i]}{P[y=0|x=i]})\\] 19.3.2 Information Value Su cálculo se realiza de la siguiente forma: \\[IV_{x_i} = (P[Y = 1|x=i]-P[y=0|x=i])*WoE_{x_i}\\] 19.3.2.1 Interpretación del Information Value IVx Poder predictivo &lt;0.02 Variable no útil 0.02-0.1 Poder débil 0.1-0.3 Poder medio 0.3-0.5 Poder alto 0.5 Sospechosa 19.4 Matrices de confusión (y derivados…) Son una forma de medir el desempeño de un algorítmo de clasificación. De ellas derivan diferentes medidas que nos ayudan a entender más a fondo el desempeño de nuestro algorítmo. En un problema de clasificación de 2 clases, la matriz se construye con la tabla cruzada entre las clases reales y las clases ajustadas/predichas Positivo Real Negativo Real Positivo predicho TP FP Negativo predicho FN TN Las matrices de confusión se pueden extender a problemás de más de dos clases. 19.4.1 Estadísticos derivados de las matrices de confusión \\(Sensitivity = \\frac{TP}{TP+FN}\\) También llamada tasa de verdaderos positivos, mide la proporción de positivos predichos de entre los verdaderos reales. \\(Specificity = \\frac{TN}{TN+FP}\\) O tasa de verdaderos negativos, mide la proporción de negativos predichos de entre los negativos reales. \\(Prevalence = \\frac{TP+FN}{TP+FP+TN+FN}\\) Mide cuántos valores reales hay \\(PPV = \\frac{sensitivity*prevalence}{(sensitivity*prevalence)+((1-specificity)*(1-prevalence))}\\) PPV: positive predicted values \\(NPV = \\frac{sensitivity*(1-prevalence)}{((1-sensitivity)*prevalence)+((specificity)*(1-prevalence))}\\) NPV: negative predicted values \\(Detection \\ rate = \\frac{TP}{TP+FP+TN+FN}\\) Mide cuántos verdaderos positivos esta detectando el modelo \\(Detection \\ prevalence = \\frac{TP+FP}{TP+FP+TN+FN}\\) Mide cuántos positivos predichos tiene el modelo \\(Balanced \\ accuracy = \\frac{sensitivity+specificity}{2}\\) \\(Precision = \\frac{TP}{TP+FP}\\) Proporción de verdaderos positivos de entre los positivos predichos \\(Recall = \\frac{TP}{TP+FN}\\) Proporción de verdaderos positivos de entre los positivos reales \\(F-beta = \\frac{(1+beta^2)*precision*recall}{(beta^2*precision)+recall}\\) 19.5 Curvas AUC-ROC Muchos algoritmos de clasificación no generan directamente el vector de clases predichas sino que primero obtienen el vector de probabilidades de pertenencia a cada clase. Dado un umbral o punto de corte para el vector de probabilidades se puede generar entonces un vector de clases asociado a ese vector. Es claro que el valor de ese umbral afectará directamente al accuracy de clasificación del modelo. Las curvas AUC-ROC nos permiten medir el desempeño del algorítmo bajo distintos umbrales. ROC es una curva probabilística mientras que AUC es una medida de separación. 19.5.1 Construcción de una curva AUC-ROC Para generar una curva de ROC debemos graficar la métrica 1-specificity vs sensitivity. Para obtener el valor AUC debemos calcular el área bajo la curva ROC. "]
]
