[
["index.html", "Herramientas Estadísticas para Ciencia de Datos Herramientas Estadísticas para Ciencia de Datos 0.1 Objetivos 0.2 Estructura 0.3 Detalles técnicos Licencia", " Herramientas Estadísticas para Ciencia de Datos Sofía Villers Gómez David Alberto Mateos Montes de Oca Herramientas Estadísticas para Ciencia de Datos Primera edición del libro de texto para el curso Herramientas Estadísticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias. 0.1 Objetivos Como el título lo indica, a lo largo de este libro se expondrán diferentes modelos estadísticos y sus aplicaciones con un enfoque a Ciencia de Datos. El objetivo es proveer al lector de las herramientas necesarias para comprender los fundamentos de estos modelos y sus aplicaciones mediante el uso del lenguaje R. 0.2 Estructura El libro se descompone en dos grandes secciones, una de ellas enfocada a presentar diferentes paquetes de R considerados de gran utilidad para la práctica de Ciencia de Datos, la otra se enfoca en la teoría y aplicación de diferentes modelos. La estructura del libro sigue el orden en que se imparte la materia en la Facultad de Ciencias sin embargo hemos diseñado los capítulos suficientemente independientes como para ser consultados en el orden de preferencia del lector. 0.3 Detalles técnicos Este libro fue escrito con bookdown usando RStudio. Esta versión fue escrita con: ## Finding R package dependencies ... Done! ## setting value ## version R version 4.0.0 (2020-04-24) ## os Windows 10 x64 ## system x86_64, mingw32 ## ui RStudio ## language (EN) ## collate Spanish_Mexico.1252 ## ctype Spanish_Mexico.1252 ## tz America/Mexico_City ## date 2020-09-11 Licencia This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],
["instalando-r-y-rstudio.html", "Capítulo 1 Instalando R y RStudio 1.1 R y RStudio 1.2 RStudio Cloud 1.3 Recursos adicionales", " Capítulo 1 Instalando R y RStudio 1.1 R y RStudio Instala R, a free software environment for statistical computing and graphics desde el CRAN de acuerdo a tu sistema operativo. Instala el entorno de desarrollo integrado RStudio Si previamente habías instalado alguna versión de R y RStudio en tu computadora, sugerimos reinstalar ambos para tener las versiones más actualizadas. 1.2 RStudio Cloud Aunque a lo largo del libro se mostrará a detalle el código de R usado, también proveeremos de links a proyectos de RStudio Cloud para facilitar reproducir los resultados aquí mostrados. Para acceder a tales proyectos será necesario contar con una cuenta en RStudio Cloud. 1.3 Recursos adicionales Si no estás del todo familiarizado con el lenguaje, te recomendamos lo siguientes recursos donde podrás encontrar información útil sobre R y RStudio. Cómo usar RStudio RStudio para el aprendizaje de R R: Preguntas Frecuentes R: Instalación y administración "],
["conocimientos-básicos-de-r.html", "Capítulo 2 Conocimientos básicos de R 2.1 Buscando ayuda 2.2 Vectores 2.3 El ambiente (environment). 2.4 Lectura y escritura de archivos 2.5 Algunas funciones variadas 2.6 Objetos 2.7 Algunos ejemplos más avanzados 2.8 Paquetes", " Capítulo 2 Conocimientos básicos de R 2.1 Buscando ayuda Por sí mismo, R cuenta con funciones que nos permitirán obtener ayuda sobre R en general pero también sobre las diferentes funciones que estemos usando para resolver algún problema. Para mostrar un manual de ayuda general en formato HTML corremos el siguiente código en la consola: help.start() El manual será mostrado en el panel de ayuda dentro de RStudio pero puede abrirse con el navegador en caso de que así lo deseemos. Supongamos ahora que dentro de nuestro flujo de trabajo estamos utilizando la funcíón log() sin embargo no estamos familiarizados con ella y deseamos obtener ayuda sobre el uso de la función. Para a tener acceso a la documentación de la función podemos usar alguna de las siguientes líneas de código: help(log) ?log En el caso particular de RStudio, existe un atajo con el que podemos abrir la documentación de cualquier función sin necesidad de correr alguna de las líneas anteriores en la consola. El atajo consiste de colocar el cursor de texto entre cualesquiera caractéres de la función y presionar el botón F1 de nuestro teclado. Adicionalmente tenemos las siguientes funciones: apropos(&quot;plot&quot;) help.search(&quot;plot&quot;) apropos() nos ayudará a encontrar las funciones que incluyan el texto (en este caso plot) en su nombre. Por otro lado, help.search() nos mostrará la documentación de las funciones que incluyan el texto, en este caso plot. Como se indica, cualquiera de esas opciones te permitirá tener acceso a la documentación de la funciones donde podrás encontrar: para qué se usan, qué argumentos necesitan, lo que regresan y algunos ejemplos del uso de las mismas. Una buena parte de los errores que podamos encontrarnos en nuestros flujos de trabajo se resolverán al revisar la documentación de la función que esté generando el error, sin embargo habrá ocasiones en las que los errores sean más complejos y la documentación no nos dará una buena pista de cómo remediarlos, en tal caso la mejor estrategía sera utilizar tu buscador favorito para buscar el error. Será tan fácil cómo copiar el error que la consola esté arrojando y ponerlo en la barra de búsqueda de e.g. [Google][google]. 2.2 Vectores Quizá ya hayas escuchado lo siguiente en algún lado: R es un lenguaje vectorial. El comportamiento al que hace referencia esta frase será de mucha ayuda en la construcción de rutinas avanzadas de programación que veremos más adelante, por ahora la implicación más relevante reside en el hecho de que la estructura básica en R serán justamente vectores. Los siguientes ejemplos nos muestran cómo crear vectores numéricos en R: roma &lt;- c(0.1, 0.2, 0.3) assign(&quot;mora&quot;, c(0.4, 0.5, 0.6)) c(0.7, 0.8, 0.9) -&gt; z mora &lt;- c(roma, 0, 0, 0, roma) N.B. Los vectores en R no solo pueden ser numéricos también los hay aquellos con cadenas de texto. En seguida vemos algunos ejemplos que nos permitirán acceder a elementos de un vector: mora[3] ## [1] 0.3 mora[-3] ## [1] 0.1 0.2 0.0 0.0 0.0 0.1 0.2 0.3 mora[c(1,5,7)] ## [1] 0.1 0.0 0.1 2.2.1 Arítmetica de vectores El siguiente ejemplo muestra el comportamiento asociado a un lenguaje vectorial que mencionabamos anteriormente: v &lt;- 2*roma+mora+1 v ## [1] 1.3 1.6 1.9 1.2 1.4 1.6 1.3 1.6 1.9 Existen una enorme cantidad de funciones para manipular los datos dentro de nuestros vectores, en seguida se muestran algunas: sum(v) ## suma todos los elementos del objeto prod(v) ## multiplica todos los elementos del objeto max(v) ## encuentra el valor máximo min(v) ## encuentra el valor mínimo which.max(v) ## índice o posición del máximo which.min(v) ## índice o posición del mínimo range(v) ## vector de dos entradas con el mínimo y máximo respectivamente 2.3 El ambiente (environment). Los objetos que guardemos o asignemos en nuestros flujos de trabajo serán almacenados en un espacio de memoria que se llama ambiente o en inglés environment. Desde RStudio podremos ver lo que se guardado en este espacio de memoria en el panel Environment. Adicional a ello, existen algunas funciones que nos permiten explorar nuestro ambiente: ## Muestra los nombres de los objetos en memoria ls() ## Muestra las variables con cierta serie de caracteres en su nombre ls(pat=&quot;m&quot;) ## Muestra las variables las cuales su nombre empieza con el caracter dado ls(pat=&quot;^m&quot;) ## Muestra detalles de los objetos en memoria ls.str() ## Eliminar todas las variables de &#39;Global Environment&#39; rm(list=ls()) ## Eliminar únicamente variables que empiezan con la letra m rm(list=ls(pat=&quot;^m&quot;)) ## Tipo de elementos del objeto v mode(v) ## Longitud del objeto v length(v) 2.4 Lectura y escritura de archivos En cuanto a archivos externos, a lo largo del libro únicamente usaremos archivos con extensiones .csv y .txt sin embargo R no está limitado a este tipo de archivos, existen una amplia variedad de formatos que se pueden leer a través de paquetes diseñados específicamente para ese fin. El siguiente ejemplo muestra cómo se haría la lectura de un archivo con extensión .txt: datos&lt;-read.table(&quot;data.txt&quot;, # nombre del archivo (con extensión) entre comillas header = TRUE, # TRUE o FALSE, indicando si el archivo tiene como primer renglón el nombre de las columnas sep=&quot;\\t&quot; # separador de los campos ) Una vez que leemos el archivo externo en R, el objeto donde se almacenará la información será de tipo data.frame. Muchas veces será de nuestro interés exportar objetos data.frame a archivos externos, tal objetivo lo podemos lograr con alguna de las siguientes dos opciones: write.table(datos, &quot;toma.txt&quot;, append=F, sep=&quot;\\t&quot;) write.csv(datos, &quot;toma2.csv&quot;) N:B. Como en la lectura de datos, R no está limitado a exportar archivos .csv o .txt también existe una amplia variedad de formatos que podemos generar. 2.5 Algunas funciones variadas A continuación se muestra un conjunto bastante pequeño de funciones que nos ayudarán a alcanzar distintos objetivos: ## Generar un vector con valores subsecuentes x &lt;- 1:30 ## Generar un vector con cierta secuencia y &lt;- seq(1, 5, 0.5) w &lt;- seq(10, 0, -0.5) ## Genera un vector que repite un dato cierto número de veces z &lt;- rep(1, 20) ## Genera un vector con series valores subsecuentes hasta los números indicados q &lt;- sequence(3:5) q &lt;- sequence(c(10, 5)) ## Genera series regulares de factores dados. q &lt;- gl(3, 5, length = 30) q &lt;- gl(2, 6, label = c(&quot;Hombre&quot;,&quot;Mujer&quot;)) ## A diferencia de los anteriores, la siguiente función genera un data frame con todas las ## posible combinaciones de vectores o factores dados como argumentos q &lt;- expand.grid(h = c(60, 80), w = c(100, 300), sex = c(&quot;Hombre&quot;, &quot;Mujer&quot;)) 2.6 Objetos Los ejemplos anteriores consisten en el uso de funciones, dichas funciones generan cierto tipo de objetos para regresar el resultado. Si queremos crear de manera manual cierto tipo de objetos, podemos hacerlo de la siguiente manera: 2.6.1 Vectores ## Vector (numeric, logical, character) a &lt;- vector(mode= &quot;logical&quot;, length=5) b &lt;- logical(length=5) 2.6.2 Factores a &lt;- factor(1:3, labels = c(&quot;Hola&quot;, &quot;Adios&quot;, &quot;Ah&quot;)) b&lt;- factor(x = c(2, 5), levels = 2:5) c &lt;- factor(1:5, exclude = 4) 2.6.3 Matrices a &lt;- matrix(1:6, 2, 3, byrow = F) ## Otra forma de crear una matriz b &lt;- 1:15 dim(b) &lt;- c(5,3) 2.6.4 Data Frames x &lt;- 1:4 n &lt;- 10 M &lt;- c(10, 35) a &lt;- data.frame(x, n, M) 2.6.5 Listas a &lt;- list(x, M) b &lt;- list(A = x, B = M) 2.6.6 Expresiones x &lt;- 3 y &lt;- 2.5 z &lt;- 1 exp &lt;- expression(x/(y + exp(z))) 2.7 Algunos ejemplos más avanzados 2.7.1 Operadores Aritméticos Comparación Lógicos Suma (+) Menor que (&lt;) NOT (!) Resta (-) Mayor que (&gt;) AND (&amp;) Multiplicación (*) Menor o igual que (&lt;=) OR (|) División (/) Mayor o igual que (&gt;=) Cierto (TRUE) Potencia (^) Igual (==) Falso (FALSE) Modulo (%%) Diferente (!=) División entera (%/%) 2.7.2 Ciclos, condiciones y funciones Al ser R un lenguaje de programación podemos usar: ciclos ## Ciclo for for (anio in c(2000,2001,2002,2003,2004,2005,2006,2007)){ print(paste(&quot;Cuenta&quot;,anio)) } ## Ciclo while i &lt;- 1 while(i &lt; 10){ print(i) i &lt;- i+1 } condiciones for (i in 1:10){ ## Condición if if(!i %% 2){ next } print(i) } Particularmente en la programación con R, será de mucha utilidad aprender a crear funciones: funcionfactorial&lt;-function(a){ res &lt;- 1 if(a&lt;0){ return(&quot;No existen factoriales de números negativos.&quot;) } else if(a==0){ return(res) } else{ for(i in 1:a){ res &lt;- res*i } return(res) } } 2.8 Paquetes En R llamamos librerías o paquetes a conjuntos de funciones diseñados para un fin específico. Por ejemplo el paquete [ggplot2][ggplot2-github] contiene funciones diseñadas para crear gráficos usando una sintáxis llamada la gramática de gráficas. Para descargar alguna librería de nuestra interés debemos utilizar el siguiente código: install.packages(&quot;ggplot2&quot;) # Dentro de las comillas ponemos el nombre del paquete que queremos instalar Dado que se trata de una instalación, el código anterior solamente será necesario la primera vez que instalemos el paquete. Una vez instalado, necesitamos hacer que R sepa que queremos usar la funciones de ese paquete, para lo cual usamos el siguiente código: library(ggplot2) # El argumento de la función será el nombre del paquete previamente instalado "],
["notación.html", "Capítulo 3 Notación", " Capítulo 3 Notación A lo largo del libro usaremos la notación típica de estadística pero también haremos uso de la siguiente: \\(x^{(i)}\\): el conjunto de inputs (variables explicativas) \\(y^{(i)}\\): es la variable de output/salida (variable dependiente) que queremos predecir (ajustar) A la pareja \\((x^{(i)},y^{(i)})\\) le llamaremos ejemplo de entrenamiento El conjunto de entrenamiento se denota por: \\(\\{(x^{(i)},y^{(i)})|i\\in N\\}\\) De forma general, denotaremos por \\(\\mathcal{X}\\) al espacio de inputs y por \\(\\mathcal{Y}\\) al espacio de outputs N.B. Omitiremos el uso de indices en donde sea claro a qué nos referimos. "],
["glosario-dscml-estadística.html", "Capítulo 4 Glosario DSc/ML - Estadística", " Capítulo 4 Glosario DSc/ML - Estadística Machine Learning / Ciencia de Datos Estadística red, grafo (network, graphs) modelo (model) pesos (weigths) parámetros (parameters) aprendizaje (learning) ajuste (fiting) prueba, generalización (testing, generalization) ajuste en el conjunto de prueba aprendizaje supervisado (supervised learning) regresión, clasificación aprendizaje no supervisado (unsupervised learning) estimación de densidades, clusterización "],
["entrenamiento-de-modelos.html", "Capítulo 5 Entrenamiento de modelos", " Capítulo 5 Entrenamiento de modelos Se le denomina de esta manera a la acción de ajustar el mejor modelo a los datos. Formalmente se define como sigue: Dado un conjunto de entrenamiento \\((x^{(i)},y^{(i)})\\in(\\mathcal{X} \\times \\mathcal{Y})\\) el objetivo es aprender (ajustar) una función \\(h:\\mathcal{X}\\rightarrow \\mathcal{Y}\\) tal que \\(h(x)\\) sea un buen predictor de \\(y\\). La función \\(h\\) suele llamarse hipótesis. Cuando el conjunto \\(\\mathcal{Y}\\) es continuo, estamos frente a un problema de regresión. Si se trata de un conjunto discreto entonces tenemos un problema de clasificación. "],
["regresión-lineal.html", "Capítulo 6 Regresión Lineal 6.1 Un poco de história 6.2 Objetivos del análisis de regresión 6.3 El algorítmo de regresión lineal 6.4 Regresión lineal simple 6.5 Solución al problema de regresión lineal simple 6.6 Regresión lineal múltiple 6.7 Solución al problema de regresión lineal múltiple. 6.8 Aplicación en R", " Capítulo 6 Regresión Lineal 6.1 Un poco de história Los primeros problemas prácticos tipo regresión iniciaron en el siglo XVIII, relacionados con la navegación basada en la Astronomía. Legendre desarrolló el método de mínimo cuadrados en 1805. Gauss afirma que él desarrolló este método algunos años antes y demuestra, en 1809, que mínimos cuadrados proporciona una solución óptima cuando los errores se distribuyen normal. Francis Galton acuña el término regresión al utilizar el modelo para explicar el fenómeno de que los hijos de padres altos, tienden a ser altos en su generación, pero no tan altos como lo fueron sus padres en la propia, por lo que hay un efecto de regresión. El modelo de regresión lineal es, probablemente, el modelo de su tipo más conocido en estadística. El modelo de regresión se usa para explicar o modelar la relación entre una sola variable, \\(y\\), llamada dependiente o respuesta, y una o más variables predictoras, independientes, covariables, o explicativas, \\(x_1, x_2, ..., x_p\\). Si \\(p = 1\\), se trata de un modelo de regresión simple y si \\(p &gt; 1\\), de un modelo de regresión múltiple. En este modelo se asume que la variable de respuesta, \\(y\\), es aleatoria y las variables explicativas son fijas, es decir, no aleatorias. La variable de respuesta debe ser continua, pero los regresores pueden tener cualquier escala de medición. 6.2 Objetivos del análisis de regresión Existen varios objetivos dentro del análisis de regresión, entre otros: Determinar el efecto, o relación, entre las variables explicativas y la respuesta. Predicción de una observación futura. Describir de manera general la estructura de los datos. 6.3 El algorítmo de regresión lineal Sea \\(\\Phi: \\mathcal{X} \\rightarrow \\mathbb{R}^N\\) y consideremos la familia de hipótesis lineales \\[H=\\{x\\mapsto w \\cdot \\Phi(x)+b | w\\in\\mathbb{R}^N, b\\in\\mathbb{R}\\}\\] La regresión lineal consiste en buscar la hipótesis \\(h\\in H\\) con el menor error cuadrático medio, es decir, se debe resolver el problema de optimización: \\[\\min \\frac{1}{m}\\sum_{i=1}^{m}(h(x_i)-y_i)^2\\] 6.4 Regresión lineal simple Para este modelo supondremos que nuestra respuesta, \\(y\\), es explicada únicamente por una covariable, \\(x\\). Entonces, escribimos nuestro modelo como: \\[y^{(i)}=\\beta_0+\\beta_1x^{(i)}+\\epsilon^{(i)},\\ \\ i=1,2,\\dots,n\\] Como podemos observar, se ha propuesto una relación lineal entre la variable \\(y\\) y la variable explicativa \\(x\\), que es nuestro primer supuesto sobre el modelo: La relación funcional entre \\(x\\) y \\(y\\) es una línea recta. Observamos que la relación no es perfecta, ya que se agrega el término de error, \\(\\epsilon\\). Dado que la parte aleatoria del modelo es la variable \\(y\\), asumimos que al error se le “cargan” los errores de medición de \\(y\\), así como las perturbaciones que le pudieran ocasionar los términos omitidos en el modelo. Gauss desarrolló este modelo a partir de la teoría de errores de medición, que es de donde se desprenden los supuestos sobre este término: \\(\\mathbb{E}(\\epsilon^{(i)})=0\\) \\(\\mathbb{V}ar(\\epsilon^{(i)})=\\sigma^2\\) \\(\\mathbb{C}ov(\\epsilon^{(i)},\\epsilon^{(j)})=0, \\ \\forall i\\neq j\\) N.B. Los errores \\(\\epsilon^{(i)}\\) son variables aleatorias no observables. 6.5 Solución al problema de regresión lineal simple 6.5.1 Mínimos cuadrados ordinarios En una situación real, tenemos \\(n\\) observaciones de la variable de respuesta así como de la variable explicativa, que conforman las parejas de entrenamiento \\((x_i, y_i), \\ i = 1, 2, ..., n\\). Entonces, nuestro objetivo será encontrar la recta que mejor ajuste a los datos observados. Utilizaremos el método de mínimos cuadrados para estimar los parámetros del modelo, que consiste en minimizar la suma de los errores al cuadrado, esto es: \\[\\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n(y_i-(\\beta_0+\\beta_1x^{(i)}))^2\\] Al minimizar la expresión anteriore obtenemos las siguientes expresiones para los estimadores: \\[\\hat{\\beta_1}=\\frac{\\sum_{i=1}^ny_i(x_i-\\bar{x})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\] \\[\\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x}\\] Una desventaja del método de mínimos cuadrados, es que no se pueden hacer procesos de inferencia sobre los parámetros de interés \\(\\beta_0\\) y \\(\\beta_1\\); procesos como intervalos de confianza o pruebas de hipótesis. Para subsanar esta deficiencia, es necesario asumir una distribución para el error, \\(\\epsilon_i\\), que, siguiendo la teoría general de errores, se asume que tiene distribución normal, con media cero y varianza \\(\\sigma^2\\). Este supuesto garantiza que las distribuciones de \\(y_i,\\ \\hat{\\beta_0},\\ \\hat{\\beta_1}\\) sean normales, lo que permite tanto la construcción de intervalos de confianza como de pruebas de hipótesis. N.B El estimador de \\(\\sigma^2\\) está dado por \\(\\hat{\\sigma^2}=\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{n-2}\\) 6.5.1.1 Pruebas de hipótesis En el modelo de regresión lineal simple, la prueba de hipótesis más importante es determinar si estadísticamente existe la dependencia líneal entre \\(x\\) y \\(y\\), y que no sea producto del muestreo (debido al azar). Es decir, realizar la prueba de hipótesis: \\[H_0:\\beta_1=0 \\ vs.\\ H_a:\\beta_1\\neq 0\\] No rechazar la hipótesis nula, implicaría que la variable \\(x\\) no ayuda a explicar a \\(y\\) o bien que, tal vez, la relación entre estas variables no es lineal. En este modelo, esta última explicación es un poco cuestionable, ya que se parte, de inicio, del diagrama de dispersión de los datos. Si rechazamos la hipótesis nula, implicará que \\(x\\) es importante para explicar la respuesta \\(y\\) y que la relación lineal entre ellas puede ser adecuada. Rechazar esta hipótesis nula, también podría implicar que existe una relación lineal entre las variables pero, tal vez, se pueda mejorar el ajuste con algún otro término no lineal. 6.5.1.2 Interpretación de los parámetros Cuando se tiene una recta en el sentido determinista, los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) tienen una interpretación muy clara; \\(\\beta_0\\) se interpreta como el valor de \\(y\\) cuando \\(x\\) es igual a cero y \\(\\beta_1\\) como el cambio que experimenta la variable de respuesta \\(y\\) por unidad de cambio en \\(x\\). La interpretación, desde el punto de vista estadístico, de los parámetros estimados en el modelo de regresión es muy similar: \\(\\hat{\\beta_0}\\) es el promedio esperado de la respuesta \\(y\\) cuando \\(x = 0\\) (este parámetro tendrá una interpretación dentro del modelo, si tiene sentido que \\(x\\) tome el valor cero, de lo contrario, no tiene una interpretación razonable) y \\(\\hat{\\beta_1}}\\) es el cambio promedio o cambio esperado en \\(y\\) por unidad de cambio en \\(x\\). 6.6 Regresión lineal múltiple La mayoría de los fenómenos reales son multicausales, por esta razón, un modelo de regresión más acorde a estudios reales es el modelo de regresión lineal múltiple, que es la generalización del modelo simple. En este modelo supondremos que la variable de respuesta, \\(y\\), puede explicarse a través de una colección de \\(k\\) covariables \\(x_1,\\dots,x_k\\). El modelo se escribe de la siguiente manera: \\[y_i = \\beta_0+\\beta_1 x_1^{(i)}+\\beta_2 x_2^{(i)}+\\dots++\\beta_k x_k^{(i)}+\\epsilon_i\\] Al igual que en el caso simple, los parámetros del modelo se pueden estimar por mínimos cuadrados, con el inconveniente de que no se pueden realizar inferencias sobre ellos. Nuevamente, para poder hacer intervalos de confianza y pruebas de hipótesis sobre los verdaderos parámetros hay que suponer que el vector de errores se distribuye normal, en este caso multivariada, es decir: \\[\\epsilon\\sim N_n(0,\\sigma^2\\mathbb{I})\\] Esta estructura del error permite tener las mismas propiedades distribucionales que en regresión simple, es decir, \\(y_i\\) se distribuye normal y \\(\\beta_i\\) tiene distribución normal, facilitando las inferencias sobre cada parámetro y la construcción de intervalos de predicción para las \\(y\\)’s. 6.7 Solución al problema de regresión lineal múltiple. 6.7.1 Ecuaciones normales Las expresiones para estimar los parámetros involucrados en el modelo son: \\[\\hat{\\beta}=(X^TX)^{-1}X^Ty\\] \\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{n-p}\\] donde \\(p=k+1\\) es el número total de parámetros en el modelo. Tanto en el modelo simple como en el múltiple, la variación total de las \\(y\\)’s se puede descomponer en una parte que explica el modelo, i.e., los \\(k\\) regresores o variables explicativas y otra no explicada por estas variables, llamada error. \\[\\sum_{i=1}^n(y_i-\\bar{y})^2=\\sum_{i=1}^n(\\hat{y_i}-\\bar{y})^2+\\sum_{i=1}^n(\\hat{y_i}-y_i)^2\\] #### Pruebas de hipótesis La descomposición anterior ayuda para realizar la importante prueba de hipótesis: \\[H_0:\\beta_1=\\beta_2=\\dots=\\beta_k=0\\ vs.\\ H_a:\\beta_i\\neq0 \\ p.a. \\ i\\] misma que se realiza a través del cociente entre los errores cuadráticos medios: \\[F_0=\\frac{SS_R/k}{SS_E/(n-k-1)}=\\frac{MS_R}{MS_E}\\sim F_{(k,n-k-1)}\\] Esta estadística se desprende de la tabla de análisis de varianza, que es muy similar a la tabla ANOVA que se utiliza para hacer pruebas de hipótesis. En este caso la tabla es: Fuente de variación Grados de libertad Suma de cuadrados Cuadrados medios F Regresión k \\(SS_R\\) \\(MS_R=SS_R/k\\) Error n-k-1 \\(SS_E\\) \\(MS_E=SS_E/(n-k-1)\\) \\(F=\\frac{MS_R}{MS_E}\\) Total n-1 \\(S_{yy}\\) Por lo general, esta estadística rechaza la hipótesis nula, ya que de lo contrario, implicaría que ninguna de las variables contribuye a explicar la respuesta, \\(y\\). Como se puede observar en la hipótesis alternativa, el rechazar \\(H_0\\) solo implica que al menos uno de los regresores contribuye significativamente a explicar \\(y\\). Asimismo, el rechazar \\(H_0\\) no implica que todos contribuyan ni tampoco dice cuál o cuáles contribuyen, por esta razón, una salida estándar de regresión múltiple tiene pruebas individuales sobre la significancia de cada regresor en el modelo. El estadístico para hacer tanto los contrastes de hipótesis como los intervalos de confianza individuales, es: \\[t=\\frac{\\hat{\\beta_i}-\\beta_0^{(i)}}{\\sqrt{\\hat{\\mathbb{V}ar}(\\hat{\\beta_i})}}\\sim t_{(n-p)}\\] Podemos apreciar que los constrastes de hipótesis se pueden hacer contra cualquier valor particular del parámetro \\(\\beta_0^{(i)}\\), en general. No obstante, en las pruebas estándar sobre los parámetros de un modelo, este valor particular es 0, ya que se intenta determinar si la variable asociada al \\(i\\)-ésimo parámetro es estadísticamente significativa para explicar la respuesta. Por lo que el estadístico para este caso es: \\[t=\\frac{\\hat{\\beta_i}}{\\sqrt{\\hat{\\mathbb{V}ar}(\\hat{\\beta_i})}}\\sim t_{(n-p)}\\] De este estadístico se desprenden también los intervalos de confianza para cada parámetro: \\[\\beta_i\\in(\\hat{\\beta_i}\\pm t_{(n-p,1-\\alpha/2)} \\sqrt{\\hat{\\mathbb{V}ar} (\\hat{\\beta_i})})\\] #### Interpretación de los parámetros La interpretación de cada parámetro es similar a la del coeficiente de regresión \\(\\hat{\\beta_1}\\) en el modelo simple, anexando la frase: “manteniendo constantes el resto de las variables”. Esto es, \\(\\hat{\\beta_i}}\\) es el cambio promedio o cambio esperado en \\(y\\) por unidad de cambio en \\(x_i\\), sin considerar cambio alguno en ninguna de las otras variables dentro del modelo, es decir, suponiendo que estas otras variables permanecen fijas. Esta interpretación es similar a la que se hace de la derivada parcial en un modelo determinista. Nuevamente, la interpretación de \\(\\hat{\\beta_0}\\) estará sujeta a la posibilidad de que, en este caso, todas las variables puedan tomar el valor cero. 6.7.1.1 Predicción de nuevos valores Uno de los usos más frecuentes del modelo de regresión es el de predecir un valor de la respuesta para un valor particular de las covariables en el modelo. Si la predicción se realiza para un valor de las covariables dentro del rango de observación de las mismas, se tratará de una interpolación, y si se realiza para un valor fuera de este rango, hablaremos de una extrapolación. En cualquiera de los dos casos, estaremos interesados en dos tipos de predicciones: Predicción de la respuesta media: \\(y_0=\\mathbb{E}(y|X_0)\\) Predicción de una nueva observación: \\(y_0\\) En ambos casos, la estimación puntual es la misma: \\(\\hat{y_0}=X_0^T\\hat{\\beta}\\) Lo que difiere es el intervalo de predicción. Para la respuesta media es: \\(y_0=(\\hat{y_0}\\pm t_{(n-p,1-\\alpha/2)}\\sqrt{\\hat{\\sigma^2}X_0^T(X^TX)^{-1}X_0})\\) Y para predecir una observación: \\(y_0=(\\hat{y_0}\\pm t_{(n-p,1-\\alpha/2)}\\sqrt{\\hat{\\sigma^2}(1+X_0^T(X^TX)^{-1}X_0)})\\) 6.7.1.2 Coeficiente de determinación Un primer elemento de juicio sobre el modelo de regresión lo constituye el coeficiente de determinación \\(R^2\\), que es la proporción de variabilidad de las \\(y\\)’s que es explicada por las \\(x\\)’s y que se escribe como: \\[R^2=\\frac{SS_R}{S_{yy}}=1-\\frac{SS_E}{S_{yy}}\\] Una \\(R^2\\) cercana a uno implicaría que mucha de la variabilidad de la respuesta es explicada por el conjunto de regresores incluidos en el modelo. Es deseable tener una \\(R^2\\) grande en nuestro modelo, pero esto no significa, como mucha gente piensa, que ya el modelo está bien ajustado. 6.7.2 Evaluación de supuestos Los dos modelos de regresión presentados, el simple y el múltiple, se construyeron sobre los supuestos de: La relación funcional entre la variable de respuesta \\(y\\) y cada regresor \\(x_i\\) es lineal La esperanza de los errores es cero, \\(\\mathbb{E}(\\epsilon_i=0)\\) La varianza de los errores es constante, \\(\\mathbb{V}ar(\\epsilon_i) = \\sigma^2\\) Los errores no están correlacionados, \\(\\mathbb{C}ov(\\epsilon_i, \\epsilon_j) = 0;\\ i\\neq j\\) Los errores tienen distribución normal con media cero y varianza \\(\\sigma^2\\) Entonces, para garantizar que el modelo es adecuado, es indispensable verificar estos supuestos. 6.7.2.1 Residuos Los elementos más importantes para verificar estos supuestos son los residuos, definidos como: \\[e_i=y_i-\\hat{y}_i\\] Estos residuos representan la discrepancia entre la respuesta predicha por el modelo ajustado, \\(\\hat{y}_i\\) y el correspondiente valor observado, \\(y_i\\). En la literatura de regresi ́on lineal existen cuatro tipos de residuos, a saber Residuo crudo: \\(e_i\\) Residuo estandarizado: \\(d_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma}^2}}\\) Residuo estudentizado interno: \\(r_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma}^2(1-h_{ii}})}\\) Residuo estudentizado externo: \\(t_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma_{(-i)}}^2(1-h_{ii})}}\\) Estos residuos se utilizan en los distintos procedimientos para evaluar los supuestos y lo adecuado del ajuste del modelo. La mayoría de las pruebas conocidas para la verificación de los supuestos, son pruebas gráficas. Indudablemente, la prueba más importante es sobre la normalidad de los errores, ya que sobre este supuesto descansan todas la inferencias de este modelo. La manera de verificarlo es a través de la gráfica conocida como QQ-plot o QQ-norm, que grafica los cuantiles teóricos de una distribución normal (eje x) vs. los cuantiles asociados a los residuos. Entonces, si los residuos realmente provienen de una normal, la gráfica debe mostrar la función identidad. Fuertes desviaciones de esta línea darían evidencia de que los errores no se distribuyen normal. 6.7.2.2 Linealidad de los predictores La manera estándar de evaluar la linealidad de las variables explicativas es a través de la gráfica de cada una de ellas contra los residuos. Si la variable en cuestión ingresa al modelo de manera lineal, esta gráfica debe mostrar un patrón totalmente aleatorio entre los puntos dispuestos en ella. Cuando la variable explicativa es politómica, este tipo de gráficas son poco ilustrativas en este sentido. 6.7.2.3 Supuestos sobre los errores Si la gráfica entre los valores ajustados y los residuos estandarizados, muestra un patrón aleatorio, es simétrica alrededor del cero y los puntos están comprendidos entre los valores -2 y 2, entonces se tendrá evidencia de que los errores tienen media cero, varianza constante y no están correlacionados. Los métodos mostrados hasta ahora, permiten evaluar el modelo de manera global y no por cada observación dentro del mismo. Dado que una observación puede resultar determinante sobre alguna(s) característica(s) del modelo, es conveniente verificar el impacto que cada observación pueda tener en los distintos aspectos del modelo. Las estadísticas para evaluar el impacto que tiene una observación sobre todo el vector de parámetros, alguno de los regresores y sobre los valores predichos, se basan en la misma idea, que consiste en cuantificar el cambio en la característica de interés con y sin la observación que se está evaluando. 6.7.2.3.1 Puntos palanca Antes de presentar las estadísticas que servirán para hacer este diagnóstico, introduciremos un elemento que es común a ellas: la llamada palanca (leverage) de una observación. Recordemos que el ajuste del modelo se expresaba como: \\[\\hat{\\beta}=(X^TX)^{-1}X^Ty \\Rightarrow \\hat{y}=X\\hat{\\beta}=Hy\\] Con \\(H\\) conocida como la matriz sombrero. Un resultado fundamental sobre esta matriz sombrero es: \\[\\mathbb{V}ar(e)=(I-H)\\sigma^2 \\Rightarrow \\mathbb{V}ar(e_i)=(1-h_i)\\sigma^2\\] Con \\(h_i\\) el i-ésimo elemento de la diagonal de la matriz \\(H\\). Observemos que esta palanca sólo depende de \\(X\\), entonces, una observación con una palanca, \\(h_i\\), grande, es aquella con valores extremos en alguna(s) de su(s) covariable(s). Ya que el promedio de las \\(h_i&#39;s\\) es \\(p/n\\), consideraremos una observación con palanca grande si su palanca es mayor a \\(2p/n\\). En este sentido, \\(h_i\\) corresponde a la distancia de Mahalanobis de \\(X\\) definida como \\((X-\\bar{X})^T\\hat{\\Sigma}^{-1}(X-\\bar{X})\\). La dependencia de las estadísticas para el diagnóstico de las observaciones, estriba en que sus cálculos dependen de los valores de la palanca de cada individuo. Estas estadísticas son: Distancia de Cook Dfbetas Dffits Distancia de Cook: Sirve para determinar si una observación es influyente en todo el vector de parámetros. Una observación se considera influyente, si su distancia de Cook sobrepasa el valor uno. ## List of 93 ## $ line :List of 6 ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : num 1 ## ..$ lineend : chr &quot;butt&quot; ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ rect :List of 5 ## ..$ fill : chr &quot;white&quot; ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : num 1 ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_rect&quot; &quot;element&quot; ## $ text :List of 11 ## ..$ family : chr &quot;&quot; ## ..$ face : chr &quot;plain&quot; ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 11 ## ..$ hjust : num 0.5 ## ..$ vjust : num 0.5 ## ..$ angle : num 0 ## ..$ lineheight : num 0.9 ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ title : NULL ## $ aspect.ratio : NULL ## $ axis.title : NULL ## $ axis.title.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 2.75points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 2.75points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x.bottom : NULL ## $ axis.title.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : num 90 ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 2.75points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.y.left : NULL ## $ axis.title.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 2.75points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : chr &quot;grey30&quot; ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 2.2points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 2.2points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x.bottom : NULL ## $ axis.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 1 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 2.2points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.y.left : NULL ## $ axis.text.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 2.2points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.ticks : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ axis.ticks.x : NULL ## $ axis.ticks.x.top : NULL ## $ axis.ticks.x.bottom : NULL ## $ axis.ticks.y : NULL ## $ axis.ticks.y.left : NULL ## $ axis.ticks.y.right : NULL ## $ axis.ticks.length : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ axis.ticks.length.x : NULL ## $ axis.ticks.length.x.top : NULL ## $ axis.ticks.length.x.bottom: NULL ## $ axis.ticks.length.y : NULL ## $ axis.ticks.length.y.left : NULL ## $ axis.ticks.length.y.right : NULL ## $ axis.line : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ axis.line.x : NULL ## $ axis.line.x.top : NULL ## $ axis.line.x.bottom : NULL ## $ axis.line.y : NULL ## $ axis.line.y.left : NULL ## $ axis.line.y.right : NULL ## $ legend.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ legend.spacing : &#39;simpleUnit&#39; num 11points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ legend.spacing.x : NULL ## $ legend.spacing.y : NULL ## $ legend.key : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.key.size : &#39;simpleUnit&#39; num 1.2lines ## ..- attr(*, &quot;unit&quot;)= int 3 ## $ legend.key.height : NULL ## $ legend.key.width : NULL ## $ legend.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.text.align : NULL ## $ legend.title :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.title.align : NULL ## $ legend.position : chr &quot;right&quot; ## $ legend.direction : NULL ## $ legend.justification : chr &quot;center&quot; ## $ legend.box : NULL ## $ legend.box.just : NULL ## $ legend.box.margin : &#39;margin&#39; num [1:4] 0cm 0cm 0cm 0cm ## ..- attr(*, &quot;unit&quot;)= int 1 ## $ legend.box.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.box.spacing : &#39;simpleUnit&#39; num 11points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ panel.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.border : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.spacing : &#39;simpleUnit&#39; num 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ panel.spacing.x : NULL ## $ panel.spacing.y : NULL ## $ panel.grid :List of 6 ## ..$ colour : chr &quot;grey92&quot; ## ..$ size : NULL ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.grid.major : NULL ## $ panel.grid.minor :List of 6 ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.5 ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.grid.major.x : NULL ## $ panel.grid.major.y : NULL ## $ panel.grid.minor.x : NULL ## $ panel.grid.minor.y : NULL ## $ panel.ontop : logi FALSE ## $ plot.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ plot.title :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 1.2 ## ..$ hjust : num 0 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 5.5points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.title.position : chr &quot;panel&quot; ## $ plot.subtitle :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 5.5points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.caption :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : num 1 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 5.5points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.caption.position : chr &quot;panel&quot; ## $ plot.tag :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 1.2 ## ..$ hjust : num 0.5 ## ..$ vjust : num 0.5 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.tag.position : chr &quot;topleft&quot; ## $ plot.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ strip.background.x : NULL ## $ strip.background.y : NULL ## $ strip.placement : chr &quot;inside&quot; ## $ strip.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : chr &quot;grey10&quot; ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 4.4points 4.4points 4.4points 4.4points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.text.x : NULL ## $ strip.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.switch.pad.grid : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.switch.pad.wrap : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.text.y.left :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num 90 ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi TRUE ## - attr(*, &quot;validate&quot;)= logi TRUE Dfbetas: Sirven para determinar si una observación es influyente en alguno de los coeficientes de regresión. Hay un dfbeta por cada parámetro dentro del modelo, incluido, por supuesto, el de la ordenada al origen. La regla de dedo es que la observación \\(i\\) es influyente en el j-ésimo coeficiente de regresión si: \\[|Dfbetas_{j,i}|&gt;\\frac{2}{\\sqrt{n}}\\] Dffits: Se utilizan para determinar si una observación es influyente en la predicción de \\(y\\). Se dice que la i-ésima observación es influyente para predecir \\(y\\), si: \\[|Dffits_i|&gt;2\\sqrt{\\frac{p}{n}}\\] 6.7.2.4 Multicolinealidad El modelo de regresión lineal múltiple, se construye bajo el supuesto de que los regresores son ortogonales, i.e., son independientes. Desafortunadamente, en la mayoría de las aplicaciones el conjunto de regresores no es ortogonal. Algunas veces, esta falta de ortogonalidad no es seria; sin embargo, en algunas otras los regresores están muy cerca de una perfecta relación lineal, en tales casos las inferencias realizadas a través del modelo de regresión lineal pueden ser erróneas. Cuando hay una cercana dependencia lineal entre los regresores, se dice que estamos en presencia de un problema de multicolinealidad. Efectos de la multicolinealidad: Varianzas de los coeficientes estimados son muy grandes. Los estimadores calculados de distintas sub muestras de la misma población, pueden ser muy diferentes. La significancia de algún regresor se puede ver afectada (volverse no significativo) por que su varianza es más grande de lo que debería ser en realidad o por la correlación de la variable con el resto dentro del modelo. Es común que algún signo de un parámetro cambie, haciendo ilógica su interpretación dentro del modelo. 6.7.2.4.1 ¿Cómo detectar multicolinealidad? Matriz de correlación. Examinar las correlaciones entre pares de variables: \\[r_{ij}\\ \\ \\ i, j = 1, 2, \\dots, k\\ \\ i\\neq j\\] Pero, si dos o más regresores están linealmente relacionados, es posible que ninguna de las correlaciones entre cada par de variables, sea grande. Factor de inflación de la varianza. \\[VIF_j=(1-R_j^2)^{-1}\\] Con \\(R_j^2\\) el coeficiente de determinación del modelo de regresión entre el j-ésimo regresor, \\(x_j\\) (tomado como variable de respuesta) y el resto de los regresores \\(x_i\\), \\(i\\neq j\\). Experiencias prácticas indican que si algunos de los VIF’s excede a 10, su coeficiente asociado es pobremente estimado por el modelo debido a multicolinealidad. Análisis del eigensistema. Basado en los eigenvalores de la matriz \\(X^TX\\). Número de condición. \\[K=\\frac{\\lambda_{max}}{\\lambda_{min}}\\] Si el número de condición es menor que 100, no existen problemas serios de multicolinealidad. Si está entre 100 y 1000 existe de moderada a fuerte multicolinealidad y si excede a 1000, hay severa multicolinealidad. Índice de condición. \\[k_j=\\frac{\\lambda{max}}{\\lambda_j}\\] Si el índice de condición es menor que 10, no hay ningún problema. Si está entre 10 y 30, hay moderada multicolinealidad, y si es mayor que 30, existe una fuerte colinealidad en la j-ésima variable en el modelo. N.B. En algunos paquetes estos índices se presentan aplicando la raíz cuadrada a su expresión, entonces hay que extraer raíz a los puntos de corte de los criterios correspondientes. 6.7.2.5 Relación funcional Un supuesto importante en el modelo de regresión es el que considera que debe existir una relación funcional lineal entre cada regresor y la variable de respuestas. Pero, ¿qué debemos hacer si no se cumple esta relación lineal de la respuesta con alguno(s) de los regresor(es)? Primero, ya dijimos que este supuesto se evalúa realizando la gráfica de dispersión entre los residuos del modelo y los valores de la variable en cuestión. Cuando no hay una asociación lineal entre la respuesta y la covariable, generalmente este diagrama de dispersión muestra un patrón (tendencia) que sugiere qué tipo de transformación se debería hacer a la covariable para lograr linealidad con la respuesta. Debe quedar claro que la transformación puede realizarse a la variable explicativa o a la variable de respuesta. A muchos investigadores no les gusta transformar la respuesta porque argumentan que pierden interpretabilidad del modelo. Aunque esto puede ser cierto, existen transformaciones de la respuesta que pueden regresarse para interpretar el modelo con la respuesta original. ## List of 93 ## $ line :List of 6 ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : num 1 ## ..$ lineend : chr &quot;butt&quot; ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ rect :List of 5 ## ..$ fill : chr &quot;white&quot; ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : num 1 ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_rect&quot; &quot;element&quot; ## $ text :List of 11 ## ..$ family : chr &quot;&quot; ## ..$ face : chr &quot;plain&quot; ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 11 ## ..$ hjust : num 0.5 ## ..$ vjust : num 0.5 ## ..$ angle : num 0 ## ..$ lineheight : num 0.9 ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ title : NULL ## $ aspect.ratio : NULL ## $ axis.title : NULL ## $ axis.title.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 2.75points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 2.75points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x.bottom : NULL ## $ axis.title.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : num 90 ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 2.75points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.y.left : NULL ## $ axis.title.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 2.75points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : chr &quot;grey30&quot; ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 2.2points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 2.2points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x.bottom : NULL ## $ axis.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 1 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 2.2points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.y.left : NULL ## $ axis.text.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 2.2points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.ticks : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ axis.ticks.x : NULL ## $ axis.ticks.x.top : NULL ## $ axis.ticks.x.bottom : NULL ## $ axis.ticks.y : NULL ## $ axis.ticks.y.left : NULL ## $ axis.ticks.y.right : NULL ## $ axis.ticks.length : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ axis.ticks.length.x : NULL ## $ axis.ticks.length.x.top : NULL ## $ axis.ticks.length.x.bottom: NULL ## $ axis.ticks.length.y : NULL ## $ axis.ticks.length.y.left : NULL ## $ axis.ticks.length.y.right : NULL ## $ axis.line : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ axis.line.x : NULL ## $ axis.line.x.top : NULL ## $ axis.line.x.bottom : NULL ## $ axis.line.y : NULL ## $ axis.line.y.left : NULL ## $ axis.line.y.right : NULL ## $ legend.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ legend.spacing : &#39;simpleUnit&#39; num 11points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ legend.spacing.x : NULL ## $ legend.spacing.y : NULL ## $ legend.key : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.key.size : &#39;simpleUnit&#39; num 1.2lines ## ..- attr(*, &quot;unit&quot;)= int 3 ## $ legend.key.height : NULL ## $ legend.key.width : NULL ## $ legend.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.text.align : NULL ## $ legend.title :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.title.align : NULL ## $ legend.position : chr &quot;right&quot; ## $ legend.direction : NULL ## $ legend.justification : chr &quot;center&quot; ## $ legend.box : NULL ## $ legend.box.just : NULL ## $ legend.box.margin : &#39;margin&#39; num [1:4] 0cm 0cm 0cm 0cm ## ..- attr(*, &quot;unit&quot;)= int 1 ## $ legend.box.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.box.spacing : &#39;simpleUnit&#39; num 11points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ panel.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.border : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.spacing : &#39;simpleUnit&#39; num 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ panel.spacing.x : NULL ## $ panel.spacing.y : NULL ## $ panel.grid :List of 6 ## ..$ colour : chr &quot;grey92&quot; ## ..$ size : NULL ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.grid.major : NULL ## $ panel.grid.minor :List of 6 ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.5 ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.grid.major.x : NULL ## $ panel.grid.major.y : NULL ## $ panel.grid.minor.x : NULL ## $ panel.grid.minor.y : NULL ## $ panel.ontop : logi FALSE ## $ plot.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ plot.title :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 1.2 ## ..$ hjust : num 0 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 5.5points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.title.position : chr &quot;panel&quot; ## $ plot.subtitle :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 5.5points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.caption :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : num 1 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 5.5points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.caption.position : chr &quot;panel&quot; ## $ plot.tag :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 1.2 ## ..$ hjust : num 0.5 ## ..$ vjust : num 0.5 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.tag.position : chr &quot;topleft&quot; ## $ plot.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ strip.background.x : NULL ## $ strip.background.y : NULL ## $ strip.placement : chr &quot;inside&quot; ## $ strip.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : chr &quot;grey10&quot; ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 4.4points 4.4points 4.4points 4.4points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.text.x : NULL ## $ strip.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.switch.pad.grid : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.switch.pad.wrap : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.text.y.left :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num 90 ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi TRUE ## - attr(*, &quot;validate&quot;)= logi TRUE Un problema asociado a esta identificación por parte del usuario, es que debe tener experiencia para asociar estas formas a una función analítica específica; hecho no necesariamente cierto. Por lo tanto, requiere de alguna herramienta técnica que pudiera auxiliarlo en esta labor. Un buen auxiliar, en el caso de que se crea que es necesario transformar la respuesta, es usar la llamada trasformación Box-Cox. 6.7.2.5.1 Transformación Box-Cox La transformación Box-Cox de la respuesta, es una función que sirve para normalizar la distribución del error, estabilizar la varianza de este error y mejorar la relación lineal entre \\(y\\) y las \\(X’s\\). Se define como: \\[y_i^{\\lambda} = \\left\\{ \\begin{array}{ll} \\frac{y_i^{\\lambda-1}}{\\lambda}, &amp; \\lambda \\neq 0;\\\\ ln(y_i), &amp; \\lambda=0 .\\end{array} \\right.\\] La siguiente tabla muestra el rango de valores de \\(\\lambda\\) que estarían asociados a una transformación analítica común. Rango \\(\\lambda\\) Transformación Asociada (-2.5, -1.5] \\(\\frac{1}{y^2}\\) (-1.5, -0.75] \\(\\frac{1}{y}\\) (-0.75, -0.25] \\(\\frac{1}{sqrt{y}}\\) (-0.25, 0.25] \\(ln(y)\\) (0.25, 0.75] \\(\\sqrt{y}\\) (0.75, 1.25] \\(y\\) (1.25, 2.5) \\(y^2\\) 6.7.2.5.2 Transformación Box-Tidwell Box y Tidwell implementan un proceso iterativo para encontrar la mejor transformación de las variables predictoras en el modelo de regresión lineal. Definiendo como \\(X_j^{\\gamma_j}\\) la correspondiente transformación Box-Tidwell de la variable \\(j\\). La tabla anterior para las transfomaciones analíticas de la respuesta, también aplican para estas transformaciones de los predictores. 6.8 Aplicación en R … Work in progres … "],
["modelos-lineales-generalizados.html", "Capítulo 7 Modelos lineales generalizados 7.1 Regresión logística 7.2 Modelo de regresión logísitica simple 7.3 Modelo de regresión logísitica multiple 7.4 Regresión Multinomial 7.5 Modelos para conteos 7.6 GLM binomial negativa 7.7 Exponencial 7.8 Gamma", " Capítulo 7 Modelos lineales generalizados En el capítulo anterior exploramos el modelo básico que nos permite responder a la pregunta: ¿puede ser la variable de interés predicha por un conjunto de variables explicativas? Sin embargo, para poder utilizar dicho modelo, es necesario que la variable respuesta sea continua y cumpla las hipótesis estándar del modelo lineal (datos normales, varianza constante, etc.) Si la variable de interés es, por ejemplo binaria podemos ajustar un modelo de regresión logística en donde lo que predecimos son las probabilidades de la ocurrencia del evento medido con la variable binaria. En 1944, Berkson utilizó por primera vez la regresión logística como una forma de solucionar el problema de explicar una variable dicotómica a través de una variable continua. En este caso, la función logit hace que en lugar de trabajar con valores de la variable respuesta entre \\((0, 1)\\), trabajemos con una variable respuesta que puede tomar cualquier valor. No fue hasta 1972 cuando John Nelder introdujo los modelos lineales generalizados (GLM por sus siglas en inglés), de ahí que en general se considere a la regresión logística como algo distinto a los GLM, cuando lo que ocurre es que tanto la regresión múltiple como la logística, de Poisson, ordinal, etcétera, son casos particulares de un GLM. Para entender lo que es un GLM, volvamos al modelo de regresión múltiple, en este modelos suponemos que: \\[Y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}+\\epsilon\\] \\[E[Y]=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] Es decir, que existe una relación lineal entre las \\(X\\) y \\(E[Y]\\) (el valor medio de Y dado un cierto valor de las variables explicativas). Si las observaciones son binarias, entonces: \\[P(Y = 1) = p\\] \\[P(Y = 0) = 1-p\\] Y \\(E[Y] = 0\\times P(Y = 0) + 1 \\times P(Y = 1) = p\\), por lo tanto un modelo de regresión múltiple relacionará directamente la probabilidad de que ocurra un suceso con las variables explicativas, lo cual no es lo que se busca al ajustar un modelo de regresión lineal. Lo que hacen los GLM es establecer esa relación lineal no entre la media de la variable respuesta y los predictores, sino entre una función de la media de variable respuesta y los predictores, es decir: \\[g(E[Y])=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] Según de qué tipo sea la varible \\(Y\\), así será la función \\(g(\\cdot)\\). Entonces se puede decir que un GLM tiene 3 componentes: Componente aleatorio: La variable respuesta \\(Y\\) . Para poder utilizar un GLM, la distribución de \\(Y\\) ha de pertenecer a la familia exponencial, es decir, su función de densidad ha de poder escribirse como: \\[f(y;\\theta,\\phi)=exp\\{\\frac{y\\theta-b(\\theta)}{a(\\phi)}+c(y,\\phi)\\}\\] donde \\(a(\\cdot)\\), \\(b(\\cdot)\\) y \\(c(\\cdot)\\) son funciones específicas. El parámetro \\(\\theta\\) es lo que se llama parámetro canónico de localización y \\(\\phi\\) es un parámetro de dispersión. Pertenecen a la familia exponecial la distribución Normal, Bernouilli, Binomial, Poisson, Exponecial, Gamma, entre otras. Componente sistemático: Las variables predictoras \\(X_i \\ \\ i = 1,...,k\\) Función liga: La función que relaciona la media, \\(E[Y]\\), con las variables predictoras \\(X\\). En el caso del modelo de regresión ordinaria, \\(\\mu = \\nu\\), por lo tanto la función liga es la identidad. Hay muchas opciones par la función liga. La función liga canonica es una función que transforma la media en el parámetro canónico \\(\\theta\\) \\[g(E[Y])=\\theta\\] Entoces \\(g(\\cdot)\\) es la función liga canónica. La siguiente tabla muestra las funciones link canónicas para las distribuciones más comunes usadas en los GLMs: Distribución Liga canónica Normal \\(X \\beta = E[Y]\\) (identidad) Binomial \\(X \\beta = ln(\\frac{P}{1-P})\\) (logística) Poisson \\(X \\beta = ln(E[Y])\\) (logarítmica) Exponencial \\(X \\beta = \\frac{1}{E[Y]})\\) (recíproca) Gamma \\(X \\beta = \\frac{1}{E[Y]})\\) (recíproca) La diferencia que hay entre usar la función liga y usar una transformación, es que la función liga transforma la media, \\(E[Y]\\), y no los datos, \\(Y\\). Los GLM generalizan la regresión ordinaria de dos modos: permitiendo que la variable de respuesta \\(Y\\) tenga distribuciones diferentes a la normal y, por otro lado, incluyendo distintas funciones liga de la media, lo cual resulta muy útil para datos categóricos. 7.1 Regresión logística El modelo de regresión logistica es un GLM donde la distribución de probabilidad es Bernoulli o Binomial, y la función liga es el logit (ya que relaciona a la media, que en una Bernouilli es la probabilidad con el predictor lineal). Por lo tanto la estimación de los parámetros y los contrastes de hipótesis utilizan la teoría desarrollada para los GLMs. Estos modelos se utilizan cuando se desea conocer la relación entre Una variable dependiente cualitativa, dicotómica. Una o más variables explicativas independientes, llamadas covariables ya sean cualitativas o cuantitativas Por tanto, el objetivo de la regresión logística no es, como en regresión lineal, predecir el valor de la variable \\(Y\\) a partir de una o varias variables predictoras, sino que queremos predecir la probabilidad de que ocurra \\(Y\\) conocidos los valores de las variables \\(X_i&#39;s\\). Recordemos que las covariables cualitativas deben transformarse en las covariables cualitativas dicotómicas ficticias necesarias (variables dummy). De manera que al hacer esta transformación cada categoría de la variable entrará en el modelo de forma individual. 7.2 Modelo de regresión logísitica simple Para este modelo supondremos que nuestra respuesta, \\(Y\\), es explicada únicamente por una covariable, \\(X\\). Asumimos que la variable independiente \\(Y\\) está codificada como un 0 o un 1. Entonces, escribimos nuestro modelo como: \\[ln(\\frac{p}{1-p})=\\beta_{0}+\\beta_{1}X\\] \\[\\frac{p}{1-p}=e^{\\beta_{0}+\\beta_{1}X}\\] \\[p=e^{\\beta_{0}+\\beta_{1}X}-p\\times e^{\\beta_{0}+\\beta_{1}X}\\] \\[p(1+e^{\\beta_{0}+\\beta_{1}X})=e^{\\beta_{0}+\\beta_{1}X}\\] \\[p=\\frac{e^{\\beta_{0}+\\beta_{1}X}}{1+e^{\\beta_{0}+\\beta_{1}X}}\\] Y: \\[1-p=\\frac{1}{1+e^{\\beta_{0}+\\beta_{1}X}}\\] Los valores posibles de estas ecuaciones varían entre 0 y 1. Un valor cercano a 0 significa que es muy improbable que \\(Y\\) haya ocurrido, y un valor cercano a 1 significa que es muy probable que tuviese lugar. Similar a regresión lineal los valores de los parámetros se estiman utilizando el método de máxima verosimilitud que selecciona los coeficientes que hacen más probable que los valores observados ocurran. Para este análisis tenemos la razón de momios (odds ratio), que corresponde a la razón entre las posibilidades de respuesta. \\[OR=\\frac{\\frac{P(Y=1|X=1)}{1-P(Y=1|X=1)}}{\\frac{P(Y=1|X=0)}{1-P(Y=1|X=0)}}\\] El valor nulo para la razón de momios es el 1. Un \\(OR = 1\\) implica que las dos categorías comparadas son iguales. El valor mínimo posible es 0 y el máximo teóricamente posible es infinito. Un OR inferior a la unidad se interpreta como que el desenlace es menos frecuente en la categoría o grupo que se ha elegido como de interés con respecto al otro grupo o categoría de referencia. Un OR = 3 se interpreta como una ventaja 3 veces superior de una de las categorías \\(X = 1\\) relativamente a la otra categoría \\(X=0\\). 7.3 Modelo de regresión logísitica multiple Análogo a lo que observamos en los modelos de regresión lineal, el modelo de regresión logística se puede facilmente generalizar de un modelo simple a un múltiple. \\[ln(\\frac{p}{1-p})=\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_{2}+...\\beta_{k}X_{k}\\] \\[p=P(Y)=\\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_{2}+...\\beta_{k}X_{k})}}\\] De nuevo los valores posibles de estas ecuaciones varían entre 0 y 1. El propósito del análisis es predecir la probabilidad de que un evento \\(Y\\) ocurra para el \\(i-eismo\\) individuo. Para dicha \\(i-ésima\\) persona, \\(Y\\) será 0 (la respuesta no ocurre) o 1 (la respuesta ocurre), y el valor predicho, \\(\\mathbb{P}(Y)\\), tendrá un valor 0 (no hay probabilidad de que el resultado ocurra) o 1 (el resultado seguro que ocurre). 7.3.1 Regresión lineal en R …WIP.. 7.4 Regresión Multinomial Hasta ahora hemos revisado el caso en el que la variable respuesta era dicotómica. Ahora nos centramos en el caso en el que la variable de interés tiene más de dos categorías, por ejemplo, afiliación política; resultado de un partido de fútbol; marcas de teléfonos celulares, etc. Por simplicidad, se ilustrará la metodología para el caso de tres categorías, ya que la generalización a más de tres es inmediata. Supongamos que codificamos las tres categorías de la variable respuesta como 0, 1 y 2. En el caso de regresión logística, el logit es: \\[ln(\\frac{p}{1-p})=ln(\\frac{P[Y=1]}{P[Y=0]})\\] Ahora el modelo necesita dos funciones logit ya que tenemos tres categorías, y necesitamos decidir que categorías queremos comparar. Lo más general es utilizar \\(Y = 0\\) como referencia y formar logits comparándola con \\(Y = 1\\) y \\(Y = 2\\). Supongamos que tenemos k variables explicativas, entonces: \\[ln(\\frac{P[Y=1]}{P[Y=0]})=\\beta_{10}+\\beta_{11}X_1+\\beta_{12}X_{2}+...\\beta_{1k}X_{k}\\] \\[ln(\\frac{P[Y=2]}{P[Y=0]})=\\beta_{20}+\\beta_{21}X_1+\\beta_{22}X_{2}+...\\beta_{2k}X_{k}\\] Y ahora tenemos el doble de coeficientes que en el caso de regresión logística. Las probabilidades se calcularán como: \\[P[Y=0|X]=\\frac{1}{1+e^{g_1(X)}+e^{g_2(X)}}\\] \\[P[Y=1|X]=\\frac{e^{g_1(X)}}{1+e^{g_1(X)}+e^{g_2(X)}}\\] \\[P[Y=2|X]=\\frac{e^{g_2(X)}}{1+e^{g_1(X)}+e^{g_2(X)}}\\] \\[g_1(X)=\\beta_{10}+\\beta_{11}X_1+\\beta_{12}X_{2}+...\\beta_{1k}X_{k}\\] \\[g_2(X)=\\beta_{20}+\\beta_{21}X_1+\\beta_{22}X_{2}+...\\beta_{2k}X_{k}\\] 7.4.1 Regresión Multinomial en R …WIP.. 7.5 Modelos para conteos En muchos casos las variables respuesta son conteos, y en ocasiones estos recuentos aparecen al resumir en tablas de contingencia otras variables. Hay cuatro razones por las que sería erroneo utlizar un modelo de regresión normal para datos de conteo : Puede dar lugar a predicciones negativas. La varianza de la variable respuesta no es independiente de la media. Los errores no siguen una distribución Normal. Los ceros que aparecen en la variable respuesta dan problemas a la hora de transformar la variables. Sin embargo, si la variable es de conteo pero los datos toman valores elevados, entonces si podría ser posible utilizar la distribución Normal. El modelo más simple para cuando la variable de respuesta son recuentos es asumir que el componente aleatorio \\(Y\\) sigue una distribución de Poisson. Esta distribución es unimodal y su propiedad más destacada es que la media y la varianza coinciden. \\[E(Y)=Var(Y)=\\mu\\] De modo que cuando el número de recuentos es mayor en media, también tienden a tener mayor variabilidad. La principal diferencia entre la distribución de Poisson y la Binomial, es que, aunque ambas cuentan el número de veces que ocurre algo, en la distribución de Poisson no sabemos cuántas veces no ocurrio, y en la Binomial sí lo sabemos. Supongamos que estamos haciendo un estudio sobre cuantas larvas de insectos hay en ciertos árboles, los datos de los que disponemos corresponden al número de larvas por hoja \\((Y)\\). Habrá hojas que no tengan ninguna, y otras que tenga hasta 5 ó 6. Si el número medio de larvas por hoja es \\(\\mu\\), la probabilidad de observar \\(y_0\\) larvas por hoja viene dada por la siguiente ecuación: \\[P(y_0)=\\frac{e^{\\mu}\\mu^{y_0}}{y_0!}\\] Donde \\(\\mu\\) se puede aproximar con \\(\\mu=np\\), para \\(n\\) grande y \\(p\\) pequeño. Es decir, que una distribución de Poisson se obtiene a partir de una Binomial con \\(p\\) pequeño y \\(n\\). Entonces el modelo GLM para los conteos se basará en modelar la relación entre la media muestral \\(\\mu\\) y las variables explicativas. \\[\\mu=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] Por las características de la variable (son conteos) buscamos que la parte derecha de la ecuación sólo tome valores positivos. Por esta razón habitualmente se usa el logaritmo de la media como la función liga, de modo que el modelo log-lineal se puede expresar como: \\[log(\\mu) = log(E[Y])=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] de modo que al despejar \\(\\mu\\) obtenemos: \\[\\mu = e^{\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}}\\] 7.5.1 Sobredispersión en GLM Poisson En una distribución de Poisson, la media y la varianza son iguales, pero en la práctica, los datos de conteo muestran mayor variabilidad de la que se espera en un modelo binomial o Poisson. En el caso de este último, es común que la varianza sea mucho mayor que la media \\(\\mathbb{V}\\left(Y\\right)&gt;&gt;\\mathbb{E}\\left(Y\\right)=\\mu\\), este fenómeno se conoce como sobredispersión. Por ejemplo, podemos suponer que cada individuo tienen igual probabilidad de padecer cierta enfermedad; no obstante, siendo más realistas, es claro que que estas probabilidades varían debido a factores genéticos, de salubridad y de localización geográfica, entre otros, propiciando mayor variabilidad sobre el número de sujetos enfermos en un periodo determinado, que los que puede predecir el modelo Poisson asociado. Una forma de medir la sobredispersión en los datos es ajustando una distribución quasipoisson, la cual ajustará el modelo con distribución Poisson pero no asumirá varianza igual a la media y calculará el parámetro de dispersión. 7.5.2 GLM Poisson R …WIP… 7.6 GLM binomial negativa Una distribución que puede usarse como alternativa a una Poisson es la . Dado que su varianza es más grande que su media, constituye una excelente alternativa para modelar datos de conteo sobredispersos, que son muy comunes en aplicaciones reales. Si una variable aleatoria \\(Y\\) se distribuye como una binomial negativa, entonces la función de probabilidad es: \\[P(y|k,\\mu)=\\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}\\left( \\frac{k}{\\mu+k} \\right)^k\\left( 1-\\frac{k}{\\mu+k} \\right)^y\\] con \\(y=0,1,2,...\\) donde \\(k\\) y \\(\\mu\\) son los parámetros de la distribución y se tiene que \\[E(Y)=\\mu\\] \\[Var(Y)=\\mu+\\frac{\\mu^2}{k}\\] El parámetro \\(\\frac{1}{k}\\) es un parámetro de dispersión, de modo que si \\(\\frac{1}{k} \\rightarrow 0\\) entonces \\(Var(Y)\\rightarrow \\mu\\) y la distribución binomial negativa converge a una distribución Poisson. Por otro lado, para un valor fijo de \\(k\\) esta distribución pertenece a la familia exponencial natural, de modo que se puede definir un modelo GLM binomial negativo. En general, se usa una función liga de tipo logaritmo. La regresión binomial negativa se puede utilizar para datos sobredispersos de recuentos, es decir cuando la varianza condicional es mayor que la media condicional. Se puede considerar como una generalización de la regresión de Poisson, ya que tiene su misma estructura de medias y además un parámetro adicional para el modelo de sobredispersión. Si la distribución condicional de la variable observada es más dispersa, los intervalos de confianza para la regresión binomial negativa es probable que sean más estrechos que los correspondientes a un modelo de regresión de Poisson. 7.6.1 GLM Binomial Negativa en R …WIP… 7.7 Exponencial Se dice que la variable respuesta \\(Y\\) es de tipo Exponencial cuando hemos observado el tiempo transcurrido hasta que ocurre un evento de interés como resultado de un conjunto de variables predictoras que pueden ser de tipo numérico o categórico. En este caso similar al caso de conteos también se asume que \\(Y\\) sólo toma valores positivos y es continua. En este caso la función liga utilizada es el recíproco, de modo que el modelo lineal se puede expresar como: \\[\\frac{1}{E[Y]}=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] 7.7.1 GLM Exponencial en R …WIP… 7.8 Gamma Como se mencionó en la sección anterior las distribución exponencial es un caso particular de la distribución gamma. En general cuando la variable de respuesta es de tipo numérico, pero sólo puede tomar valores positivos de forma asimétrica, es decir, se encuentra concentrada en un conjunto de valores y su frecuencia disminuye cuando aumenta el valor de la respuesta, se dice que la variable se distribuye gamma. La distribución gamma sólo está definida para valores mayores a cero, por lo que si la variable de respuesta \\(Y\\) toma valores negativos o cero, para poder utilizar esta distribución será necesario realizar una transformación de los datos, sumando una constante lo suficientemente grande que haga todas las obervaciones positivas. Análogo al caso exponencial, la función liga utilizada es el recíproco, de modo que el modelo lineal se expresa como: \\[\\frac{1}{E[Y]}=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] 7.8.1 GLM Gamma en R …WIP… "],
["modelos-lineales-generalizados-construcción-y-evaluación.html", "Capítulo 8 Modelos Lineales Generalizados (Construcción y Evaluación) 8.1 Exploración de los datos. 8.2 Elección de la estructura de errores y función liga. 8.3 Bondad de ajuste. 8.4 Simplificación del modelo. 8.5 Criterios de evaluación de modelos. 8.6 Análisis de los residuos. 8.7 Evaluación de GLMs en R", " Capítulo 8 Modelos Lineales Generalizados (Construcción y Evaluación) En la construcción de modelos lineales generalizados es importante tener en cuenta que NO existe un único modelo que sea válido. En la mayoría de los casos, habrá un número variable de modelos plausibles que puedan ajustarse a un conjunto determinado de datos. Parte del trabajo de construcción y evaluación del modelo es determinar cuál de todos estos modelos son adecuados, y entre todos los modelos adecuados, cuál es el que explica la mayor proporción de la varianza sujeto a la restricción de que todos los parámetros del modelo deberán ser estadísticamente significativos. En algunos casos habrá más de un modelo que ajuste igual de bien a los datos y en esos casos queda a criterio del modelador elegir uno u otro. Los pasos que hay que seguir en la construcción y evaluación de un GLM son muy similares a los de cualquier modelo estadístico. 8.1 Exploración de los datos. Siempre es conveniente conocer los datos con los que se esta trabajando. Puede resultar interesante obtener gráficos que nos muestren la relación entre la variable de respuesta y cada una de las variables explicativas, gráficos de caja (box-plot) para variables categóricas, o matrices de correlación entre las variables explicativas. El objetivo de este análisis exploratorio es: Buscar posibles relaciones de la variable respuesta/dependiente con la(s) variable(s) explicativa(s). Considerar la necesidad de aplicar transformaciones de las variables. Eliminar variables explicativas que estén altamente correlacionadas. 8.2 Elección de la estructura de errores y función liga. A veces resultará fácil elegir estas propiedades del modelo basandose en las características de la variable de respuesta. Pero en otras ocasiones resultará tremendamente difícil, y será a posteriori cuando comprobemos, analizando los residuos, la idoneidad de la distribución de errores elegida. Por otro lado, puede ser una práctica recomendable el comparar modelos con distintas funciones liga para ver cuál se ajusta mejor a nuestros datos. 8.3 Bondad de ajuste. Los tests de significación para los estimadores del modelo. (p-values de los estimadores) La cantidad de varianza explicada por el modelo. Esto en GLM se conoce como devianza. La devianza nos da una idea de la variabilidad del los datos. Por ello, para obtener una medida de la variabilidad explicada por el modelo, hemos de comparar la devianza del modelo nulo (Null deviance) con la devianza residual (Residual deviance), esto es, una medida de cuánto de la variabilidad de la variable respuesta no es explicado por el modelo. (Prueba de la Chi-cuadrada sobre la devianza) 8.4 Simplificación del modelo. El principio de parsimonia requiere que el modelo sea tan simple como sea posible. Esto significa que no debe contener parámetros o niveles de un factor que sean redundantes. La simplificación del modelo implica por tanto: La eliminación de las variables explicativas que no sean significativas. La agrupación de los niveles de factores (variables categóricas) que no difieran entre sí. La simplificación del modelo tiene que tener, además, una cierta lógica para el analista y no debe incrementar de manera significativa la devianza residual. 8.5 Criterios de evaluación de modelos. Podemos utilizar la reducción de la devianza como una medida del ajuste del modelo a los datos. Los tests de significación para los parámetros del modelo son también útiles para ayudarnos a simplificar el modelo. Un criterio comúnmente utilizado es el llamado Criterio de Información de Akaike (AIC del inglés Akaike Information Criterion). Este índice evalúa tanto el ajuste del modelo a los datos como la complejidad del modelo. Cuanto más pequeño es el AIC mejor es el ajuste. El AIC es muy útil para comparar modelos similares con distintos grados de complejidad o modelos iguales (mismas variables) pero con funciones liga distintas. 8.6 Análisis de los residuos. Los residuos son las diferencias entre los valores estimados por el modelo y los valores observados. Sin embargo, muchas veces se utilizan los residuos estandarizados, que tienen que seguir una distribución normal. Conviene analizar los siguientes gráficos: Histograma de los residuos. Gráfico de residuos frente a valores estimados. Estos gráficos pueden indicar falta de linealidad, heterocedasticidad (varianza no constante) y valores atípicos. Gráficos de valores atípicos. Existen tests que permiten detectar valores atípicos. Los índices más comunes son el índice de Cook y el de apalancamiento o leverage. Estos gráficos ayudan la evaluación del modelo utilizado. En caso necesario, sería preciso volver a plantear el modelo, tal vez utilizando una estructura de errores más adecuada, otra función liga o incluso eliminando ciertos datos que pueden estar desviando el análisis. 8.7 Evaluación de GLMs en R …WIP… "],
["qué-es-una-red-neuronal.html", "Capítulo 9 ¿Qué es una red neuronal? 9.1 Ejemplo", " Capítulo 9 ¿Qué es una red neuronal? Las redes neuronales artificiales están inspiradas en la forma en la que las neuronas de nuestro cerebro trabajan en conjunto para resolver una tarea compleja. Éstas modelan una variable respuesta, ya sea de tipo continua o categórica, como función de las covariables a través de la composición de funciones no lineales. De manera general, una red nueronal consiste de una arquitectura, una regla de activación y una regla de salida. Arquitectura: Puede ser descrita vía un gráfo dirigido cuyo nodos son llamados neuronas. Existen una grán cantidad de arquitecturas dependiendo de la naturaleza del grafo i.e. de las relaciones entre los nodos. Una descripción bastante completa de las distintas arquitecturas y sus nombres puede ser consultada Regla de activación: Típicamente el valor en cada nodo \\(v\\) puede ser calculado como \\[x_v = f(\\sum_{u \\rightarrow v}\\beta_{uv}x_u)\\] donde la suma se obtiene sobre los nodos predecesores de \\(v\\) y \\(\\beta_{uv}\\) son los coeficientes (desconocidos) de la red. A la función \\(f(\\cdot)\\) se le conoce como función de activación. Las más comúnmente usadas son la sigmoide logística \\(f(x) = \\frac{e^y}{1+e^y}\\) y la función rectificadora \\(f(x) = \\max\\{{y,0}\\}\\) 9.1 Ejemplo Dada la siguiente arquitectura calcularemos el valor de salida dado por el nodo \\(x_6\\). \\[\\begin{align*} x_{6} &amp; =f\\left(\\beta_{3,6}\\cdot x_{3}+\\beta_{4,6}\\cdot x_{4}+\\beta_{5,6}\\cdot x_{5}\\right)\\\\ &amp; \\hookrightarrow x_{3}=f\\left(\\beta_{1,3}\\cdot x_{1}+\\beta_{2,3}\\cdot x_{2}\\right)\\\\ &amp; \\hookrightarrow x_{4}=f\\left(\\beta_{1,4}\\cdot x_{1}+\\beta_{2,4}\\cdot x_{2}\\right)\\\\ &amp; \\hookrightarrow x_{5}=f\\left(\\beta_{1,5}\\cdot x_{1}+\\beta_{2,5}\\cdot x_{2}\\right)\\\\ &amp; \\vdots \\end{align*}\\] donde \\(f(\\cdot)\\) es la función de activación. Si suponemos que \\(f(x) = \\mathbb{I}(x)\\) entonces el problema a resolver será el de regresión lineal. Nodos de sesgo: Dependiendo de la utilidad, podríamos estar interesados, como en el problema de regresión lineal, en un término de intercepto o de nivel base, para ello puede simplemente crearse un nodo extra cuyo valor sea la constante 1. Regla de salida: En el nodo de salida \\(v\\) se calcula: \\[s_v = \\sum_{u\\rightarrow v}\\beta_{uv}x_u\\] y en lugar de aplicar la función de activación \\(f(\\cdot)\\), se usa una regla de salida para obtener el valor o vector de ajuste/predicción. Por ejemplo, en un problema de clasificación de \\(N\\) clases, la capa de salida consitirá de \\(N\\) nodos, cada uno asociado a cada clase; la predicción/ajuste \\(\\hat{y}\\in \\mathbb{R}^p\\) está dada por \\[\\hat{y}=\\arg max\\{s_{vn}\\}\\] Notemos que para este ejemplo, si la arquitectura de la red carece de capas intermedias/ocultas y con función de activación sigmoide logística entonces el modelo corresponde al de regresión logística múltiple. Al igual que en los modelos logísticos, podemos hacer que la red neuronal tenga como salida un vector de probabilidades \\(z=(z_1,\\dots,z_N)\\) dado por \\[z_n=\\frac{e^{s_{v_n}}}{e^{s_{v_1}}+\\dots+e^{s_{v_N}}}\\] Al proceso de normalización de los valores exponenciados se le conoce como la operación softmax. De manera particular una red neuronal cuya arquitectura está representada por un grafo dirigido no cíclico, (feedforward) con función de activación sigmoide logística y regla de salida softmax es un modelo paramétrico descrito por: \\[y \\sim Multinomial(1, (p_1,\\dots,p_N)\\] \\[(p_1,\\dots,p_N)=g^{nnet}(x_v; \\{\\beta_{uv}:u\\rightarrow v\\})\\] "],
["teorema-de-universalidad.html", "Capítulo 10 Teorema de Universalidad", " Capítulo 10 Teorema de Universalidad La flexibilidad y el poder de las redes neuronales radica en la estructura de composición de funciones de activación no lineales. El siguiente teorema implica que una red neuronal puede aproximar cualquier función en particular nos es relevante el poder aproximar cualquier modelo no paramétrico de regresión. Teorema. Sea \\(\\Delta^{L}=\\left\\{ \\left(y_{1},\\ldots,y_{L}\\right)\\in\\mathbb{R}_{\\geq0}^{L}\\,|\\,y_{1}+\\dots+y_{L}=1\\right\\} \\in \\mathbb{R}^{L}\\), y \\(B\\subseteq\\mathbb{R}^{p}\\) un conjunto acotado que contiene al origen y \\(\\mu\\) una medida de probabilidad en \\(B\\). Para \\(g:\\,B\\rightarrow\\Delta\\) una función medible arbitraria existe una red neuronal totalmente conectada (feedforward), de una capa oculta que tiene \\(m\\) neuronas, activación sigmoide y regla output softmax tal que su correspondiente función de probabilidades \\(g^{NNET}\\) satisface: \\[\\int_{B}\\left\\lVert g-g^{NNET}\\right\\lVert _{2}^{2}d\\mu\\leq\\frac{Clog^{2}\\left(n\\right)}{n}\\] para alguna constante universal \\(C\\). "],
["entrenamiento-de-una-red-neuronal.html", "Capítulo 11 Entrenamiento de una red neuronal 11.1 Back-propagation 11.2 Saturación 11.3 Regularización 11.4 Redes Neuronales en R", " Capítulo 11 Entrenamiento de una red neuronal En el ámbito del machine learning se le conoce como entrenamiento al proceso de encontrar buenos estimadores de los parámetros del mmodelo usado, en este caso, de la red neuronal. Sea \\(\\beta = (\\beta_{uv}; u\\rightarrow v)\\) el vector de parámetros de una red neuronal. Dados los datos de entrenamiento \\((x^1,y^1),\\dots,(x^n,y^n)\\), la log-verosimilitud de \\(\\beta\\) está dada por: \\[{\\it l}\\left(\\beta\\right)=\\sum_{i=1}^{n}\\sum_{{\\it l}=1}^{L}\\mathbb{I}_{\\left\\{ y_{i}={\\it l}\\right\\} }log\\left(\\frac{e^{s_{v_{{\\it l}}}\\left(x_{i},\\beta\\right)}}{e^{s_{v_{1}}\\left(x_{1},\\beta\\right)}+\\dots+e^{s_{v_{L}}\\left(x_{L},\\beta\\right)}}\\right) =\\colon \\sum_{i=1}^{n}{\\it l}_{i}\\left(\\beta\\right)\\] A continuación mostraremos cómo podemos maximizar la log-verosimilitud para obtener los parámetros. 11.1 Back-propagation Gracias a la estructura de composición de funciones del modelo, la derivada puede encontrarse fácilmente usando la regla de la cadena. 11.1.1 Ejemplo (back-propagation). Consideremos la siguiente red neuronal: Dado, el ejemplo de entrenamiento \\(x^i=(x_1^i,x_2^i)\\) con etiqueta/valor \\(y^i\\) la red neuronal calcula la probabilidad de salida \\(z_1=\\mathbb{P}[Y=1|X = x^i]\\) de la siguiente forma: \\[\\begin{align*} s_{3}=\\beta_{1,3}x_{1}+\\beta_{2,3}x_{2} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,x_{3}\\leftarrow f\\left(s_{3}\\right)\\\\ s_{4}=\\beta_{1,4}x_{1}+\\beta_{2,4}x_{2} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,x_{4}\\leftarrow f\\left(s_{4}\\right)\\\\ s_{5}=\\beta_{1,5}x_{1}+\\beta_{2,5}x_{2} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,x_{5}\\leftarrow f\\left(s_{5}\\right)\\\\ s_{6}=\\beta_{3,6}x_{3}+\\beta_{4,6}x_{4}+\\beta_{5,6}x_{5} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,z_{1}\\leftarrow f\\left(s_{6}\\right) \\end{align*}\\] Donde, por facilidad estamos considerando que \\(f(x)=\\frac{e^x}{1+e^x}\\). El valor de la log-verosimilitud para este ejemplo de entrenamiento se calcula como: \\[{\\it l}_{i}\\left(\\beta;\\,\\left(x^{i},\\,y^{i}\\right)\\right)=\\mathbb{I}_{\\left\\{ y^{i}=1\\right\\} }log\\left(z_{1}\\right)+\\mathbb{I}_{\\left\\{ y^{i}=2\\right\\} }log\\left(1-z_{1}\\right)\\] Y el gradiente estocástico como: \\(\\frac{\\partial l}{\\partial z_1} = \\frac{\\mathbb{I}_{\\{y^1=1\\}}}{z_1}-\\frac{\\mathbb{I}_{\\{y^1=2\\}}}{1-z_1}\\) \\(\\frac{\\partial l}{\\partial s_6} = \\frac{\\partial l}{\\partial z_1}\\cdot f&#39;(s_6)\\) \\(\\frac{\\partial l}{\\partial \\beta_{3,6}} = \\frac{\\partial l}{\\partial s_6}\\cdot x_3\\) \\(\\frac{\\partial l}{\\partial \\beta_{4,6}} = \\frac{\\partial l}{\\partial s_6}\\cdot x_4\\) \\(\\frac{\\partial l}{\\partial \\beta_{5,6}} = \\frac{\\partial l}{\\partial s_6}\\cdot x_5\\) \\(\\frac{\\partial l}{\\partial x_3} = \\frac{\\partial l}{\\partial s_6}\\cdot \\beta_{3,6}\\) \\(\\frac{\\partial l}{\\partial x_4} = \\frac{\\partial l}{\\partial s_6}\\cdot \\beta_{4,6}\\) \\(\\frac{\\partial l}{\\partial x_5} = \\frac{\\partial l}{\\partial s_6}\\cdot \\beta_{5,6}\\) \\(\\frac{\\partial l}{\\partial s_3} = \\frac{\\partial l}{\\partial x_3}\\cdot f&#39;(s_3)\\) \\(\\frac{\\partial l}{\\partial s_4} = \\frac{\\partial l}{\\partial x_4}\\cdot f&#39;(s_4)\\) \\(\\frac{\\partial l}{\\partial s_4} = \\frac{\\partial l}{\\partial x_5}\\cdot f&#39;(s_5)\\) \\(\\frac{\\partial l}{\\partial \\beta_{1,3}} = \\frac{\\partial l}{\\partial s_3}\\cdot x_1\\) \\(\\frac{\\partial l}{\\partial \\beta_{2,3}} = \\frac{\\partial l}{\\partial s_3}\\cdot x_2\\) \\(\\frac{\\partial l}{\\partial \\beta_{1,4}} = \\frac{\\partial l}{\\partial s_4}\\cdot x_1\\) \\(\\frac{\\partial l}{\\partial \\beta_{2,4}} = \\frac{\\partial l}{\\partial s_4}\\cdot x_2\\) \\(\\frac{\\partial l}{\\partial \\beta_{1,5}} = \\frac{\\partial l}{\\partial s_5}\\cdot x_1\\) \\(\\frac{\\partial l}{\\partial \\beta_{2,5}} = \\frac{\\partial l}{\\partial s_5}\\cdot x_2\\) \\(\\frac{\\partial l}{\\partial x_1} = \\frac{\\partial l}{\\partial s_3}\\cdot \\beta_{1,3}+\\frac{\\partial l}{\\partial s_4}\\cdot \\beta_{1,4}+\\frac{\\partial l}{\\partial s_5}\\cdot \\beta_{1,5}\\) \\(\\frac{\\partial l}{\\partial x_2} = \\frac{\\partial l}{\\partial s_3}\\cdot \\beta_{2,3}+\\frac{\\partial l}{\\partial s_4}\\cdot \\beta_{2,4}+\\frac{\\partial l}{\\partial s_5}\\cdot \\beta_{2,5}\\) En resúmen, para optimizar la verosimilitud por gradiente estocástico debemos seguir lo siguientes pasos (llamados _back-propagation): 1. Inicializar los parámetros \\(\\hat{\\beta}^{(0,n)}=\\{\\hat{\\beta_{u,v}}^{(0,n)}:u\\rightarrow v\\}\\) 2. Para cada fase de entrenamiento \\(t=1,2,\\dots\\) Para \\(i = 1,2,\\dots,n\\) - Obtener el vector de probabilidades \\(z = (z_1,\\dots,z_N)\\) usando como input el ejemplo \\(x^i\\) y los parámteros \\(\\hat{\\beta}^{(t, i-1)}\\) Obtener las derivadas parciales del la log-verosimilitud con respecto a: \\[\\{z_l:1\\leq l\\leq N\\} \\ \\{s_v:v\\in S_M\\} \\ \\{\\beta_{uv}:u\\in S_{M-1}, v\\in S_M\\}\\] \\[\\{x_v:v\\in S_{M-1}\\} \\ \\{s_v:v\\in S_{M-1}\\} \\ \\{\\beta_{uv}:u\\in S_{M-2}, v\\in S_{M-1}\\}\\] \\[\\{x_v:v\\in S_{M-2}\\} \\ \\{s_v:v\\in S_{M-2}\\} \\ \\{\\beta_{uv}:u\\in S_{M-3}, v\\in S_{M-1}\\}\\] \\[\\vdots\\] \\[\\{x_v:v\\in S_2\\} \\ \\{s_v:v\\in S_2\\} \\ \\{\\beta_{uv}:u\\in S_1, v\\in S_2\\}\\] en ese orden. 3. Actualizar los parámetros \\[\\hat{\\beta_{uv}}^{(t,i)} \\leftarrow \\hat{\\beta_{uv}}^{(t,i-1)}+\\alpha \\cdot \\frac{\\partial l_i}{\\partial \\beta_{uv}}\\] En general, a este procedimiento de optimización se le conoce como descenso gradiente. 11.2 Saturación Si durante el proceso de entrenamiento, algún nodo \\(v\\) tiene una valor \\(|s_v|\\) muy grande entonces el valor de \\(f&#39;(s_v)\\) será muy cercano a cero para funciones de activación sigmoides. Debido a la regla de la cadena, los valores de \\(\\beta_{uv}\\) se moverán muy lentamente hacia el óptimo, en este caso se dice que el nodo \\(v\\) está saturado. Para evitar problemas de saturación al inicio del entrenamiento comúnmente debemos estandarizar las covariables para que tengan media cero y varianza unitaria. También es conveniente inicializar los parámetos \\(\\hat{\\beta}^{(0,n)}\\) cercanos a cero, comúnmente elegidos uniformemente entre \\([-c,c]\\) para \\(c \\in (0,1)\\). Sin embargo debemos tener cuidado pues si \\(\\hat{\\beta}^{(0,n)}=0\\) entonces \\(\\frac{\\partial l}{\\partial x_v}=0 \\ \\forall \\ v\\in S_{M-1}\\) lo que hará que el algoritmo no se mueva. 11.3 Regularización Un problema típico en las redes neuronales es que suelen sobre ajustar los datos de entrenamiento dado que suelen haber más parámetros que variables. Una forma de regularizar es imponer una penalización equivalente al cuadrado de los parámetros, también llamada ridge o L2, a la log-verosimilitud y maximizar respecto a esta log-verosimilitud penalizada. \\[{\\it l}\\left(\\beta\\right)^{ridge}={\\it l}\\left(\\beta\\right)+\\frac{\\lambda}{2}||\\beta||_2^2 = \\sum_{i=1}^{n}\\sum_{{\\it l}=1}^{L}\\mathbb{I}_{\\left\\{ y_{i}={\\it l}\\right\\} }log\\left(\\frac{e^{s_{v_{{\\it l}}}\\left(x_{i},\\beta\\right)}}{e^{s_{v_{1}}\\left(x_{1},\\beta\\right)}+\\dots+e^{s_{v_{L}}\\left(x_{L},\\beta\\right)}}\\right)+\\frac{\\lambda}{2}||\\beta||_2^2\\] 11.4 Redes Neuronales en R …WIP… "],
["qué-es-una-svm.html", "Capítulo 12 ¿Qué es una SVM?", " Capítulo 12 ¿Qué es una SVM? Las maquinas de soporte vectorial son, en principio, un clasificador lineal con las característica de que no asume ninguna dsitribución a los datos y directamente busca el hiperplano óptimo que separa los datos en clases. De forma inherente una SVM es un clasificador de dos clases, sin embargo para usarlas en problemas con \\(L&gt;2\\) clases bastará con aplicar el algorítmo repetidamente comparando cada clase contra el resto o bien aplicar distintas SVM para cada par de clases y finalmente clasificar un nuevo punto vía mayoría clasificada para las SVMs. En este capítulo exploraremos brevemente la teoría detras de las SVM para dos clases. Será conveniente etiquetar las clases como \\(\\mathcal{L}=\\{-1,-1\\}\\). Dado cualquier par \\((\\beta,\\beta_0) \\in \\mathbb{S}^{p-1}\\times\\mathbb{R}\\) existe un clasificador lineal que asigna a los puntos \\(\\{x:x^T\\beta+\\beta_0&gt;0\\}\\) a 1 y a los puntos \\(\\{x:x^T\\beta+\\beta_0&lt;0\\}\\) a -1. Este clasificador es ambiguo en la decisión para la frontera del hiperplano dado por \\(H_{\\beta_0,\\beta}\\colon= \\{x:x^T\\beta+\\beta_0=0\\}\\). Para cualquier observación \\((x_i,y_i)\\), el clasificador funciona correctamente si y solo si \\(y_i(x_i^T\\beta +\\beta_0)&gt;0\\). Más aún, el valor \\(|y_i(x_i^T\\beta +\\beta_0)|\\) indica la distancia de \\(x_i\\) a la frontera de decisión. Las SMV buscan aquel hiperplano que separa completamente a las dos clases y que maximiza la distancia hacia el punto más cercano. Este plano puede determinarse a través del siguiente problema de optimización: \\[ \\begin{equation} \\max_{\\beta,\\beta_0,||\\beta||_2=1} M \\ sujeto \\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq M,\\ i=1,\\dots,n \\end{equation} \\] Donde \\(M\\) lo podemos interpretar como el margen alrededor del hiperplano optimo que no contiene a ninguna observación. "],
["estimación-de-los-coeficientes.html", "Capítulo 13 Estimación de los coeficientes", " Capítulo 13 Estimación de los coeficientes El problema de optimización mostrado es complicado de resolver fundamentalmente por la restricción \\(||\\beta||_2=1\\), sin embargo dado que el plano \\(H_{\\beta_0,\\beta}\\) es invariente bajo escalamiento sobre \\(\\beta_0\\) y \\(\\beta\\) podemos aplicar el factor \\(1/M\\) (de tal forma que \\(||\\beta||_2=1/M\\)) y el problema se transformaría en: \\[\\begin{equation} \\max_{\\beta,\\beta_0}\\frac{1}{||\\beta||_2} \\ sujeto\\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq 1,\\ i=1,\\dots,n \\end{equation} \\] Si además trabajamos con el problema equivalente \\[\\begin{equation} \\min_{\\beta,\\beta_0}\\frac{1}{2}||\\beta||_2^2 \\ sujeto\\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq 1,\\tag{13.1},n (#eq:optim) \\end{equation} \\] tendremos ahora un problema de optimización con una función objetivo cuadrática y restricciones lineales que puede resolverse eficientemente usando optimizadores convexos estándar. Para resolver el problema de optimización se usa el problema dual de Lagrange cuya teoría nos dice que la mejor cota inferior para \\(||\\beta||_2^2/2\\) es igual al valor óptimo: \\[\\begin{equation} \\max_{\\lambda\\in \\mathbb{R_{\\geq0}^n}} \\min_{\\beta,\\beta_0} L(\\beta,\\beta_0;\\lambda) \\end{equation} \\] donde \\(L(\\beta,\\beta_0;\\lambda)\\) es el lagrangiano de \\(\\@ref(eq:optim)\\). Los estimadores de los coeficientes son calculados usando a los puntos más cercanos a la frontera de decisión como puntos soporte de ahí que a esos puntos se les llame vectores soporte Sin meternos demasiado en la teoría, el proceso de estimación de los coeficientes de una SVM y la obtención de predicciones usando a dualidad de Lagrange es el que sigue: Resolver el problema dual de \\(\\@ref(eq:optim)\\) Sean \\(S=\\{i:\\lambda_i^*\\neq0\\}\\) los índices de los vectores soporte Calcular \\(\\beta^*=\\sum_{i\\in S}\\lambda_i^*y_i x_i\\) y \\(\\beta_0^*=-\\frac{1}{2}\\{\\min_{i:y_i=1}x_i^T\\beta^*+\\max_{i:y_i=-1}x_i^T\\beta^*\\}\\) Para cada punto nuevo \\(x\\), lo clasificamos con \\[\\psi^{SVM}(x)=sgn(x^T\\beta^*+\\beta_0^*)\\] "],
["rkhs-y-el-método-kernel.html", "Capítulo 14 RKHS y el método kernel 14.1 ¿Cómo escoger un kernel k?", " Capítulo 14 RKHS y el método kernel Típicamente nos encontraremos con datos para los cuales no será evidente la existencia de un hiperplano que pueda separar a las clases. Esto puede suceder porque la frontera de clasificación es no lineal, o bien porque los datos tienen mucha varianza (ruido) y las densidades de las clases se intersectan. Existen dos formas de generalizar el hiperplano de separación para resolver ambos problemas. Una forma de resolver el problema de no separabilidad es llevar los datos a alguna dimensión superior mediante \\(x \\mapsto \\phi(x)=(\\phi_1(x),\\phi_2(x),\\dots)\\) donde \\(\\phi_1,\\phi_2,\\dots\\) son funciones reales en algún conjunto \\(\\mathcal{X}\\), donde \\(\\mathcal{X}\\) no requiere alguna estructura específica. A la función \\(\\phi\\) se le conoce como función característica (feature map) y a sus componentes \\(\\phi_1,\\phi_2,\\dots\\) se les llama características (features) En dimensiones suficientemente grandes los puntos correspondientes a dos clases son siempre separables. Una pregunta natural es ¿cómo escoger buenas características y cuántas deberíamos escoger? Antes de responder estas preguntas asumamos que ya tenemos la función \\(\\phi\\) y definamos \\(k(x,x&#39;)=\\langle \\phi(x),\\phi(x&#39;)\\rangle\\) para cualesquiera \\(x,x&#39;\\in \\mathcal{X}\\), donde \\(\\langle \\cdot,\\cdot \\rangle\\) es el producto interior en el espacio de características. Con ello podemos obtener los coeficientes de la SVM como sigue: 1: Resolver el problema dual en el espacio de características \\[\\max_{\\lambda\\in \\mathbb{R}_{\\geq 0}^n,\\sum_i\\lambda_i y_i=0}\\sum_{i=1}^n\\lambda_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\lambda_i \\lambda_j y_i y_j k(x_i,x_j)\\] para obtener \\(\\lambda_1^*,\\cdots,\\lambda_n^*\\) 2: Sea \\(S=\\{i:\\lambda_i^*\\neq 0\\}\\) el conjunto de índices de los vectores soporte. 3: Calcular \\(\\beta_0^*=\\sum_{i\\in S}\\lambda_i^*y_i \\phi(x_i)\\) y \\[\\beta_0^*=-\\frac{1}{2}\\{\\min_{i:y_i=1}\\sum_{j\\in S}\\lambda _j^*y_j k(x_i,x_j)+\\max_{i:y_i=-1}\\sum_{j \\in S}\\lambda_j^* y_j k(x_i,x_j) \\}\\] 4: Cada nueva observación la clasificamos de acuerdo a \\[sgn\\{ \\sum_{i\\in S}\\lambda_i^* y_i k(x_i,x) + \\beta_0^* \\}\\] Notemos que el procedimiento anterior depende de \\(\\phi\\) a través de \\(k(\\cdot,\\cdot)\\), excepto por \\(\\beta^*\\) pero si solo queremos hacer predicciones entonces no es necesario saber exactamente quién es \\(\\phi\\) siempre que la función \\(k\\) esté dada. El siguiente teorema nos dice que cualquier kernel definido positivo \\(k\\) siempre puede obtenerse como el producto interno en algún espacio de características de dimensión infinita. Recordemos que un espacio de Hilbert es un espacio producto interior donde toda sucesión de Cauchy tiene límite respecto a la norma asociada al producto interior. Podemos pensar a un espacio de Hilbert como la generalización del espacio Euclideano. Definición: Una función simétrica \\(k:\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}\\) se llama kernel definido positivo en \\(\\mathcal{X}\\) si \\((k(x_i,x_j))_{i,j=1,\\dots,n}\\) es una matriz semi-definida positiva para cualquier \\(n\\in \\mathbb{N}\\) y \\(x_1,\\dots,x_n\\in \\mathcal{X}\\) Teorema: Si \\(k:\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}\\) es un kernel definido positivo, entonces existe un espacio de Hilbert \\(\\mathcal{H}\\) y una función \\(\\phi :\\mathcal{X}\\rightarrow \\mathcal{H}\\) tal que \\(k(x_1,x_2)=\\langle\\phi(x_1),\\phi(x_2)\\rangle\\) donde \\(\\langle\\cdot,\\cdot \\rangle_\\mathcal{H}\\) es el producto interno en \\(\\mathcal{H}\\). Esto significa que en lugar de trabajar directamente en espacios de dimensión superior, podemos trabajar con kernels definidos positivos, más aún la calidad de las SVM dependerá solamente de la elección de las funciones krnel \\(k\\). 14.1 ¿Cómo escoger un kernel k? Algunos kernels usados en la practica: Kernel lineal: \\(k(x,x&#39;)=x^Tx&#39;\\) Kernel Polinomial: $ k(x,x’)=(c+xTx’)d$. Típicamente usado si suponemos que la similaridad entre dos observaciones está dada por las covariables y interacciones de ellas. Kernel Gaussiano: \\(k(x,x&#39;)=\\exp\\{-\\frac{1}{2\\sigma^2}||x-x&#39;||_2^2\\}\\). Es el más popular para trabajar con características no lineales. Kernel de Laplace: \\(k(x,x&#39;)=\\exp\\{-\\frac{1}{\\sigma}||x-x&#39;||_2\\}\\). Similar al gaussiano, éste mide la similaridad de observaciones basados en la distancia en \\(\\mathcal{X}\\) "],
["margen-suave.html", "Capítulo 15 Margen suave", " Capítulo 15 Margen suave Recordemos que en el contexto de las SVM llamamos margen al espacio entre la frontera de decisión y el punto más cercano de cada clase. Una segunda forma de lidiar con problemas de no separabilidad es buscar el hiperplano óptimo con el margen \\(M\\), más amplio tal que el número de observaciones clasificadas erroneamente sea pequeño. Matemáticamente, lo que hacemos es agregar variables de holgura al problema de optimización original: \\[ \\begin{equation} \\max_{\\beta,\\beta_0,||\\beta||_2=1} M \\ sujeto \\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq M(1-\\xi_i),\\ i=1,\\dots,n \\end{equation} \\] \\[\\sum_{i=1}^n \\xi_i\\leq K\\] "],
["svms-en-r.html", "Capítulo 16 SVMs en R", " Capítulo 16 SVMs en R ..WIP… "],
["antecedentes.html", "Capítulo 17 Antecedentes", " Capítulo 17 Antecedentes También llamados métodos basados en árboles, son algorítmos que hacen particiones al espacio de covariables y a cada una de ellas le ajustan un modelo simple, por ejemplo una constante. Para hacer la tarea sencilla pensemos las particiones como binarias, es decir, el primer pasó será partir el espacio de covariables en dos, luego cada parte se divide en dos regiones más y continuamos el proceso hasta cumplir alguna regla de paro. Una parte muy importante de este tipo de modelos binarios es la interpretabilidad pues el árbol estratifica la población de acuerdo a sus características. "],
["árboles-de-regresión.html", "Capítulo 18 Árboles de regresión", " Capítulo 18 Árboles de regresión Supongamos que tenemos \\(p\\) covariables, una variable de respuesta/objetivo y \\(N\\) observaciones. El argoritmo debe poder decidir las variables a particionar, los puntos de quiebre y la topología del árbol. Supongamos que tenemos una partición de en \\(M\\) regiones \\(R_1,R_2,\\dots,R_M\\) y modelamos la respuesta como la constante \\(c_m\\) en cada región: \\[f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)\\] Si tomamos como criterio minimizar la suma de cuadrados \\(\\sum(y_i-f(x_i))^2\\), es sencillo obtener que la mejor \\(\\hat{c}_m\\) es el promedio de \\(y_i\\) en la región \\(R_m\\): \\[\\hat{c}_m=prom(y_i|x_i\\in R_m)\\] Ahora para encontrar la mejor partición binaria en términos de la suma de cuadrados seguimos lo siguiente pasos: Consideremos la variable de partición \\(j\\) y el punto de quiebre \\(s\\) definimos el par de semiplanos \\[R_1(j,s)=\\{X|X_j\\leq s\\}\\] \\[R_2(j,s)=\\{X|X_j &gt;s\\}\\] Entonces buscamos la variabe \\(j\\) y el punto de quiebre \\(s\\) que resuelve \\[\\min_{j,s}[\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]\\] Para cualquier elección de \\(j\\) y \\(s\\) la minimización dentro de los corchetes se resuelve como \\[\\hat{c}_1=prom(y_i|x_i \\in R_1(j,s))\\] y \\[\\hat{c}_2=prom(y_i|x_i \\in R_2(j,s))\\] Para cada variable de partición, encontrar el punto de quiebre \\(s\\) se puede hacer muy rápidamente pasando por todos los puntos. Una vez encontrada la mejor partición, volvemos a hacer particiones siguiendo el mismo método. ¿Qué tanto debemos dejar que un árbol crezca? El crecimiento de un árbol hace referencia a la cantidad de particiones que contiene. En este sentido un árbol muy grande podría sobre ajustar los datos mientras que uno muy pequeño podría no capturar la estructura de ellos. El tamaño del árbol es un parámetro que incide directamente en la complejidad del modelo y el tamaño óptimo debe ser elegido de forma adaptativa según los datos. La solución usada es dejar crecer el árbol (\\(T_0\\)) hasta tener un tamaño de nodo específico y entonces podar el árbol usando una estrategia costo-complejidad como mostramos en seguida. Definimos un sub-árbol \\(T \\subset T_0\\) como cualquier árbol que pueda ser obtenido de podar el árbol \\(T_0\\), es decir, colapsar el número de sus nodos internos. Indexemos los nodos terminales con \\(m\\), siendo el nodo \\(m\\) representante de la región \\(R_m\\). Sea \\(|T|\\) el número de nodos terminales en el árbol \\(T\\) y sean \\[N_m=\\#\\{x_i\\in R_m\\}\\] \\[\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m}y_i\\] \\[Q_m(T)=\\frac{1}{N_m}\\sum_{x_i\\in R_m}(y_i-\\hat{c}_m)^2\\] definimos entonces el criterio costo-complejidad como: \\[C_\\alpha(T)=\\sum_{m=1}^{|T|}N_mQ_m(T)+\\alpha |T|\\] La idea es encontrar, para cada \\(\\alpha\\), el sub-árbol \\(T_\\alpha \\subseteq T_0\\) que minimice \\(C_\\alpha(T)\\). El parámetro \\(\\alpha \\geq 0\\) controla el tradeoff entre el tamaño del árbol y la bondad de ajuste a los datos. Valores grandes de \\(\\alpha\\) resultará en árboles pequeños y viceversa. "],
["árboles-de-clasificación.html", "Capítulo 19 Árboles de clasificación", " Capítulo 19 Árboles de clasificación Si el objetivo es clasificar los cambios necesarios al algoritmo ocurren en los criterios de partición y de poda. Para árboles de regresión usamos la medida de impureza del nodo, error cuadrático, \\(Q_m(T)\\) pero ésta no es útil para clasificación. Para cada nodo \\(m\\), representando una región \\(R_m\\) con \\(N_m\\) observaciones sea \\[\\hat{p}_{mk}=\\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i=k)\\] la proporción de observaciones de la clase \\(k\\) en el nodo \\(m\\). Clasificamos, entonces, las observaciones en el nodo \\(m\\) a la clase \\(k(m)=\\arg \\max_k \\ \\hat{p}_{mk}\\), es decir, la clase mayoritaria en el nodo \\(m\\). Algunas medidas \\(Q_m(T)\\) de impureza del nodo son: Error de clasificación: \\(\\frac{1}{N_m}\\sum_{i \\in R_m}I(y_i \\neq k(m))=1-\\hat{p}_{mk(m)}\\) Índice GINI: \\(\\sum_{k \\ne k&#39;}\\hat{p}_{mk}\\hat{p}_{mk&#39;}=\\sum_{k=1}^K\\hat{p}_{mk}(1-\\hat{p}_{mk})\\) Entropía cruzada: \\(-\\sum_{k=1}^K\\hat{p}_{mk}\\log\\hat{p}_{mk}\\) En el caso de dos clases, si \\(p\\) es la proporción de la segunda clase, las medidas son: Error de clasificación: \\(1-max(p,1-p)\\) Índice GINI: \\(2p(1-p)\\) Entropía: \\(-p\\log p-(1-p) \\log (1-p)\\) "],
["alguno-problemas-en-los-árboles.html", "Capítulo 20 Alguno problemas en los árboles 20.1 Covariables categóricas 20.2 La matriz de pérdida", " Capítulo 20 Alguno problemas en los árboles 20.1 Covariables categóricas Al trabajar con una covariable/predictora categórica con \\(q\\) posibles valores existen \\(2^{q-1}-1\\) posibles particiones binarias y el tiempo de computación se vuelve enorme para valores grandes de \\(q\\). Sin embargo si tenemos un problema de clasificación de dos clases \\((0,1)\\) podemos simplificar el problema ordenando los valores de la variable categórica de acuerdo a la propoción en la clase 1, entonces hacemos las particiones como si la variable fuera ordinal. Es posible demostrar que este procedimiento logra la partición óptima en términos de entropía o índice Gini de entre las \\(2^{q-1}-1\\) posibles. 20.2 La matriz de pérdida En problemas de clasificación, las consecuencias de hacerlo incorrectamente pueden ser más importantes para alguna clase que para otras. Para resolver esta situación definimos una matriz de pérdida \\(L\\) de \\(K\\times K\\), con \\(L_{kk&#39;}\\) siendo la pérdida incurrida por clasificar erroneamente una clase \\(k\\) como una \\(k&#39;\\). Típicamente \\(L_{kk}=0\\ \\forall k\\). Para incorporar las pérdidas en el modelo podemos modificar el índice Gini como \\(\\sum_{k \\neq k&#39;}L_{kk&#39;}\\hat{p}_{mk}\\hat{p}_{mk&#39;}\\). Aunque en el caso de \\(k&gt;2\\) clases es muy útil, no lo es para el caso \\(k=2\\), ¿por qué? "],
["árboles-en-r.html", "Capítulo 21 Árboles en R", " Capítulo 21 Árboles en R …WIP… "]
]
