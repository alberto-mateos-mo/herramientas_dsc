[
["index.html", "Herramientas Estadísticas para Ciencia de Datos Herramientas Estadísticas para Ciencia de Datos 0.1 Objetivos 0.2 Estructura 0.3 Detalles técnicos Licencia", " Herramientas Estadísticas para Ciencia de Datos Sofía Villers Gómez David Alberto Mateos Montes de Oca Herramientas Estadísticas para Ciencia de Datos Primera edición del libro de texto para el curso Herramientas Estadísticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias. 0.1 Objetivos Como el título lo indica, a lo largo de este libro se expondrán diferentes modelos estadísticos y sus aplicaciones con un enfoque a Ciencia de Datos. El objetivo es proveer al lector de las herramientas necesarias para comprender los fundamentos de estos modelos y sus aplicaciones mediante el uso del lenguaje R. 0.2 Estructura El libro se descompone en dos grandes secciones, una de ellas enfocada a presentar diferentes paquetes de R considerados de gran utilidad para la práctica de Ciencia de Datos, la otra se enfoca en la teoría y aplicación de diferentes modelos. La estructura del libro sigue el orden en que se imparte la materia en la Facultad de Ciencias sin embargo hemos diseñado los capítulos suficientemente independientes como para ser consultados en el orden de preferencia del lector. 0.3 Detalles técnicos Este libro fue escrito con bookdown usando RStudio. Esta versión fue escrita con: ## Finding R package dependencies ... Done! ## setting value ## version R version 4.0.0 (2020-04-24) ## os Windows 10 x64 ## system x86_64, mingw32 ## ui RStudio ## language (EN) ## collate Spanish_Mexico.1252 ## ctype Spanish_Mexico.1252 ## tz America/Mexico_City ## date 2020-09-12 Licencia This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],
["instalando-r-y-rstudio.html", "Capítulo 1 Instalando R y RStudio 1.1 R y RStudio 1.2 RStudio Cloud 1.3 Recursos adicionales", " Capítulo 1 Instalando R y RStudio 1.1 R y RStudio Instala R, a free software environment for statistical computing and graphics desde el CRAN de acuerdo a tu sistema operativo. Instala el entorno de desarrollo integrado RStudio Si previamente habías instalado alguna versión de R y RStudio en tu computadora, sugerimos reinstalar ambos para tener las versiones más actualizadas. 1.2 RStudio Cloud Aunque a lo largo del libro se mostrará a detalle el código de R usado, también proveeremos de links a proyectos de RStudio Cloud para facilitar reproducir los resultados aquí mostrados. Para acceder a tales proyectos será necesario contar con una cuenta en RStudio Cloud. 1.3 Recursos adicionales Si no estás del todo familiarizado con el lenguaje, te recomendamos lo siguientes recursos donde podrás encontrar información útil sobre R y RStudio. Cómo usar RStudio RStudio para el aprendizaje de R R: Preguntas Frecuentes R: Instalación y administración "],
["conocimientos-básicos-de-r.html", "Capítulo 2 Conocimientos básicos de R 2.1 Buscando ayuda 2.2 Vectores 2.3 El ambiente (environment). 2.4 Lectura y escritura de archivos 2.5 Algunas funciones variadas 2.6 Objetos 2.7 Algunos ejemplos más avanzados 2.8 Paquetes", " Capítulo 2 Conocimientos básicos de R 2.1 Buscando ayuda Por sí mismo, R cuenta con funciones que nos permitirán obtener ayuda sobre R en general pero también sobre las diferentes funciones que estemos usando para resolver algún problema. Para mostrar un manual de ayuda general en formato HTML corremos el siguiente código en la consola: help.start() El manual será mostrado en el panel de ayuda dentro de RStudio pero puede abrirse con el navegador en caso de que así lo deseemos. Supongamos ahora que dentro de nuestro flujo de trabajo estamos utilizando la funcíón log() sin embargo no estamos familiarizados con ella y deseamos obtener ayuda sobre el uso de la función. Para a tener acceso a la documentación de la función podemos usar alguna de las siguientes líneas de código: help(log) ?log En el caso particular de RStudio, existe un atajo con el que podemos abrir la documentación de cualquier función sin necesidad de correr alguna de las líneas anteriores en la consola. El atajo consiste de colocar el cursor de texto entre cualesquiera caractéres de la función y presionar el botón F1 de nuestro teclado. Adicionalmente tenemos las siguientes funciones: apropos(&quot;plot&quot;) help.search(&quot;plot&quot;) apropos() nos ayudará a encontrar las funciones que incluyan el texto (en este caso plot) en su nombre. Por otro lado, help.search() nos mostrará la documentación de las funciones que incluyan el texto, en este caso plot. Como se indica, cualquiera de esas opciones te permitirá tener acceso a la documentación de la funciones donde podrás encontrar: para qué se usan, qué argumentos necesitan, lo que regresan y algunos ejemplos del uso de las mismas. Una buena parte de los errores que podamos encontrarnos en nuestros flujos de trabajo se resolverán al revisar la documentación de la función que esté generando el error, sin embargo habrá ocasiones en las que los errores sean más complejos y la documentación no nos dará una buena pista de cómo remediarlos, en tal caso la mejor estrategía sera utilizar tu buscador favorito para buscar el error. Será tan fácil cómo copiar el error que la consola esté arrojando y ponerlo en la barra de búsqueda de e.g. [Google][google]. 2.2 Vectores Quizá ya hayas escuchado lo siguiente en algún lado: R es un lenguaje vectorial. El comportamiento al que hace referencia esta frase será de mucha ayuda en la construcción de rutinas avanzadas de programación que veremos más adelante, por ahora la implicación más relevante reside en el hecho de que la estructura básica en R serán justamente vectores. Los siguientes ejemplos nos muestran cómo crear vectores numéricos en R: roma &lt;- c(0.1, 0.2, 0.3) assign(&quot;mora&quot;, c(0.4, 0.5, 0.6)) c(0.7, 0.8, 0.9) -&gt; z mora &lt;- c(roma, 0, 0, 0, roma) N.B. Los vectores en R no solo pueden ser numéricos también los hay aquellos con cadenas de texto. En seguida vemos algunos ejemplos que nos permitirán acceder a elementos de un vector: mora[3] ## [1] 0.3 mora[-3] ## [1] 0.1 0.2 0.0 0.0 0.0 0.1 0.2 0.3 mora[c(1,5,7)] ## [1] 0.1 0.0 0.1 2.2.1 Arítmetica de vectores El siguiente ejemplo muestra el comportamiento asociado a un lenguaje vectorial que mencionabamos anteriormente: v &lt;- 2*roma+mora+1 v ## [1] 1.3 1.6 1.9 1.2 1.4 1.6 1.3 1.6 1.9 Existen una enorme cantidad de funciones para manipular los datos dentro de nuestros vectores, en seguida se muestran algunas: sum(v) ## suma todos los elementos del objeto prod(v) ## multiplica todos los elementos del objeto max(v) ## encuentra el valor máximo min(v) ## encuentra el valor mínimo which.max(v) ## índice o posición del máximo which.min(v) ## índice o posición del mínimo range(v) ## vector de dos entradas con el mínimo y máximo respectivamente 2.3 El ambiente (environment). Los objetos que guardemos o asignemos en nuestros flujos de trabajo serán almacenados en un espacio de memoria que se llama ambiente o en inglés environment. Desde RStudio podremos ver lo que se guardado en este espacio de memoria en el panel Environment. Adicional a ello, existen algunas funciones que nos permiten explorar nuestro ambiente: ## Muestra los nombres de los objetos en memoria ls() ## Muestra las variables con cierta serie de caracteres en su nombre ls(pat=&quot;m&quot;) ## Muestra las variables las cuales su nombre empieza con el caracter dado ls(pat=&quot;^m&quot;) ## Muestra detalles de los objetos en memoria ls.str() ## Eliminar todas las variables de &#39;Global Environment&#39; rm(list=ls()) ## Eliminar únicamente variables que empiezan con la letra m rm(list=ls(pat=&quot;^m&quot;)) ## Tipo de elementos del objeto v mode(v) ## Longitud del objeto v length(v) 2.4 Lectura y escritura de archivos En cuanto a archivos externos, a lo largo del libro únicamente usaremos archivos con extensiones .csv y .txt sin embargo R no está limitado a este tipo de archivos, existen una amplia variedad de formatos que se pueden leer a través de paquetes diseñados específicamente para ese fin. El siguiente ejemplo muestra cómo se haría la lectura de un archivo con extensión .txt: datos&lt;-read.table(&quot;data.txt&quot;, # nombre del archivo (con extensión) entre comillas header = TRUE, # TRUE o FALSE, indicando si el archivo tiene como primer renglón el nombre de las columnas sep=&quot;\\t&quot; # separador de los campos ) Una vez que leemos el archivo externo en R, el objeto donde se almacenará la información será de tipo data.frame. Muchas veces será de nuestro interés exportar objetos data.frame a archivos externos, tal objetivo lo podemos lograr con alguna de las siguientes dos opciones: write.table(datos, &quot;toma.txt&quot;, append=F, sep=&quot;\\t&quot;) write.csv(datos, &quot;toma2.csv&quot;) N:B. Como en la lectura de datos, R no está limitado a exportar archivos .csv o .txt también existe una amplia variedad de formatos que podemos generar. 2.5 Algunas funciones variadas A continuación se muestra un conjunto bastante pequeño de funciones que nos ayudarán a alcanzar distintos objetivos: ## Generar un vector con valores subsecuentes x &lt;- 1:30 ## Generar un vector con cierta secuencia y &lt;- seq(1, 5, 0.5) w &lt;- seq(10, 0, -0.5) ## Genera un vector que repite un dato cierto número de veces z &lt;- rep(1, 20) ## Genera un vector con series valores subsecuentes hasta los números indicados q &lt;- sequence(3:5) q &lt;- sequence(c(10, 5)) ## Genera series regulares de factores dados. q &lt;- gl(3, 5, length = 30) q &lt;- gl(2, 6, label = c(&quot;Hombre&quot;,&quot;Mujer&quot;)) ## A diferencia de los anteriores, la siguiente función genera un data frame con todas las ## posible combinaciones de vectores o factores dados como argumentos q &lt;- expand.grid(h = c(60, 80), w = c(100, 300), sex = c(&quot;Hombre&quot;, &quot;Mujer&quot;)) 2.6 Objetos Los ejemplos anteriores consisten en el uso de funciones, dichas funciones generan cierto tipo de objetos para regresar el resultado. Si queremos crear de manera manual cierto tipo de objetos, podemos hacerlo de la siguiente manera: 2.6.1 Vectores ## Vector (numeric, logical, character) a &lt;- vector(mode= &quot;logical&quot;, length=5) b &lt;- logical(length=5) 2.6.2 Factores a &lt;- factor(1:3, labels = c(&quot;Hola&quot;, &quot;Adios&quot;, &quot;Ah&quot;)) b&lt;- factor(x = c(2, 5), levels = 2:5) c &lt;- factor(1:5, exclude = 4) 2.6.3 Matrices a &lt;- matrix(1:6, 2, 3, byrow = F) ## Otra forma de crear una matriz b &lt;- 1:15 dim(b) &lt;- c(5,3) 2.6.4 Data Frames x &lt;- 1:4 n &lt;- 10 M &lt;- c(10, 35) a &lt;- data.frame(x, n, M) 2.6.5 Listas a &lt;- list(x, M) b &lt;- list(A = x, B = M) 2.6.6 Expresiones x &lt;- 3 y &lt;- 2.5 z &lt;- 1 exp &lt;- expression(x/(y + exp(z))) 2.7 Algunos ejemplos más avanzados 2.7.1 Operadores Aritméticos Comparación Lógicos Suma (+) Menor que (&lt;) NOT (!) Resta (-) Mayor que (&gt;) AND (&amp;) Multiplicación (*) Menor o igual que (&lt;=) OR (|) División (/) Mayor o igual que (&gt;=) Cierto (TRUE) Potencia (^) Igual (==) Falso (FALSE) Modulo (%%) Diferente (!=) División entera (%/%) 2.7.2 Ciclos, condiciones y funciones Al ser R un lenguaje de programación podemos usar: ciclos ## Ciclo for for (anio in c(2000,2001,2002,2003,2004,2005,2006,2007)){ print(paste(&quot;Cuenta&quot;,anio)) } ## Ciclo while i &lt;- 1 while(i &lt; 10){ print(i) i &lt;- i+1 } condiciones for (i in 1:10){ ## Condición if if(!i %% 2){ next } print(i) } Particularmente en la programación con R, será de mucha utilidad aprender a crear funciones: funcionfactorial&lt;-function(a){ res &lt;- 1 if(a&lt;0){ return(&quot;No existen factoriales de números negativos.&quot;) } else if(a==0){ return(res) } else{ for(i in 1:a){ res &lt;- res*i } return(res) } } 2.8 Paquetes En R llamamos librerías o paquetes a conjuntos de funciones diseñados para un fin específico. Por ejemplo el paquete [ggplot2][ggplot2-github] contiene funciones diseñadas para crear gráficos usando una sintáxis llamada la gramática de gráficas. Para descargar alguna librería de nuestra interés debemos utilizar el siguiente código: install.packages(&quot;ggplot2&quot;) # Dentro de las comillas ponemos el nombre del paquete que queremos instalar Dado que se trata de una instalación, el código anterior solamente será necesario la primera vez que instalemos el paquete. Una vez instalado, necesitamos hacer que R sepa que queremos usar la funciones de ese paquete, para lo cual usamos el siguiente código: library(ggplot2) # El argumento de la función será el nombre del paquete previamente instalado "],
["introducción.html", "Capítulo 3 Introducción", " Capítulo 3 Introducción En esta sección se revisaran algunos de los puntos más importentes sobre la forma en que se presenta la información para que el mensaje que se quiere transmitir con ellos sea comunicado mas eficientemente. Los datos, pueden ser duros es decir los datos originales o resumenes de estos. En los resumenes podemos tener frecuencias acumuladas, porcentajes, tablas, gráficos entre otros. Hay 4 conceptos a tener en cuenta cuando se presentan datos, estos deben ser: Correctos. Todos los datos presentados deben ser verificados previo a cualquier difusión de información Claros. Se debe tener claro el mensaje que se quiere dar con los datos y la presentación de la información debe transmitir claramente este mensaje. Concisos. La presentación debe tener sólo la información necesaria para transmitir el mensaje. Consistentes. La información debe ser consistente en la unidades usadas, asi como abreviaciones y uso de decimales. "],
["manejo-de-números.html", "Capítulo 4 Manejo de números", " Capítulo 4 Manejo de números Uno de los elementos a cuidar en la presentación de datos estadísticos es el manejo de los números. Uso de separador de miles. 128456 vs 128,456 o 128 456 Uso de separador decimal. (consistencia) 66.6, 56.7, 34,7 Nivel de comparación. (usar mismas unidades) Presupuesto de secretaria de salud es de 11,048 millones de pesos, representa un incremento de 563 mil pesos (\\(5.4\\%\\)) respecto al año pasado. El presupuesto de secretaria de salud aumento 0.6 millones (\\(5\\%\\)) siendo 11.0 millones. Justificación de los datos. Usar la misma tipografia, tamaño y número de dígitos. Justificar a la derecha cuando los datos se ponen en columnas, para alinear las unidades. Redondeo. Redondear los datos para que sean comparables al mismo nivel. Redondear para hacerlo memorable y más fácil de comunicar (14,245 a 14,000). "],
["tablas.html", "Capítulo 5 Tablas", " Capítulo 5 Tablas Posicionar los totales por renglón en una última columna, y los totales por columna en un último renglón. Los títulos deben ser reducidos. Centrar los encabezados de las columnas. Cuando la tabla tenga mas de 5 renglones, agregar espaciado cada 5 renglones (es más fácil leer en bloques) Marcar la fuente de donde se tomaron los datos. Datos temporales se ordenan cronológicamente de arriba a abajo o dercha a izquierda. "],
["tabla-de-resumen.html", "Capítulo 6 Tabla de resumen", " Capítulo 6 Tabla de resumen Al realizar una tabla que resume datos es buena idea considerar el mesaje que se quiere transmitir asi como la audiencia a la que va dirigida. Para ello las siguiente preguntas son útiles. ¿Qué números de la tabla se quiere que el lector compare? ¿Cuántos números son necesarios para transmitir el mensaje? ¿Qué tanto detalle es necesario? ¿Se puede agregar información derivada para ayudar a transmitir el mensaje? ¿Deben estar los datos presentados en algún orden? "],
["gráficos.html", "Capítulo 7 Gráficos", " Capítulo 7 Gráficos Cuando se usan gráficos por lo general se busca la comprensión de los datos sólo con el objeto visual sin ayuda de ejes, pies de página o información adicional. Al igual que con las tablas, antes de realizar un gráfico se debe tener claro el mensaje que se desea transmitir. Puntos a considerar: Acomodar los datos para maximizar el uso de texto en forma horizontal. Seleccionar el tipo de gráfico correcto según los datos y el mensaje a transmitir. Evitar colores fuertes primarios (son dominantes para el cebrebro). Cuando se grafican números, siempre empezar el eje “y” en cero. Usar la misma escala en gráficos que serán comparados. Considerar tranformar datos en diferentes escalas antes de graficarlos. Asegurarse que las escalas tengan el formato apropiado. (Redondeo, decimales, etc) Sólo usar gráficos en dos dimensiones para datos bidimensionales. "],
["gráfico-circular.html", "Capítulo 8 Gráfico circular", " Capítulo 8 Gráfico circular En particular para los gráficos circulares se recomienda: Asegurarse que los porcentajes sumen \\(100\\%\\) Iniciar con el primer segmento a las 12 del día e ir llenando en sentido de las manecillas del reloj. Cuando se pueda, ordenar los segmentos por tamaño de mayor a menor. Evitar pies de mas de 6 segmentos. Usar tonos del mismo color para identificar proporciones de una variable. "],
["notación.html", "Capítulo 9 Notación", " Capítulo 9 Notación A lo largo del libro usaremos la notación típica de estadística pero también haremos uso de la siguiente: \\(x^{(i)}\\): el conjunto de inputs (variables explicativas) \\(y^{(i)}\\): es la variable de output/salida (variable dependiente) que queremos predecir (ajustar) A la pareja \\((x^{(i)},y^{(i)})\\) le llamaremos ejemplo de entrenamiento El conjunto de entrenamiento se denota por: \\(\\{(x^{(i)},y^{(i)})|i\\in N\\}\\) De forma general, denotaremos por \\(\\mathcal{X}\\) al espacio de inputs y por \\(\\mathcal{Y}\\) al espacio de outputs N.B. Omitiremos el uso de indices en donde sea claro a qué nos referimos. "],
["glosario-dscml-estadística.html", "Capítulo 10 Glosario DSc/ML - Estadística", " Capítulo 10 Glosario DSc/ML - Estadística Machine Learning / Ciencia de Datos Estadística red, grafo (network, graphs) modelo (model) pesos (weigths) parámetros (parameters) aprendizaje (learning) ajuste (fiting) prueba, generalización (testing, generalization) ajuste en el conjunto de prueba aprendizaje supervisado (supervised learning) regresión, clasificación aprendizaje no supervisado (unsupervised learning) estimación de densidades, clusterización "],
["entrenamiento-de-modelos.html", "Capítulo 11 Entrenamiento de modelos", " Capítulo 11 Entrenamiento de modelos Se le denomina de esta manera a la acción de ajustar el mejor modelo a los datos. Formalmente se define como sigue: Dado un conjunto de entrenamiento \\((x^{(i)},y^{(i)})\\in(\\mathcal{X} \\times \\mathcal{Y})\\) el objetivo es aprender (ajustar) una función \\(h:\\mathcal{X}\\rightarrow \\mathcal{Y}\\) tal que \\(h(x)\\) sea un buen predictor de \\(y\\). La función \\(h\\) suele llamarse hipótesis. Cuando el conjunto \\(\\mathcal{Y}\\) es continuo, estamos frente a un problema de regresión. Si se trata de un conjunto discreto entonces tenemos un problema de clasificación. "],
["regresión-lineal.html", "Capítulo 12 Regresión Lineal 12.1 Un poco de história 12.2 Objetivos del análisis de regresión 12.3 El algorítmo de regresión lineal 12.4 Regresión lineal simple 12.5 Solución al problema de regresión lineal simple 12.6 Regresión lineal múltiple 12.7 Solución al problema de regresión lineal múltiple. 12.8 Aplicación en R", " Capítulo 12 Regresión Lineal 12.1 Un poco de história Los primeros problemas prácticos tipo regresión iniciaron en el siglo XVIII, relacionados con la navegación basada en la Astronomía. Legendre desarrolló el método de mínimo cuadrados en 1805. Gauss afirma que él desarrolló este método algunos años antes y demuestra, en 1809, que mínimos cuadrados proporciona una solución óptima cuando los errores se distribuyen normal. Francis Galton acuña el término regresión al utilizar el modelo para explicar el fenómeno de que los hijos de padres altos, tienden a ser altos en su generación, pero no tan altos como lo fueron sus padres en la propia, por lo que hay un efecto de regresión. El modelo de regresión lineal es, probablemente, el modelo de su tipo más conocido en estadística. El modelo de regresión se usa para explicar o modelar la relación entre una sola variable, \\(y\\), llamada dependiente o respuesta, y una o más variables predictoras, independientes, covariables, o explicativas, \\(x_1, x_2, ..., x_p\\). Si \\(p = 1\\), se trata de un modelo de regresión simple y si \\(p &gt; 1\\), de un modelo de regresión múltiple. En este modelo se asume que la variable de respuesta, \\(y\\), es aleatoria y las variables explicativas son fijas, es decir, no aleatorias. La variable de respuesta debe ser continua, pero los regresores pueden tener cualquier escala de medición. 12.2 Objetivos del análisis de regresión Existen varios objetivos dentro del análisis de regresión, entre otros: Determinar el efecto, o relación, entre las variables explicativas y la respuesta. Predicción de una observación futura. Describir de manera general la estructura de los datos. 12.3 El algorítmo de regresión lineal Sea \\(\\Phi: \\mathcal{X} \\rightarrow \\mathbb{R}^N\\) y consideremos la familia de hipótesis lineales \\[H=\\{x\\mapsto w \\cdot \\Phi(x)+b | w\\in\\mathbb{R}^N, b\\in\\mathbb{R}\\}\\] La regresión lineal consiste en buscar la hipótesis \\(h\\in H\\) con el menor error cuadrático medio, es decir, se debe resolver el problema de optimización: \\[\\min \\frac{1}{m}\\sum_{i=1}^{m}(h(x_i)-y_i)^2\\] 12.4 Regresión lineal simple Para este modelo supondremos que nuestra respuesta, \\(y\\), es explicada únicamente por una covariable, \\(x\\). Entonces, escribimos nuestro modelo como: \\[y^{(i)}=\\beta_0+\\beta_1x^{(i)}+\\epsilon^{(i)},\\ \\ i=1,2,\\dots,n\\] Como podemos observar, se ha propuesto una relación lineal entre la variable \\(y\\) y la variable explicativa \\(x\\), que es nuestro primer supuesto sobre el modelo: La relación funcional entre \\(x\\) y \\(y\\) es una línea recta. Observamos que la relación no es perfecta, ya que se agrega el término de error, \\(\\epsilon\\). Dado que la parte aleatoria del modelo es la variable \\(y\\), asumimos que al error se le “cargan” los errores de medición de \\(y\\), así como las perturbaciones que le pudieran ocasionar los términos omitidos en el modelo. Gauss desarrolló este modelo a partir de la teoría de errores de medición, que es de donde se desprenden los supuestos sobre este término: \\(\\mathbb{E}(\\epsilon^{(i)})=0\\) \\(\\mathbb{V}ar(\\epsilon^{(i)})=\\sigma^2\\) \\(\\mathbb{C}ov(\\epsilon^{(i)},\\epsilon^{(j)})=0, \\ \\forall i\\neq j\\) N.B. Los errores \\(\\epsilon^{(i)}\\) son variables aleatorias no observables. 12.5 Solución al problema de regresión lineal simple 12.5.1 Mínimos cuadrados ordinarios En una situación real, tenemos \\(n\\) observaciones de la variable de respuesta así como de la variable explicativa, que conforman las parejas de entrenamiento \\((x_i, y_i), \\ i = 1, 2, ..., n\\). Entonces, nuestro objetivo será encontrar la recta que mejor ajuste a los datos observados. Utilizaremos el método de mínimos cuadrados para estimar los parámetros del modelo, que consiste en minimizar la suma de los errores al cuadrado, esto es: \\[\\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n(y_i-(\\beta_0+\\beta_1x^{(i)}))^2\\] Al minimizar la expresión anteriore obtenemos las siguientes expresiones para los estimadores: \\[\\hat{\\beta_1}=\\frac{\\sum_{i=1}^ny_i(x_i-\\bar{x})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\] \\[\\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x}\\] Una desventaja del método de mínimos cuadrados, es que no se pueden hacer procesos de inferencia sobre los parámetros de interés \\(\\beta_0\\) y \\(\\beta_1\\); procesos como intervalos de confianza o pruebas de hipótesis. Para subsanar esta deficiencia, es necesario asumir una distribución para el error, \\(\\epsilon_i\\), que, siguiendo la teoría general de errores, se asume que tiene distribución normal, con media cero y varianza \\(\\sigma^2\\). Este supuesto garantiza que las distribuciones de \\(y_i,\\ \\hat{\\beta_0},\\ \\hat{\\beta_1}\\) sean normales, lo que permite tanto la construcción de intervalos de confianza como de pruebas de hipótesis. N.B El estimador de \\(\\sigma^2\\) está dado por \\(\\hat{\\sigma^2}=\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{n-2}\\) 12.5.1.1 Pruebas de hipótesis En el modelo de regresión lineal simple, la prueba de hipótesis más importante es determinar si estadísticamente existe la dependencia líneal entre \\(x\\) y \\(y\\), y que no sea producto del muestreo (debido al azar). Es decir, realizar la prueba de hipótesis: \\[H_0:\\beta_1=0 \\ vs.\\ H_a:\\beta_1\\neq 0\\] No rechazar la hipótesis nula, implicaría que la variable \\(x\\) no ayuda a explicar a \\(y\\) o bien que, tal vez, la relación entre estas variables no es lineal. En este modelo, esta última explicación es un poco cuestionable, ya que se parte, de inicio, del diagrama de dispersión de los datos. Si rechazamos la hipótesis nula, implicará que \\(x\\) es importante para explicar la respuesta \\(y\\) y que la relación lineal entre ellas puede ser adecuada. Rechazar esta hipótesis nula, también podría implicar que existe una relación lineal entre las variables pero, tal vez, se pueda mejorar el ajuste con algún otro término no lineal. 12.5.1.2 Interpretación de los parámetros Cuando se tiene una recta en el sentido determinista, los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) tienen una interpretación muy clara; \\(\\beta_0\\) se interpreta como el valor de \\(y\\) cuando \\(x\\) es igual a cero y \\(\\beta_1\\) como el cambio que experimenta la variable de respuesta \\(y\\) por unidad de cambio en \\(x\\). La interpretación, desde el punto de vista estadístico, de los parámetros estimados en el modelo de regresión es muy similar: \\(\\hat{\\beta_0}\\) es el promedio esperado de la respuesta \\(y\\) cuando \\(x = 0\\) (este parámetro tendrá una interpretación dentro del modelo, si tiene sentido que \\(x\\) tome el valor cero, de lo contrario, no tiene una interpretación razonable) y \\(\\hat{\\beta_1}}\\) es el cambio promedio o cambio esperado en \\(y\\) por unidad de cambio en \\(x\\). 12.6 Regresión lineal múltiple La mayoría de los fenómenos reales son multicausales, por esta razón, un modelo de regresión más acorde a estudios reales es el modelo de regresión lineal múltiple, que es la generalización del modelo simple. En este modelo supondremos que la variable de respuesta, \\(y\\), puede explicarse a través de una colección de \\(k\\) covariables \\(x_1,\\dots,x_k\\). El modelo se escribe de la siguiente manera: \\[y_i = \\beta_0+\\beta_1 x_1^{(i)}+\\beta_2 x_2^{(i)}+\\dots++\\beta_k x_k^{(i)}+\\epsilon_i\\] Al igual que en el caso simple, los parámetros del modelo se pueden estimar por mínimos cuadrados, con el inconveniente de que no se pueden realizar inferencias sobre ellos. Nuevamente, para poder hacer intervalos de confianza y pruebas de hipótesis sobre los verdaderos parámetros hay que suponer que el vector de errores se distribuye normal, en este caso multivariada, es decir: \\[\\epsilon\\sim N_n(0,\\sigma^2\\mathbb{I})\\] Esta estructura del error permite tener las mismas propiedades distribucionales que en regresión simple, es decir, \\(y_i\\) se distribuye normal y \\(\\beta_i\\) tiene distribución normal, facilitando las inferencias sobre cada parámetro y la construcción de intervalos de predicción para las \\(y\\)’s. 12.7 Solución al problema de regresión lineal múltiple. 12.7.1 Ecuaciones normales Las expresiones para estimar los parámetros involucrados en el modelo son: \\[\\hat{\\beta}=(X^TX)^{-1}X^Ty\\] \\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{n-p}\\] donde \\(p=k+1\\) es el número total de parámetros en el modelo. Tanto en el modelo simple como en el múltiple, la variación total de las \\(y\\)’s se puede descomponer en una parte que explica el modelo, i.e., los \\(k\\) regresores o variables explicativas y otra no explicada por estas variables, llamada error. \\[\\sum_{i=1}^n(y_i-\\bar{y})^2=\\sum_{i=1}^n(\\hat{y_i}-\\bar{y})^2+\\sum_{i=1}^n(\\hat{y_i}-y_i)^2\\] #### Pruebas de hipótesis La descomposición anterior ayuda para realizar la importante prueba de hipótesis: \\[H_0:\\beta_1=\\beta_2=\\dots=\\beta_k=0\\ vs.\\ H_a:\\beta_i\\neq0 \\ p.a. \\ i\\] misma que se realiza a través del cociente entre los errores cuadráticos medios: \\[F_0=\\frac{SS_R/k}{SS_E/(n-k-1)}=\\frac{MS_R}{MS_E}\\sim F_{(k,n-k-1)}\\] Esta estadística se desprende de la tabla de análisis de varianza, que es muy similar a la tabla ANOVA que se utiliza para hacer pruebas de hipótesis. En este caso la tabla es: Fuente de variación Grados de libertad Suma de cuadrados Cuadrados medios F Regresión k \\(SS_R\\) \\(MS_R=SS_R/k\\) Error n-k-1 \\(SS_E\\) \\(MS_E=SS_E/(n-k-1)\\) \\(F=\\frac{MS_R}{MS_E}\\) Total n-1 \\(S_{yy}\\) Por lo general, esta estadística rechaza la hipótesis nula, ya que de lo contrario, implicaría que ninguna de las variables contribuye a explicar la respuesta, \\(y\\). Como se puede observar en la hipótesis alternativa, el rechazar \\(H_0\\) solo implica que al menos uno de los regresores contribuye significativamente a explicar \\(y\\). Asimismo, el rechazar \\(H_0\\) no implica que todos contribuyan ni tampoco dice cuál o cuáles contribuyen, por esta razón, una salida estándar de regresión múltiple tiene pruebas individuales sobre la significancia de cada regresor en el modelo. El estadístico para hacer tanto los contrastes de hipótesis como los intervalos de confianza individuales, es: \\[t=\\frac{\\hat{\\beta_i}-\\beta_0^{(i)}}{\\sqrt{\\hat{\\mathbb{V}ar}(\\hat{\\beta_i})}}\\sim t_{(n-p)}\\] Podemos apreciar que los constrastes de hipótesis se pueden hacer contra cualquier valor particular del parámetro \\(\\beta_0^{(i)}\\), en general. No obstante, en las pruebas estándar sobre los parámetros de un modelo, este valor particular es 0, ya que se intenta determinar si la variable asociada al \\(i\\)-ésimo parámetro es estadísticamente significativa para explicar la respuesta. Por lo que el estadístico para este caso es: \\[t=\\frac{\\hat{\\beta_i}}{\\sqrt{\\hat{\\mathbb{V}ar}(\\hat{\\beta_i})}}\\sim t_{(n-p)}\\] De este estadístico se desprenden también los intervalos de confianza para cada parámetro: \\[\\beta_i\\in(\\hat{\\beta_i}\\pm t_{(n-p,1-\\alpha/2)} \\sqrt{\\hat{\\mathbb{V}ar} (\\hat{\\beta_i})})\\] #### Interpretación de los parámetros La interpretación de cada parámetro es similar a la del coeficiente de regresión \\(\\hat{\\beta_1}\\) en el modelo simple, anexando la frase: “manteniendo constantes el resto de las variables”. Esto es, \\(\\hat{\\beta_i}}\\) es el cambio promedio o cambio esperado en \\(y\\) por unidad de cambio en \\(x_i\\), sin considerar cambio alguno en ninguna de las otras variables dentro del modelo, es decir, suponiendo que estas otras variables permanecen fijas. Esta interpretación es similar a la que se hace de la derivada parcial en un modelo determinista. Nuevamente, la interpretación de \\(\\hat{\\beta_0}\\) estará sujeta a la posibilidad de que, en este caso, todas las variables puedan tomar el valor cero. 12.7.1.1 Predicción de nuevos valores Uno de los usos más frecuentes del modelo de regresión es el de predecir un valor de la respuesta para un valor particular de las covariables en el modelo. Si la predicción se realiza para un valor de las covariables dentro del rango de observación de las mismas, se tratará de una interpolación, y si se realiza para un valor fuera de este rango, hablaremos de una extrapolación. En cualquiera de los dos casos, estaremos interesados en dos tipos de predicciones: Predicción de la respuesta media: \\(y_0=\\mathbb{E}(y|X_0)\\) Predicción de una nueva observación: \\(y_0\\) En ambos casos, la estimación puntual es la misma: \\(\\hat{y_0}=X_0^T\\hat{\\beta}\\) Lo que difiere es el intervalo de predicción. Para la respuesta media es: \\(y_0=(\\hat{y_0}\\pm t_{(n-p,1-\\alpha/2)}\\sqrt{\\hat{\\sigma^2}X_0^T(X^TX)^{-1}X_0})\\) Y para predecir una observación: \\(y_0=(\\hat{y_0}\\pm t_{(n-p,1-\\alpha/2)}\\sqrt{\\hat{\\sigma^2}(1+X_0^T(X^TX)^{-1}X_0)})\\) 12.7.1.2 Coeficiente de determinación Un primer elemento de juicio sobre el modelo de regresión lo constituye el coeficiente de determinación \\(R^2\\), que es la proporción de variabilidad de las \\(y\\)’s que es explicada por las \\(x\\)’s y que se escribe como: \\[R^2=\\frac{SS_R}{S_{yy}}=1-\\frac{SS_E}{S_{yy}}\\] Una \\(R^2\\) cercana a uno implicaría que mucha de la variabilidad de la respuesta es explicada por el conjunto de regresores incluidos en el modelo. Es deseable tener una \\(R^2\\) grande en nuestro modelo, pero esto no significa, como mucha gente piensa, que ya el modelo está bien ajustado. 12.7.2 Evaluación de supuestos Los dos modelos de regresión presentados, el simple y el múltiple, se construyeron sobre los supuestos de: La relación funcional entre la variable de respuesta \\(y\\) y cada regresor \\(x_i\\) es lineal La esperanza de los errores es cero, \\(\\mathbb{E}(\\epsilon_i=0)\\) La varianza de los errores es constante, \\(\\mathbb{V}ar(\\epsilon_i) = \\sigma^2\\) Los errores no están correlacionados, \\(\\mathbb{C}ov(\\epsilon_i, \\epsilon_j) = 0;\\ i\\neq j\\) Los errores tienen distribución normal con media cero y varianza \\(\\sigma^2\\) Entonces, para garantizar que el modelo es adecuado, es indispensable verificar estos supuestos. 12.7.2.1 Residuos Los elementos más importantes para verificar estos supuestos son los residuos, definidos como: \\[e_i=y_i-\\hat{y}_i\\] Estos residuos representan la discrepancia entre la respuesta predicha por el modelo ajustado, \\(\\hat{y}_i\\) y el correspondiente valor observado, \\(y_i\\). En la literatura de regresi ́on lineal existen cuatro tipos de residuos, a saber Residuo crudo: \\(e_i\\) Residuo estandarizado: \\(d_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma}^2}}\\) Residuo estudentizado interno: \\(r_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma}^2(1-h_{ii}})}\\) Residuo estudentizado externo: \\(t_i=\\frac{e_i}{\\sqrt{\\hat{\\sigma_{(-i)}}^2(1-h_{ii})}}\\) Estos residuos se utilizan en los distintos procedimientos para evaluar los supuestos y lo adecuado del ajuste del modelo. La mayoría de las pruebas conocidas para la verificación de los supuestos, son pruebas gráficas. Indudablemente, la prueba más importante es sobre la normalidad de los errores, ya que sobre este supuesto descansan todas la inferencias de este modelo. La manera de verificarlo es a través de la gráfica conocida como QQ-plot o QQ-norm, que grafica los cuantiles teóricos de una distribución normal (eje x) vs. los cuantiles asociados a los residuos. Entonces, si los residuos realmente provienen de una normal, la gráfica debe mostrar la función identidad. Fuertes desviaciones de esta línea darían evidencia de que los errores no se distribuyen normal. 12.7.2.2 Linealidad de los predictores La manera estándar de evaluar la linealidad de las variables explicativas es a través de la gráfica de cada una de ellas contra los residuos. Si la variable en cuestión ingresa al modelo de manera lineal, esta gráfica debe mostrar un patrón totalmente aleatorio entre los puntos dispuestos en ella. Cuando la variable explicativa es politómica, este tipo de gráficas son poco ilustrativas en este sentido. 12.7.2.3 Supuestos sobre los errores Si la gráfica entre los valores ajustados y los residuos estandarizados, muestra un patrón aleatorio, es simétrica alrededor del cero y los puntos están comprendidos entre los valores -2 y 2, entonces se tendrá evidencia de que los errores tienen media cero, varianza constante y no están correlacionados. Los métodos mostrados hasta ahora, permiten evaluar el modelo de manera global y no por cada observación dentro del mismo. Dado que una observación puede resultar determinante sobre alguna(s) característica(s) del modelo, es conveniente verificar el impacto que cada observación pueda tener en los distintos aspectos del modelo. Las estadísticas para evaluar el impacto que tiene una observación sobre todo el vector de parámetros, alguno de los regresores y sobre los valores predichos, se basan en la misma idea, que consiste en cuantificar el cambio en la característica de interés con y sin la observación que se está evaluando. 12.7.2.3.1 Puntos palanca Antes de presentar las estadísticas que servirán para hacer este diagnóstico, introduciremos un elemento que es común a ellas: la llamada palanca (leverage) de una observación. Recordemos que el ajuste del modelo se expresaba como: \\[\\hat{\\beta}=(X^TX)^{-1}X^Ty \\Rightarrow \\hat{y}=X\\hat{\\beta}=Hy\\] Con \\(H\\) conocida como la matriz sombrero. Un resultado fundamental sobre esta matriz sombrero es: \\[\\mathbb{V}ar(e)=(I-H)\\sigma^2 \\Rightarrow \\mathbb{V}ar(e_i)=(1-h_i)\\sigma^2\\] Con \\(h_i\\) el i-ésimo elemento de la diagonal de la matriz \\(H\\). Observemos que esta palanca sólo depende de \\(X\\), entonces, una observación con una palanca, \\(h_i\\), grande, es aquella con valores extremos en alguna(s) de su(s) covariable(s). Ya que el promedio de las \\(h_i&#39;s\\) es \\(p/n\\), consideraremos una observación con palanca grande si su palanca es mayor a \\(2p/n\\). En este sentido, \\(h_i\\) corresponde a la distancia de Mahalanobis de \\(X\\) definida como \\((X-\\bar{X})^T\\hat{\\Sigma}^{-1}(X-\\bar{X})\\). La dependencia de las estadísticas para el diagnóstico de las observaciones, estriba en que sus cálculos dependen de los valores de la palanca de cada individuo. Estas estadísticas son: Distancia de Cook Dfbetas Dffits Distancia de Cook: Sirve para determinar si una observación es influyente en todo el vector de parámetros. Una observación se considera influyente, si su distancia de Cook sobrepasa el valor uno. ## List of 93 ## $ line :List of 6 ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : num 1 ## ..$ lineend : chr &quot;butt&quot; ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ rect :List of 5 ## ..$ fill : chr &quot;white&quot; ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : num 1 ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_rect&quot; &quot;element&quot; ## $ text :List of 11 ## ..$ family : chr &quot;&quot; ## ..$ face : chr &quot;plain&quot; ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 11 ## ..$ hjust : num 0.5 ## ..$ vjust : num 0.5 ## ..$ angle : num 0 ## ..$ lineheight : num 0.9 ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ title : NULL ## $ aspect.ratio : NULL ## $ axis.title : NULL ## $ axis.title.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 2.75points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 2.75points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x.bottom : NULL ## $ axis.title.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : num 90 ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 2.75points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.y.left : NULL ## $ axis.title.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 2.75points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : chr &quot;grey30&quot; ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 2.2points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 2.2points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x.bottom : NULL ## $ axis.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 1 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 2.2points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.y.left : NULL ## $ axis.text.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 2.2points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.ticks : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ axis.ticks.x : NULL ## $ axis.ticks.x.top : NULL ## $ axis.ticks.x.bottom : NULL ## $ axis.ticks.y : NULL ## $ axis.ticks.y.left : NULL ## $ axis.ticks.y.right : NULL ## $ axis.ticks.length : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ axis.ticks.length.x : NULL ## $ axis.ticks.length.x.top : NULL ## $ axis.ticks.length.x.bottom: NULL ## $ axis.ticks.length.y : NULL ## $ axis.ticks.length.y.left : NULL ## $ axis.ticks.length.y.right : NULL ## $ axis.line : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ axis.line.x : NULL ## $ axis.line.x.top : NULL ## $ axis.line.x.bottom : NULL ## $ axis.line.y : NULL ## $ axis.line.y.left : NULL ## $ axis.line.y.right : NULL ## $ legend.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ legend.spacing : &#39;simpleUnit&#39; num 11points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ legend.spacing.x : NULL ## $ legend.spacing.y : NULL ## $ legend.key : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.key.size : &#39;simpleUnit&#39; num 1.2lines ## ..- attr(*, &quot;unit&quot;)= int 3 ## $ legend.key.height : NULL ## $ legend.key.width : NULL ## $ legend.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.text.align : NULL ## $ legend.title :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.title.align : NULL ## $ legend.position : chr &quot;right&quot; ## $ legend.direction : NULL ## $ legend.justification : chr &quot;center&quot; ## $ legend.box : NULL ## $ legend.box.just : NULL ## $ legend.box.margin : &#39;margin&#39; num [1:4] 0cm 0cm 0cm 0cm ## ..- attr(*, &quot;unit&quot;)= int 1 ## $ legend.box.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.box.spacing : &#39;simpleUnit&#39; num 11points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ panel.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.border : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.spacing : &#39;simpleUnit&#39; num 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ panel.spacing.x : NULL ## $ panel.spacing.y : NULL ## $ panel.grid :List of 6 ## ..$ colour : chr &quot;grey92&quot; ## ..$ size : NULL ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.grid.major : NULL ## $ panel.grid.minor :List of 6 ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.5 ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.grid.major.x : NULL ## $ panel.grid.major.y : NULL ## $ panel.grid.minor.x : NULL ## $ panel.grid.minor.y : NULL ## $ panel.ontop : logi FALSE ## $ plot.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ plot.title :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 1.2 ## ..$ hjust : num 0 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 5.5points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.title.position : chr &quot;panel&quot; ## $ plot.subtitle :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 5.5points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.caption :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : num 1 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 5.5points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.caption.position : chr &quot;panel&quot; ## $ plot.tag :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 1.2 ## ..$ hjust : num 0.5 ## ..$ vjust : num 0.5 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.tag.position : chr &quot;topleft&quot; ## $ plot.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ strip.background.x : NULL ## $ strip.background.y : NULL ## $ strip.placement : chr &quot;inside&quot; ## $ strip.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : chr &quot;grey10&quot; ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 4.4points 4.4points 4.4points 4.4points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.text.x : NULL ## $ strip.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.switch.pad.grid : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.switch.pad.wrap : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.text.y.left :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num 90 ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi TRUE ## - attr(*, &quot;validate&quot;)= logi TRUE Dfbetas: Sirven para determinar si una observación es influyente en alguno de los coeficientes de regresión. Hay un dfbeta por cada parámetro dentro del modelo, incluido, por supuesto, el de la ordenada al origen. La regla de dedo es que la observación \\(i\\) es influyente en el j-ésimo coeficiente de regresión si: \\[|Dfbetas_{j,i}|&gt;\\frac{2}{\\sqrt{n}}\\] Dffits: Se utilizan para determinar si una observación es influyente en la predicción de \\(y\\). Se dice que la i-ésima observación es influyente para predecir \\(y\\), si: \\[|Dffits_i|&gt;2\\sqrt{\\frac{p}{n}}\\] 12.7.2.4 Multicolinealidad El modelo de regresión lineal múltiple, se construye bajo el supuesto de que los regresores son ortogonales, i.e., son independientes. Desafortunadamente, en la mayoría de las aplicaciones el conjunto de regresores no es ortogonal. Algunas veces, esta falta de ortogonalidad no es seria; sin embargo, en algunas otras los regresores están muy cerca de una perfecta relación lineal, en tales casos las inferencias realizadas a través del modelo de regresión lineal pueden ser erróneas. Cuando hay una cercana dependencia lineal entre los regresores, se dice que estamos en presencia de un problema de multicolinealidad. Efectos de la multicolinealidad: Varianzas de los coeficientes estimados son muy grandes. Los estimadores calculados de distintas sub muestras de la misma población, pueden ser muy diferentes. La significancia de algún regresor se puede ver afectada (volverse no significativo) por que su varianza es más grande de lo que debería ser en realidad o por la correlación de la variable con el resto dentro del modelo. Es común que algún signo de un parámetro cambie, haciendo ilógica su interpretación dentro del modelo. 12.7.2.4.1 ¿Cómo detectar multicolinealidad? Matriz de correlación. Examinar las correlaciones entre pares de variables: \\[r_{ij}\\ \\ \\ i, j = 1, 2, \\dots, k\\ \\ i\\neq j\\] Pero, si dos o más regresores están linealmente relacionados, es posible que ninguna de las correlaciones entre cada par de variables, sea grande. Factor de inflación de la varianza. \\[VIF_j=(1-R_j^2)^{-1}\\] Con \\(R_j^2\\) el coeficiente de determinación del modelo de regresión entre el j-ésimo regresor, \\(x_j\\) (tomado como variable de respuesta) y el resto de los regresores \\(x_i\\), \\(i\\neq j\\). Experiencias prácticas indican que si algunos de los VIF’s excede a 10, su coeficiente asociado es pobremente estimado por el modelo debido a multicolinealidad. Análisis del eigensistema. Basado en los eigenvalores de la matriz \\(X^TX\\). Número de condición. \\[K=\\frac{\\lambda_{max}}{\\lambda_{min}}\\] Si el número de condición es menor que 100, no existen problemas serios de multicolinealidad. Si está entre 100 y 1000 existe de moderada a fuerte multicolinealidad y si excede a 1000, hay severa multicolinealidad. Índice de condición. \\[k_j=\\frac{\\lambda{max}}{\\lambda_j}\\] Si el índice de condición es menor que 10, no hay ningún problema. Si está entre 10 y 30, hay moderada multicolinealidad, y si es mayor que 30, existe una fuerte colinealidad en la j-ésima variable en el modelo. N.B. En algunos paquetes estos índices se presentan aplicando la raíz cuadrada a su expresión, entonces hay que extraer raíz a los puntos de corte de los criterios correspondientes. 12.7.2.5 Relación funcional Un supuesto importante en el modelo de regresión es el que considera que debe existir una relación funcional lineal entre cada regresor y la variable de respuestas. Pero, ¿qué debemos hacer si no se cumple esta relación lineal de la respuesta con alguno(s) de los regresor(es)? Primero, ya dijimos que este supuesto se evalúa realizando la gráfica de dispersión entre los residuos del modelo y los valores de la variable en cuestión. Cuando no hay una asociación lineal entre la respuesta y la covariable, generalmente este diagrama de dispersión muestra un patrón (tendencia) que sugiere qué tipo de transformación se debería hacer a la covariable para lograr linealidad con la respuesta. Debe quedar claro que la transformación puede realizarse a la variable explicativa o a la variable de respuesta. A muchos investigadores no les gusta transformar la respuesta porque argumentan que pierden interpretabilidad del modelo. Aunque esto puede ser cierto, existen transformaciones de la respuesta que pueden regresarse para interpretar el modelo con la respuesta original. ## List of 93 ## $ line :List of 6 ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : num 1 ## ..$ lineend : chr &quot;butt&quot; ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ rect :List of 5 ## ..$ fill : chr &quot;white&quot; ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : num 1 ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_rect&quot; &quot;element&quot; ## $ text :List of 11 ## ..$ family : chr &quot;&quot; ## ..$ face : chr &quot;plain&quot; ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 11 ## ..$ hjust : num 0.5 ## ..$ vjust : num 0.5 ## ..$ angle : num 0 ## ..$ lineheight : num 0.9 ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ title : NULL ## $ aspect.ratio : NULL ## $ axis.title : NULL ## $ axis.title.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 2.75points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 2.75points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x.bottom : NULL ## $ axis.title.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : num 90 ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 2.75points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.y.left : NULL ## $ axis.title.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 2.75points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : chr &quot;grey30&quot; ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 2.2points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 2.2points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x.bottom : NULL ## $ axis.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 1 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 2.2points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.y.left : NULL ## $ axis.text.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 2.2points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.ticks : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ axis.ticks.x : NULL ## $ axis.ticks.x.top : NULL ## $ axis.ticks.x.bottom : NULL ## $ axis.ticks.y : NULL ## $ axis.ticks.y.left : NULL ## $ axis.ticks.y.right : NULL ## $ axis.ticks.length : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ axis.ticks.length.x : NULL ## $ axis.ticks.length.x.top : NULL ## $ axis.ticks.length.x.bottom: NULL ## $ axis.ticks.length.y : NULL ## $ axis.ticks.length.y.left : NULL ## $ axis.ticks.length.y.right : NULL ## $ axis.line : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ axis.line.x : NULL ## $ axis.line.x.top : NULL ## $ axis.line.x.bottom : NULL ## $ axis.line.y : NULL ## $ axis.line.y.left : NULL ## $ axis.line.y.right : NULL ## $ legend.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ legend.spacing : &#39;simpleUnit&#39; num 11points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ legend.spacing.x : NULL ## $ legend.spacing.y : NULL ## $ legend.key : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.key.size : &#39;simpleUnit&#39; num 1.2lines ## ..- attr(*, &quot;unit&quot;)= int 3 ## $ legend.key.height : NULL ## $ legend.key.width : NULL ## $ legend.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.text.align : NULL ## $ legend.title :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.title.align : NULL ## $ legend.position : chr &quot;right&quot; ## $ legend.direction : NULL ## $ legend.justification : chr &quot;center&quot; ## $ legend.box : NULL ## $ legend.box.just : NULL ## $ legend.box.margin : &#39;margin&#39; num [1:4] 0cm 0cm 0cm 0cm ## ..- attr(*, &quot;unit&quot;)= int 1 ## $ legend.box.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.box.spacing : &#39;simpleUnit&#39; num 11points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ panel.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.border : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.spacing : &#39;simpleUnit&#39; num 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ panel.spacing.x : NULL ## $ panel.spacing.y : NULL ## $ panel.grid :List of 6 ## ..$ colour : chr &quot;grey92&quot; ## ..$ size : NULL ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.grid.major : NULL ## $ panel.grid.minor :List of 6 ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.5 ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.grid.major.x : NULL ## $ panel.grid.major.y : NULL ## $ panel.grid.minor.x : NULL ## $ panel.grid.minor.y : NULL ## $ panel.ontop : logi FALSE ## $ plot.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ plot.title :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 1.2 ## ..$ hjust : num 0 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 5.5points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.title.position : chr &quot;panel&quot; ## $ plot.subtitle :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 5.5points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.caption :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : num 1 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 5.5points 0points 0points 0points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.caption.position : chr &quot;panel&quot; ## $ plot.tag :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : &#39;rel&#39; num 1.2 ## ..$ hjust : num 0.5 ## ..$ vjust : num 0.5 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.tag.position : chr &quot;topleft&quot; ## $ plot.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ strip.background.x : NULL ## $ strip.background.y : NULL ## $ strip.placement : chr &quot;inside&quot; ## $ strip.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : chr &quot;grey10&quot; ## ..$ size : &#39;rel&#39; num 0.8 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : &#39;margin&#39; num [1:4] 4.4points 4.4points 4.4points 4.4points ## .. ..- attr(*, &quot;unit&quot;)= int 8 ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.text.x : NULL ## $ strip.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.switch.pad.grid : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.switch.pad.wrap : &#39;simpleUnit&#39; num 2.75points ## ..- attr(*, &quot;unit&quot;)= int 8 ## $ strip.text.y.left :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num 90 ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi TRUE ## - attr(*, &quot;validate&quot;)= logi TRUE Un problema asociado a esta identificación por parte del usuario, es que debe tener experiencia para asociar estas formas a una función analítica específica; hecho no necesariamente cierto. Por lo tanto, requiere de alguna herramienta técnica que pudiera auxiliarlo en esta labor. Un buen auxiliar, en el caso de que se crea que es necesario transformar la respuesta, es usar la llamada trasformación Box-Cox. 12.7.2.5.1 Transformación Box-Cox La transformación Box-Cox de la respuesta, es una función que sirve para normalizar la distribución del error, estabilizar la varianza de este error y mejorar la relación lineal entre \\(y\\) y las \\(X’s\\). Se define como: \\[y_i^{\\lambda} = \\left\\{ \\begin{array}{ll} \\frac{y_i^{\\lambda-1}}{\\lambda}, &amp; \\lambda \\neq 0;\\\\ ln(y_i), &amp; \\lambda=0 .\\end{array} \\right.\\] La siguiente tabla muestra el rango de valores de \\(\\lambda\\) que estarían asociados a una transformación analítica común. Rango \\(\\lambda\\) Transformación Asociada (-2.5, -1.5] \\(\\frac{1}{y^2}\\) (-1.5, -0.75] \\(\\frac{1}{y}\\) (-0.75, -0.25] \\(\\frac{1}{sqrt{y}}\\) (-0.25, 0.25] \\(ln(y)\\) (0.25, 0.75] \\(\\sqrt{y}\\) (0.75, 1.25] \\(y\\) (1.25, 2.5) \\(y^2\\) 12.7.2.5.2 Transformación Box-Tidwell Box y Tidwell implementan un proceso iterativo para encontrar la mejor transformación de las variables predictoras en el modelo de regresión lineal. Definiendo como \\(X_j^{\\gamma_j}\\) la correspondiente transformación Box-Tidwell de la variable \\(j\\). La tabla anterior para las transfomaciones analíticas de la respuesta, también aplican para estas transformaciones de los predictores. 12.8 Aplicación en R "],
["modelos-lineales-generalizados.html", "Capítulo 13 Modelos lineales generalizados 13.1 Regresión logística 13.2 Modelo de regresión logísitica simple 13.3 Modelo de regresión logísitica multiple 13.4 Regresión Multinomial 13.5 Modelos para conteos 13.6 GLM binomial negativa 13.7 Exponencial 13.8 Gamma", " Capítulo 13 Modelos lineales generalizados En el capítulo anterior exploramos el modelo básico que nos permite responder a la pregunta: ¿puede ser la variable de interés predicha por un conjunto de variables explicativas? Sin embargo, para poder utilizar dicho modelo, es necesario que la variable respuesta sea continua y cumpla las hipótesis estándar del modelo lineal (datos normales, varianza constante, etc.) Si la variable de interés es, por ejemplo binaria podemos ajustar un modelo de regresión logística en donde lo que predecimos son las probabilidades de la ocurrencia del evento medido con la variable binaria. En 1944, Berkson utilizó por primera vez la regresión logística como una forma de solucionar el problema de explicar una variable dicotómica a través de una variable continua. En este caso, la función logit hace que en lugar de trabajar con valores de la variable respuesta entre \\((0, 1)\\), trabajemos con una variable respuesta que puede tomar cualquier valor. No fue hasta 1972 cuando John Nelder introdujo los modelos lineales generalizados (GLM por sus siglas en inglés), de ahí que en general se considere a la regresión logística como algo distinto a los GLM, cuando lo que ocurre es que tanto la regresión múltiple como la logística, de Poisson, ordinal, etcétera, son casos particulares de un GLM. Para entender lo que es un GLM, volvamos al modelo de regresión múltiple, en este modelos suponemos que: \\[Y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}+\\epsilon\\] \\[E[Y]=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] Es decir, que existe una relación lineal entre las \\(X\\) y \\(E[Y]\\) (el valor medio de Y dado un cierto valor de las variables explicativas). Si las observaciones son binarias, entonces: \\[P(Y = 1) = p\\] \\[P(Y = 0) = 1-p\\] Y \\(E[Y] = 0\\times P(Y = 0) + 1 \\times P(Y = 1) = p\\), por lo tanto un modelo de regresión múltiple relacionará directamente la probabilidad de que ocurra un suceso con las variables explicativas, lo cual no es lo que se busca al ajustar un modelo de regresión lineal. Lo que hacen los GLM es establecer esa relación lineal no entre la media de la variable respuesta y los predictores, sino entre una función de la media de variable respuesta y los predictores, es decir: \\[g(E[Y])=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] Según de qué tipo sea la varible \\(Y\\), así será la función \\(g(\\cdot)\\). Entonces se puede decir que un GLM tiene 3 componentes: Componente aleatorio: La variable respuesta \\(Y\\) . Para poder utilizar un GLM, la distribución de \\(Y\\) ha de pertenecer a la familia exponencial, es decir, su función de densidad ha de poder escribirse como: \\[f(y;\\theta,\\phi)=exp\\{\\frac{y\\theta-b(\\theta)}{a(\\phi)}+c(y,\\phi)\\}\\] donde \\(a(\\cdot)\\), \\(b(\\cdot)\\) y \\(c(\\cdot)\\) son funciones específicas. El parámetro \\(\\theta\\) es lo que se llama parámetro canónico de localización y \\(\\phi\\) es un parámetro de dispersión. Pertenecen a la familia exponecial la distribución Normal, Bernouilli, Binomial, Poisson, Exponecial, Gamma, entre otras. Componente sistemático: Las variables predictoras \\(X_i \\ \\ i = 1,...,k\\) Función liga: La función que relaciona la media, \\(E[Y]\\), con las variables predictoras \\(X\\). En el caso del modelo de regresión ordinaria, \\(\\mu = \\nu\\), por lo tanto la función liga es la identidad. Hay muchas opciones par la función liga. La función liga canonica es una función que transforma la media en el parámetro canónico \\(\\theta\\) \\[g(E[Y])=\\theta\\] Entoces \\(g(\\cdot)\\) es la función liga canónica. La siguiente tabla muestra las funciones link canónicas para las distribuciones más comunes usadas en los GLMs: Distribución Liga canónica Normal \\(X \\beta = E[Y]\\) (identidad) Binomial \\(X \\beta = ln(\\frac{P}{1-P})\\) (logística) Poisson \\(X \\beta = ln(E[Y])\\) (logarítmica) Exponencial \\(X \\beta = \\frac{1}{E[Y]})\\) (recíproca) Gamma \\(X \\beta = \\frac{1}{E[Y]})\\) (recíproca) La diferencia que hay entre usar la función liga y usar una transformación, es que la función liga transforma la media, \\(E[Y]\\), y no los datos, \\(Y\\). Los GLM generalizan la regresión ordinaria de dos modos: permitiendo que la variable de respuesta \\(Y\\) tenga distribuciones diferentes a la normal y, por otro lado, incluyendo distintas funciones liga de la media, lo cual resulta muy útil para datos categóricos. 13.1 Regresión logística El modelo de regresión logistica es un GLM donde la distribución de probabilidad es Bernoulli o Binomial, y la función liga es el logit (ya que relaciona a la media, que en una Bernouilli es la probabilidad con el predictor lineal). Por lo tanto la estimación de los parámetros y los contrastes de hipótesis utilizan la teoría desarrollada para los GLMs. Estos modelos se utilizan cuando se desea conocer la relación entre Una variable dependiente cualitativa, dicotómica. Una o más variables explicativas independientes, llamadas covariables ya sean cualitativas o cuantitativas Por tanto, el objetivo de la regresión logística no es, como en regresión lineal, predecir el valor de la variable \\(Y\\) a partir de una o varias variables predictoras, sino que queremos predecir la probabilidad de que ocurra \\(Y\\) conocidos los valores de las variables \\(X_i&#39;s\\). Recordemos que las covariables cualitativas deben transformarse en las covariables cualitativas dicotómicas ficticias necesarias (variables dummy). De manera que al hacer esta transformación cada categoría de la variable entrará en el modelo de forma individual. 13.2 Modelo de regresión logísitica simple Para este modelo supondremos que nuestra respuesta, \\(Y\\), es explicada únicamente por una covariable, \\(X\\). Asumimos que la variable independiente \\(Y\\) está codificada como un 0 o un 1. Entonces, escribimos nuestro modelo como: \\[ln(\\frac{p}{1-p})=\\beta_{0}+\\beta_{1}X\\] \\[\\frac{p}{1-p}=e^{\\beta_{0}+\\beta_{1}X}\\] \\[p=e^{\\beta_{0}+\\beta_{1}X}-p\\times e^{\\beta_{0}+\\beta_{1}X}\\] \\[p(1+e^{\\beta_{0}+\\beta_{1}X})=e^{\\beta_{0}+\\beta_{1}X}\\] \\[p=\\frac{e^{\\beta_{0}+\\beta_{1}X}}{1+e^{\\beta_{0}+\\beta_{1}X}}\\] Y: \\[1-p=\\frac{1}{1+e^{\\beta_{0}+\\beta_{1}X}}\\] Los valores posibles de estas ecuaciones varían entre 0 y 1. Un valor cercano a 0 significa que es muy improbable que \\(Y\\) haya ocurrido, y un valor cercano a 1 significa que es muy probable que tuviese lugar. Similar a regresión lineal los valores de los parámetros se estiman utilizando el método de máxima verosimilitud que selecciona los coeficientes que hacen más probable que los valores observados ocurran. Para este análisis tenemos la razón de momios (odds ratio), que corresponde a la razón entre las posibilidades de respuesta. \\[OR=\\frac{\\frac{P(Y=1|X=1)}{1-P(Y=1|X=1)}}{\\frac{P(Y=1|X=0)}{1-P(Y=1|X=0)}}\\] El valor nulo para la razón de momios es el 1. Un \\(OR = 1\\) implica que las dos categorías comparadas son iguales. El valor mínimo posible es 0 y el máximo teóricamente posible es infinito. Un OR inferior a la unidad se interpreta como que el desenlace es menos frecuente en la categoría o grupo que se ha elegido como de interés con respecto al otro grupo o categoría de referencia. Un OR = 3 se interpreta como una ventaja 3 veces superior de una de las categorías \\(X = 1\\) relativamente a la otra categoría \\(X=0\\). 13.3 Modelo de regresión logísitica multiple Análogo a lo que observamos en los modelos de regresión lineal, el modelo de regresión logística se puede facilmente generalizar de un modelo simple a un múltiple. \\[ln(\\frac{p}{1-p})=\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_{2}+...\\beta_{k}X_{k}\\] \\[p=P(Y)=\\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_{2}+...\\beta_{k}X_{k})}}\\] De nuevo los valores posibles de estas ecuaciones varían entre 0 y 1. El propósito del análisis es predecir la probabilidad de que un evento \\(Y\\) ocurra para el \\(i-eismo\\) individuo. Para dicha \\(i-ésima\\) persona, \\(Y\\) será 0 (la respuesta no ocurre) o 1 (la respuesta ocurre), y el valor predicho, \\(\\mathbb{P}(Y)\\), tendrá un valor 0 (no hay probabilidad de que el resultado ocurra) o 1 (el resultado seguro que ocurre). 13.3.1 Regresión logística simple en R En el siguiente ejercicio se busca analizar si los productos salen o no defectuosos de acuerdo de la temperatura de la máquina que los produce. Los datos son los siguientes: temperatura &lt;-c(66,70,69,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58) defecto &lt;-c( 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1) Con ambos vectores construimos un dataframe: datos &lt;- data.frame(temperatura = temperatura, defecto = defecto) rm(temperatura, defecto) Resumen visual de los datos: colores &lt;- NULL colores[datos$defecto == 0] &lt;- &quot;green&quot; colores[datos$defecto == 1] &lt;- &quot;red&quot; plot(datos$temperatura, datos$defecto, pch = 21, bg = colores, xlab = &quot;Temperatura&quot;, ylab = &quot;Prob.defecto&quot;) legend(&quot;bottomleft&quot;, c(&quot;No defecto&quot;, &quot;Si defecto&quot;), pch = 21, col = c(&quot;green&quot;, &quot;red&quot;)) Creamos el modelo de regresión logística (modelo de regresión lineal generalizado y parametrizamos por familia binomial). reg &lt;- glm(defecto ~ temperatura, data = datos, family = binomial) Tabla resumen: summary(reg) ## ## Call: ## glm(formula = defecto ~ temperatura, family = binomial, data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.84513 -0.38010 -0.09632 -0.02831 2.41364 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 32.3381 17.6301 1.834 0.0666 . ## temperatura -0.5028 0.2643 -1.902 0.0571 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 24.0850 on 22 degrees of freedom ## Residual deviance: 9.8032 on 21 degrees of freedom ## AIC: 13.803 ## ## Number of Fisher Scoring iterations: 7 Creamos una nueva variable al dataframe de nuestros datos con las probabilidades de pertenencia a la clase 1 predichas por el modelo. datos$predict&lt;-reg$fitted.values Dibujamos la recta de probabilidad para cada una de las temperaturas: datos_probab &lt;- data.frame(temperatura = seq(50, 85, 0.1)) datos.predict &lt;- predict(reg, datos_probab, type = &quot;response&quot;) plot(datos$temperatura, datos$defecto, pch = 21, bg = colores, xlab = &quot;Temperatura&quot;, ylab = &quot;Prob.defecto&quot;) legend(&quot;bottomleft&quot;, c(&quot;No defecto&quot;, &quot;Si defecto&quot;), pch = 21, col = c(&quot;green&quot;, &quot;red&quot;)) lines(datos_probab$temperatura, datos.predict, col = &quot;blue&quot;, lwd = 2) Bondad de Ajuste: dev &lt;- reg$deviance nullDev &lt;- reg$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 14.28173 chidf &lt;- reg$df.null - reg$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0.0001573848 chisq.prob es el p-value de la estadística “modelChi”, para valores pequeños se dice que el modelo es estadisticamente significativo. 13.3.2 Ejercicio. Se tiene la siguiente tabla donde se eligen varios niveles de ronquidos y se ponen en relación con una enfermedad cardíaca. Se toman como puntuaciones relativas de ronquidos los valores \\(\\{0, 2, 4, 5\\}\\). Ronquido Presencia de enfermedad cardiaca Ausencia de enfermedad cardiaca Nunca 24 1355 Ocasional 35 603 Casi cada noche 21 192 Cada noche 30 224 Fijamos los niveles de manera ordinal: roncas &lt;- c(0, 2, 4, 5) frecuencia &lt;- cbind (SI=c(24 , 35, 21, 30) , NO=c (1355 ,603 , 192 , 224)) logit.irls &lt;- glm( frecuencia~roncas , family = binomial ( link = logit )) summary ( logit.irls )$ coefficients ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.8662481 0.16621436 -23.260614 1.110885e-119 ## roncas 0.3973366 0.05001066 7.945039 1.941304e-15 El modelo queda de la siguiente forma: \\[ln(\\frac{p}{1-p})=-3.87+.40X\\] Como \\(\\beta = 0.40 &gt; 0\\) entonces la probabilidad de ataque cardíaco aumenta cuando los niveles de ronquidos se incrementan. Con un nivel de ronquido \\(X=0\\) obtenemos: \\[ln(\\frac{p}{1-p})=-3.87\\] y p &lt;- exp(-3.87)/(1+exp(-3.87)) print(p) ## [1] 0.02043219 La probabilidad de tener la enfermedad es 2.04%. Mientras que si \\(X=5\\) obtenemos: \\[ln(\\frac{p}{1-p})=-3.87+.40*5=-1.87\\] y p &lt;- exp(-1.87)/(1+exp(-1.87)) print(p) ## [1] 0.1335417 La probabilidad de tener la enfermedad aumenta a 13.35%. Calcular la probabilidad de presentar la enfermedad cardíaca cuando el nivel de ronquido es Ocasional. ¿Cuántas veces más probable es la ocurrencia de la enfermedad cardíaca cuando el nivel de ronquidos es cada noche en comparación con ocasional? 13.3.3 Regresión logística múltiple en R Los datos corresponden a información sobre la respuesta a anuncios en redes sociales, se tienen datos de género, edad, salario de los individuos asi como la información de si se realizó o no la compra del producto anunciado. datos &lt;- read.csv(&quot;example_data/social_network_ads.csv&quot;, header = TRUE) Descriptivos básicos de los datos: str(datos) ## &#39;data.frame&#39;: 400 obs. of 5 variables: ## $ User.ID : int 15624510 15810944 15668575 15603246 15804002 15728773 15598044 15694829 15600575 15727311 ... ## $ Gender : chr &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; ... ## $ Age : int 19 35 26 27 19 27 27 32 25 35 ... ## $ EstimatedSalary: int 19000 20000 43000 57000 76000 58000 84000 150000 33000 65000 ... ## $ Purchased : int 0 0 0 0 0 0 0 1 0 0 ... summary(datos) ## User.ID Gender Age EstimatedSalary Purchased ## Min. :15566689 Length:400 Min. :18.00 Min. : 15000 Min. :0.0000 ## 1st Qu.:15626764 Class :character 1st Qu.:29.75 1st Qu.: 43000 1st Qu.:0.0000 ## Median :15694342 Mode :character Median :37.00 Median : 70000 Median :0.0000 ## Mean :15691540 Mean :37.66 Mean : 69743 Mean :0.3575 ## 3rd Qu.:15750363 3rd Qu.:46.00 3rd Qu.: 88000 3rd Qu.:1.0000 ## Max. :15815236 Max. :60.00 Max. :150000 Max. :1.0000 Resumen de cuantos elementos hay en cada caso para la variable de compra del producto: table(datos$Purchased) ## ## 0 1 ## 257 143 Creación del modelo de regresión logística(modelo de regresión lineal generalizado y parametrizamos por binomial) aplicando a datos[,-1] estamos dejando fuera la variable “User.ID”: modelo &lt;- glm(Purchased ~ ., data = datos[,-1], family = binomial) summary(modelo) ## ## Call: ## glm(formula = Purchased ~ ., family = binomial, data = datos[, ## -1]) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9109 -0.5218 -0.1406 0.3662 2.4254 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.278e+01 1.359e+00 -9.405 &lt; 2e-16 *** ## GenderMale 3.338e-01 3.052e-01 1.094 0.274 ## Age 2.370e-01 2.638e-02 8.984 &lt; 2e-16 *** ## EstimatedSalary 3.644e-05 5.473e-06 6.659 2.77e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 521.57 on 399 degrees of freedom ## Residual deviance: 275.84 on 396 degrees of freedom ## AIC: 283.84 ## ## Number of Fisher Scoring iterations: 6 Bondad de ajuste: dev &lt;- modelo$deviance nullDev &lt;- modelo$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 245.7297 chidf &lt;- modelo$df.null - modelo$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0 El modelo queda de la siguiente forma: \\[ln(\\frac{p}{1-p})=-12.78+.33*Genero+.237*Edad+0.000036*Salario\\] La prueba Ji-Cuadrada para la significancia del modelo da un p-value de cero, por lo tanto el modelo propuesto con las variables de género, edad y salario resulta ser significativo. Agregamos a los datos la columna de probabilidad de compra calculada con nuestro modelo: datos &lt;- cbind(datos, modelo$fitted.values) plot(modelo$fitted.values, col = as.factor(datos$Gender), main=&quot;Probabilidad de compra por género&quot;, ylab=&quot;probabilidad de compra&quot;) plot(modelo$fitted.values, datos$Age, col = as.factor(datos$Gender), main=&quot;Probabilidad de compra vs edad coloreado por edad&quot;, xlab=&quot;probabilidad de compra&quot;,ylab=&quot;edad&quot;) plot(modelo$fitted.values, datos$EstimatedSalary, main=&quot;Probabilidad de compra vs salario&quot;, xlab=&quot;probabilidad de compra&quot;,ylab=&quot;salario&quot;) Observamos que de las 3 variables consideradas, la que parece tener más efecto en la probabilidad de compra es la edad. Veamos ahora algunas probabilidades. Según nuestro modelo ¿cuál es la probabilidad de compra de una mujer 55 años con salario de 80,000? 1/(1+exp(-(-12.78+.33*0+.237*55+0.000036*80000))) ## [1] 0.9583136 ¿Y el de una mujer de 35 años con el mismo salario? 1/(1+exp(-(-12.78+.33*0+.237*35+0.000036*80000))) ## [1] 0.167284 Mientrás que la probabilidad de compra de una mujer de 55 años con salario de 80,000 es \\(96\\%\\) una mujer con el mismo salario pero 20 años mas joven tendrá una probabilidad de sólo \\(17\\%\\) de comprar el producto anunciado. Calcular la probailidad de que un hombre de 45 años con salario de 50,000 compre el producto anunciado. ¿Cuántas veces más probable es que un hombre de 45 años con salario de 100,000 compre el producto que un hombre de la misma edad pero con salario de 50,000? 13.4 Regresión Multinomial Hasta ahora hemos revisado el caso en el que la variable respuesta era dicotómica. Ahora nos centramos en el caso en el que la variable de interés tiene más de dos categorías, por ejemplo, afiliación política; resultado de un partido de fútbol; marcas de teléfonos celulares, etc. Por simplicidad, se ilustrará la metodología para el caso de tres categorías, ya que la generalización a más de tres es inmediata. Supongamos que codificamos las tres categorías de la variable respuesta como 0, 1 y 2. En el caso de regresión logística, el logit es: \\[ln(\\frac{p}{1-p})=ln(\\frac{P[Y=1]}{P[Y=0]})\\] Ahora el modelo necesita dos funciones logit ya que tenemos tres categorías, y necesitamos decidir que categorías queremos comparar. Lo más general es utilizar \\(Y = 0\\) como referencia y formar logits comparándola con \\(Y = 1\\) y \\(Y = 2\\). Supongamos que tenemos k variables explicativas, entonces: \\[ln(\\frac{P[Y=1]}{P[Y=0]})=\\beta_{10}+\\beta_{11}X_1+\\beta_{12}X_{2}+...\\beta_{1k}X_{k}\\] \\[ln(\\frac{P[Y=2]}{P[Y=0]})=\\beta_{20}+\\beta_{21}X_1+\\beta_{22}X_{2}+...\\beta_{2k}X_{k}\\] Y ahora tenemos el doble de coeficientes que en el caso de regresión logística. Las probabilidades se calcularán como: \\[P[Y=0|X]=\\frac{1}{1+e^{g_1(X)}+e^{g_2(X)}}\\] \\[P[Y=1|X]=\\frac{e^{g_1(X)}}{1+e^{g_1(X)}+e^{g_2(X)}}\\] \\[P[Y=2|X]=\\frac{e^{g_2(X)}}{1+e^{g_1(X)}+e^{g_2(X)}}\\] \\[g_1(X)=\\beta_{10}+\\beta_{11}X_1+\\beta_{12}X_{2}+...\\beta_{1k}X_{k}\\] \\[g_2(X)=\\beta_{20}+\\beta_{21}X_1+\\beta_{22}X_{2}+...\\beta_{2k}X_{k}\\] 13.4.1 Regresión Multinomial en R Datos “rh_satisfaction” corresponde a información del área de recursos humanos de una empresa que mide el nivel de satisfacción de sus empleados (en escala de 1 a 5). También se tiene algunas características de los empleados (edad, area, salario, etc). El primer análisis a realizar que queremos analizar es la satisfacción de los empleados a través del salario y la edad. Para ello aplicaremos primero una regresión para cada variable por separado y posteriormente un modelo con ambas. La función para realizar la regresión multinomial es multinom, la cual es similar a la de los comandos para regresión logística. Esta función está en la librería nnet. Cargamos la libreria y los datos: library(nnet) datos &lt;- read.csv(&quot;example_data/hr_satisfaction.csv&quot;, header=TRUE) Ajustemos el modelo de regresión multinominal para la satisfacción de los empleados con la variable de salario. multi1 &lt;- multinom(satisfied~salary, data = datos) ## # weights: 15 (8 variable) ## initial value 804.718956 ## iter 10 value 801.134736 ## final value 800.932439 ## converged print(multi1) ## Call: ## multinom(formula = satisfied ~ salary, data = datos) ## ## Coefficients: ## (Intercept) salary ## 2 -0.1080096 3.069240e-06 ## 3 -0.2193869 2.708006e-06 ## 4 -0.1515928 6.556397e-06 ## 5 -0.4102788 1.026791e-05 ## ## Residual Deviance: 1601.865 ## AIC: 1617.865 Tenemos entonces 4 ecuaciones y el análisis está tomando la categoría 1 como la base para construir los modelos logit como vimos en la teoría. Uno de los usos de este modelo puede ser el calcular probabilidades especifícas para ciertos niveles de satisfacción que sean de interés. Por ejemplo la probabilidad de que un empleado con salario de 50,000 (la media es 50,416.06) esté muy satisfecho en la empresa es \\(21\\%\\) y se calcula de la siguiente forma: s &lt;- 50000 denom &lt;- 1+exp(-0.108+.0000031*s)+exp(-0.219+.0000027*s) + exp(-0.152+.0000066*s)+exp(-0.410+.0000103*s) exp(-0.410+.0000103*s)/denom ## [1] 0.2106376 Mientras que la probabilidad de que este empleado esté completamente insatisfecho será: 1/denom ## [1] 0.1896422 Ahora ajustemos el modelo de regresión multinominal para la satisfacción de los empleados con la variable de edad. multi2 &lt;- multinom(satisfied~age, data = datos) ## # weights: 15 (8 variable) ## initial value 804.718956 ## iter 10 value 798.978325 ## final value 798.956315 ## converged multi2 ## Call: ## multinom(formula = satisfied ~ age, data = datos) ## ## Coefficients: ## (Intercept) age ## 2 -0.0680823 0.002780933 ## 3 -0.4380887 0.008846122 ## 4 0.6359941 -0.011962449 ## 5 1.1775667 -0.028143304 ## ## Residual Deviance: 1597.913 ## AIC: 1613.913 La probabilidad de que un empleado de edad 40 años esté muy satisfecho en la empresa es 20% y se calcula de la siguiente forma: e &lt;- 40 denom &lt;- 1+exp(-0.068+.0028*e)+exp(-0.438+.0088*e) + exp(0.636-.0120*e)+exp(1.178-.0281*e) exp(1.178-.0281*e)/denom ## [1] 0.2034909 Finalmente ajustamos el modelo de regresión con las variables explicativas de salario y edad. multi3 &lt;- multinom(satisfied~salary+age, data = datos) ## # weights: 20 (12 variable) ## initial value 804.718956 ## iter 10 value 799.192997 ## final value 797.372424 ## converged multi3 ## Call: ## multinom(formula = satisfied ~ salary + age, data = datos) ## ## Coefficients: ## (Intercept) salary age ## 2 -0.2315138 3.109419e-06 0.003091909 ## 3 -0.5872992 2.836453e-06 0.009138518 ## 4 0.2954069 6.368488e-06 -0.011320961 ## 5 0.6390753 9.838360e-06 -0.027078238 ## ## Residual Deviance: 1594.745 ## AIC: 1618.745 Con este modelo entonces podemos calcular la probabilidad de que un empleado de edad 25 años esté muy satisfecho en la empresa es \\(27\\%\\) y se calcula de la siguiente forma: e &lt;- 25 s &lt;- 50000 denom &lt;- 1+exp(-0.232+.0000031*s+0.003*e)+exp(-0.587+.0000028*s+0.009*e) + exp(0.295+.0000063*s-0.011*e)+exp(0.640+.0000098*s-0.027*e) exp(0.640+.0000098*s-0.027*e)/denom ## [1] 0.2730235 Mientras que la probabilidad de estar completamente insatisfecho será: 1/denom ## [1] 0.1732192 Calcular la probabilidad de un empleado de 50 años con salario de 65,000 asigne un nivel de satisfacción de 3. Ajustar un modelo con las variables de salario y educación. Calcular la probabilidad de que un empleado con estudios de postgrado con salario de 65,000 este completamente satisfecho con la empresa. Si quisiera analizar la satisfacción por departamento ¿qué procedimiento propondría usar? 13.5 Modelos para conteos En muchos casos las variables respuesta son conteos, y en ocasiones estos recuentos aparecen al resumir en tablas de contingencia otras variables. Hay cuatro razones por las que sería erroneo utlizar un modelo de regresión normal para datos de conteo : Puede dar lugar a predicciones negativas. La varianza de la variable respuesta no es independiente de la media. Los errores no siguen una distribución Normal. Los ceros que aparecen en la variable respuesta dan problemas a la hora de transformar la variables. Sin embargo, si la variable es de conteo pero los datos toman valores elevados, entonces si podría ser posible utilizar la distribución Normal. El modelo más simple para cuando la variable de respuesta son recuentos es asumir que el componente aleatorio \\(Y\\) sigue una distribución de Poisson. Esta distribución es unimodal y su propiedad más destacada es que la media y la varianza coinciden. \\[E(Y)=Var(Y)=\\mu\\] De modo que cuando el número de recuentos es mayor en media, también tienden a tener mayor variabilidad. La principal diferencia entre la distribución de Poisson y la Binomial, es que, aunque ambas cuentan el número de veces que ocurre algo, en la distribución de Poisson no sabemos cuántas veces no ocurrio, y en la Binomial sí lo sabemos. Supongamos que estamos haciendo un estudio sobre cuantas larvas de insectos hay en ciertos árboles, los datos de los que disponemos corresponden al número de larvas por hoja \\((Y)\\). Habrá hojas que no tengan ninguna, y otras que tenga hasta 5 ó 6. Si el número medio de larvas por hoja es \\(\\mu\\), la probabilidad de observar \\(y_0\\) larvas por hoja viene dada por la siguiente ecuación: \\[P(y_0)=\\frac{e^{\\mu}\\mu^{y_0}}{y_0!}\\] Donde \\(\\mu\\) se puede aproximar con \\(\\mu=np\\), para \\(n\\) grande y \\(p\\) pequeño. Es decir, que una distribución de Poisson se obtiene a partir de una Binomial con \\(p\\) pequeño y \\(n\\). Entonces el modelo GLM para los conteos se basará en modelar la relación entre la media muestral \\(\\mu\\) y las variables explicativas. \\[\\mu=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] Por las características de la variable (son conteos) buscamos que la parte derecha de la ecuación sólo tome valores positivos. Por esta razón habitualmente se usa el logaritmo de la media como la función liga, de modo que el modelo log-lineal se puede expresar como: \\[log(\\mu) = log(E[Y])=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] de modo que al despejar \\(\\mu\\) obtenemos: \\[\\mu = e^{\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}}\\] 13.5.1 Sobredispersión en GLM Poisson En una distribución de Poisson, la media y la varianza son iguales, pero en la práctica, los datos de conteo muestran mayor variabilidad de la que se espera en un modelo binomial o Poisson. En el caso de este último, es común que la varianza sea mucho mayor que la media \\(\\mathbb{V}\\left(Y\\right)&gt;&gt;\\mathbb{E}\\left(Y\\right)=\\mu\\), este fenómeno se conoce como sobredispersión. Por ejemplo, podemos suponer que cada individuo tienen igual probabilidad de padecer cierta enfermedad; no obstante, siendo más realistas, es claro que que estas probabilidades varían debido a factores genéticos, de salubridad y de localización geográfica, entre otros, propiciando mayor variabilidad sobre el número de sujetos enfermos en un periodo determinado, que los que puede predecir el modelo Poisson asociado. Una forma de medir la sobredispersión en los datos es ajustando una distribución quasipoisson, la cual ajustará el modelo con distribución Poisson pero no asumirá varianza igual a la media y calculará el parámetro de dispersión. 13.5.2 GLM Poisson R Entre los cangrejos cacerola se sabe que cada hembra tiene un macho en su nido, pero puede tener más machos concubinos. Considerando que deseamos relacionar la variable respuesta, el número de concubinos, con las variables explicativas son: color, estado de la espina central, peso y anchura del caparazón, procederemos a ajustar un modelo lineal a los datos. En un primer análisis sólo consideramos la anchura del caparazón como variable explicativa. tabla &lt;- read.csv ( &quot;http://www.hofroe.net/stat557/data/crab.txt&quot; , header =TRUE, sep = &quot;\\t&quot; ) plot.tabla &lt;- aggregate (rep (1,nrow (tabla)), list (Sa= tabla$Satellite , W=tabla$Width), sum) plot(tabla$Width,tabla$Satellite, xlab=&quot;Ancho de caparazón&quot;, ylab=&quot;Número de Concubinos&quot;, bty=&quot;L&quot;, axes = FALSE, type =&quot;n&quot;) axis (2, at =1:15) axis (1, at=seq (20 , 34, 2)) text (y= plot.tabla$Sa , x= plot.tabla$W, labels = plot.tabla$x) Ahora agruparemos los datos según cortes específicos en la variable de ancho de caparazón (Width) para poder ver visualmente el ajuste del modelo de regresión. Discretizamos el ancho del caparazón tabla$W.fac = cut( tabla$Width , breaks =c(0, seq (23.25 , 29.25), Inf )) Calculamos el numero medio de concubinos para cada categoria según el ancho del caparazón: plot.y &lt;- aggregate ( tabla$Satellite , by= list (W= tabla$W.fac), mean )$x Determinamos la media del ancho del caparazon por categoría: plot.x &lt;- aggregate ( tabla$Width , by = list (W= tabla$W.fac ), mean )$x Representamos las medias de anchura y la media del numero de concubinos: plot (x = plot.x , y = plot.y , ylab = &quot; Numero de Concubinos &quot;, xlab = &quot; Anchura (cm) &quot;, bty = &quot;L&quot;, axes = FALSE, type = &quot;p&quot;, pch = 16) axis(2, at =0:5) axis(1, at=seq (20 , 34, 2)) En este caso podemos observar una relación lineal entre la media del número de concubinos y la media del ancho del caparazón por categoría. Ahora ajustamos el modelo lineal entre el número de concubinos y el ancho del caparazón definiendo una función de distribución Poisson. m1 &lt;- glm(Satellite~Width , family = poisson , data = tabla ) summary( m1 ) ## ## Call: ## glm(formula = Satellite ~ Width, family = poisson, data = tabla) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8526 -1.9884 -0.4933 1.0970 4.9221 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.30476 0.54224 -6.095 1.1e-09 *** ## Width 0.16405 0.01997 8.216 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 632.79 on 172 degrees of freedom ## Residual deviance: 567.88 on 171 degrees of freedom ## AIC: 927.18 ## ## Number of Fisher Scoring iterations: 6 Recordando que para medir el ajuste del modelo utilizamos la devianza, en donde la devianza nula es la desviación para el modelo que no depende de ninguna variable. Miéntras que la devianza residual es la diferencia entre la desviación del modelo que no depende de ninguna variable menos la del modelo que incluye a la variable “Width”. La diferencia entre ambas se distribuirá como una distribución Ji-cuadrada con 1 grado de libertad. Bondad de Ajuste: dev &lt;- m1$deviance nullDev &lt;- m1$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 64.91309 chisq.prob &lt;- 1 - pchisq(modelChi, 1) chisq.prob ## [1] 7.771561e-16 Se puede rechazar claramente la hipótesis nula. Por lo que concluimos que hay un aportación significativa de la variable de ancho del caparazón al modelo del número de concubinos. El modelo queda de la siguiente forma: \\[ln(\\mu)=-3.30+0.16*Width\\] Y los valores estimados por el modelo se pueden encontrar en m1$fitted.values. Y se puede predecir la media del números de concubinos para un valor de ancho de caparazón dado, por ejemplo, para una anchura igual a 26.3: predict.glm ( m1 , type = &quot;response&quot;, newdata = data.frame( Width = 26.3 )) ## 1 ## 2.744581 Entonces siguiendo este modelo, decimos que los cangrejos hembra con ancho de caparazón del 26.3cm tienen en promedio 2.7 parejas. Podriamos ajustar ahora el modelo con las variables de peso, color y estado de la espina. Se deja al lector el ejercicio de utilizando el algoritmo stepAIC encontrar el mejor modelo para el número de concubinos. 13.6 GLM binomial negativa Una distribución que puede usarse como alternativa a una Poisson es la . Dado que su varianza es más grande que su media, constituye una excelente alternativa para modelar datos de conteo sobredispersos, que son muy comunes en aplicaciones reales. Si una variable aleatoria \\(Y\\) se distribuye como una binomial negativa, entonces la función de probabilidad es: \\[P(y|k,\\mu)=\\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}\\left( \\frac{k}{\\mu+k} \\right)^k\\left( 1-\\frac{k}{\\mu+k} \\right)^y\\] con \\(y=0,1,2,...\\) donde \\(k\\) y \\(\\mu\\) son los parámetros de la distribución y se tiene que \\[E(Y)=\\mu\\] \\[Var(Y)=\\mu+\\frac{\\mu^2}{k}\\] El parámetro \\(\\frac{1}{k}\\) es un parámetro de dispersión, de modo que si \\(\\frac{1}{k} \\rightarrow 0\\) entonces \\(Var(Y)\\rightarrow \\mu\\) y la distribución binomial negativa converge a una distribución Poisson. Por otro lado, para un valor fijo de \\(k\\) esta distribución pertenece a la familia exponencial natural, de modo que se puede definir un modelo GLM binomial negativo. En general, se usa una función liga de tipo logaritmo. La regresión binomial negativa se puede utilizar para datos sobredispersos de recuentos, es decir cuando la varianza condicional es mayor que la media condicional. Se puede considerar como una generalización de la regresión de Poisson, ya que tiene su misma estructura de medias y además un parámetro adicional para el modelo de sobredispersión. Si la distribución condicional de la variable observada es más dispersa, los intervalos de confianza para la regresión binomial negativa es probable que sean más estrechos que los correspondientes a un modelo de regresión de Poisson. 13.6.1 GLM Binomial Negativa en R Usando los mismos datos de los cangrejos ahora ajustaremos un modelo con distribución binomial negativa: require( MASS ) m2 &lt;- glm.nb(Satellite~Width , data = tabla ) summary( m2 ) ## ## Call: ## glm.nb(formula = Satellite ~ Width, data = tabla, init.theta = 0.90456808, ## link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7798 -1.4110 -0.2502 0.4770 2.0177 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.05251 1.17143 -3.459 0.000541 *** ## Width 0.19207 0.04406 4.360 1.3e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(0.9046) family taken to be 1) ## ## Null deviance: 213.05 on 172 degrees of freedom ## Residual deviance: 195.81 on 171 degrees of freedom ## AIC: 757.29 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 0.905 ## Std. Err.: 0.161 ## ## 2 x log-likelihood: -751.291 En este caso el modelo queda de la siguiente forma: \\[ln(\\mu)=-4.05+0.19*Width\\] Observamos que tanto la devianza como el coeficiente de Akaike son menores en comparación con el modelo que usa una distribución Poisson. Adicionalmente habiamos visto que los datos indicaban que la varianza era poco mas de tres veces la media. Por estas razones para este ejemplo se puede decir que un modelo con distribución binomial negativa es mas adecuado. 13.6.2 Ejercicio La base de datos “productos.csv” contiene información sobre el número de productos financieros que posee cada cliente, adicional a esa información se tiene la edad, género, número de ofertas, antigüedad y el número de créditos que tiene. Deseamos obtener información respecto a la relación entre el número de productos y el resto de las variables que nos permitan incrementar la venta de productos. Ajustar un modelo al número de productos financieros en función de la edad del cliente. Asumiendo una distribución Poisson Calcular con la distribución quasiPoisson el valor del parámetro de dispersión para estos datos. ¿Qué puede concluir de este modelo? ¿Valdrá la pena cambiar a una distribución binomial negativa? Encontrar el mejor modelo que explique el número de productos financieros a partir de todas las variables disponibles. ¿Qué recomendación puede dar al área de mercadotecnia que esta buscando incrementar el número de productos por cliente? 13.7 Exponencial Se dice que la variable respuesta \\(Y\\) es de tipo Exponencial cuando hemos observado el tiempo transcurrido hasta que ocurre un evento de interés como resultado de un conjunto de variables predictoras que pueden ser de tipo numérico o categórico. En este caso similar al caso de conteos también se asume que \\(Y\\) sólo toma valores positivos y es continua. En este caso la función liga utilizada es el recíproco, de modo que el modelo lineal se puede expresar como: \\[\\frac{1}{E[Y]}=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] 13.7.1 GLM Exponencial en R Los datos de este ejemplo corresponden a la información de un banco qué busca entender la transición de sus clientes hacia un estado de alto riesgo de crédito. Con la finalidad de entender mejor el momento en que el cliente pasa a un estado de alto riesgo crediticio por falta de pago, el banco hizo un seguimiento a una muestra representativa de sus clientes. Deseamos identificar cuáles de las características de los clientes influyen en la transición de un estado de bajo riesgo a alto riesgo. datos &lt;- read.csv(&quot;example_data/default.csv&quot;, header=TRUE) Breve análisis exploratorio: str(datos) ## &#39;data.frame&#39;: 1000 obs. of 17 variables: ## $ duration : int 6 48 12 42 24 36 24 36 12 30 ... ## $ credit_history : chr &quot;critical/other existing credit&quot; &quot;existing paid&quot; &quot;critical/other existing credit&quot; &quot;existing paid&quot; ... ## $ purpose : chr &quot;radio/tv&quot; &quot;radio/tv&quot; &quot;education&quot; &quot;furniture/equipment&quot; ... ## $ credit_amount : int 1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ... ## $ installment_commitment: int 4 2 2 2 3 2 3 2 2 4 ... ## $ personal_status : chr &quot;male single&quot; &quot;female div/dep/mar&quot; &quot;male single&quot; &quot;male single&quot; ... ## $ residence_since : int 4 2 3 4 4 4 4 2 4 2 ... ## $ property_magnitude : chr &quot;real estate&quot; &quot;real estate&quot; &quot;real estate&quot; &quot;life insurance&quot; ... ## $ age : int 67 22 49 45 53 35 53 35 61 28 ... ## $ other_payment_plans : chr &quot;none&quot; &quot;none&quot; &quot;none&quot; &quot;none&quot; ... ## $ housing : chr &quot;own&quot; &quot;own&quot; &quot;own&quot; &quot;for free&quot; ... ## $ existing_credits : int 2 1 1 1 2 1 1 1 1 2 ... ## $ job : chr &quot;skilled&quot; &quot;skilled&quot; &quot;unskilled resident&quot; &quot;skilled&quot; ... ## $ num_dependents : int 1 1 2 2 2 2 1 1 1 1 ... ## $ own_telephone : chr &quot;yes&quot; &quot;none&quot; &quot;none&quot; &quot;none&quot; ... ## $ foreign_worker : chr &quot;yes&quot; &quot;yes&quot; &quot;yes&quot; &quot;yes&quot; ... ## $ default : int 0 1 0 0 1 0 0 0 0 1 ... summary(datos) ## duration credit_history purpose credit_amount installment_commitment personal_status residence_since property_magnitude ## Min. : 4.0 Length:1000 Length:1000 Min. : 250 Min. :1.000 Length:1000 Min. :1.000 Length:1000 ## 1st Qu.:12.0 Class :character Class :character 1st Qu.: 1366 1st Qu.:2.000 Class :character 1st Qu.:2.000 Class :character ## Median :18.0 Mode :character Mode :character Median : 2320 Median :3.000 Mode :character Median :3.000 Mode :character ## Mean :20.9 Mean : 3271 Mean :2.973 Mean :2.845 ## 3rd Qu.:24.0 3rd Qu.: 3972 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :72.0 Max. :18424 Max. :4.000 Max. :4.000 ## age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker ## Min. :19.00 Length:1000 Length:1000 Min. :1.000 Length:1000 Min. :1.000 Length:1000 Length:1000 ## 1st Qu.:27.00 Class :character Class :character 1st Qu.:1.000 Class :character 1st Qu.:1.000 Class :character Class :character ## Median :33.00 Mode :character Mode :character Median :1.000 Mode :character Median :1.000 Mode :character Mode :character ## Mean :35.55 Mean :1.407 Mean :1.155 ## 3rd Qu.:42.00 3rd Qu.:2.000 3rd Qu.:1.000 ## Max. :75.00 Max. :4.000 Max. :2.000 ## default ## Min. :0.0 ## 1st Qu.:0.0 ## Median :0.0 ## Mean :0.3 ## 3rd Qu.:1.0 ## Max. :1.0 Selección de observaciones en donde si hubo falta de pago: datos &lt;- datos[datos$default==1,] Construcción de la variable del tiempo transcurrido al default: datos$meses_default &lt;- datos$duration-datos$installment_commitment hist(datos$meses_default) Lo primero será ajustar un modelo que explique el tiempo en que occure la falta de pago en función de la variable tenencia de teléfono propio. N.B. La distribución exponencial es un caso particular de la distribución gamma y por esa razón en la función glm sólo aparece la familia Gamma m1 &lt;- glm(meses_default ~ own_telephone, data = datos, family = Gamma) summary(m1) ## ## Call: ## glm(formula = meses_default ~ own_telephone, family = Gamma, ## data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7845 -0.6226 -0.1556 0.2734 1.5773 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.049933 0.002235 22.34 &lt; 2e-16 *** ## own_telephoneyes -0.009344 0.003234 -2.89 0.00414 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.374624) ## ## Null deviance: 123.08 on 299 degrees of freedom ## Residual deviance: 120.01 on 298 degrees of freedom ## AIC: 2327.4 ## ## Number of Fisher Scoring iterations: 5 Bondad de ajuste: dev &lt;- m1$deviance nullDev &lt;- m1$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 3.07052 chidf &lt;- m1$df.null - m1$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0.07972398 La prueba de bondad de ajuste da una probabilidad de \\(7\\%\\) que es menor al \\(10\\%\\) y puede considerarse suficiente para decir que el modelo con esta variable si aporta información estadísticamente significativa. El modelo queda de la siguiente forma: \\[\\frac{1}{E(Y)}=0.050-0.009*TeléfonoPropio\\] Con este modelo podriamos calcular, en promedio, en cuantos meses de iniciado el crédito se presentará el no pago en función de si el teléfono es propio o no. table(m1$fitted.values) %&gt;% knitr::kable() Var1 Freq 20.0267380435668 187 24.6371681425079 113 Recordando que el objetivo del ejercicio es entender mejor la relación entre las caracteristicas de los clientes y su riesgo crediticio nos podemos plantear las siguientes preguntas: ¿A qué se debe que sólo tenemos 2 estimaciones de tiempo? ¿Con estos datos un modelo en donde la variable explicativa es la edad, es mejor? ¿Cómo podemos mejorar el modelo y/o las estimaciones de los tiempo de falta de pago? 13.8 Gamma Como se mencionó en la sección anterior las distribución exponencial es un caso particular de la distribución gamma. En general cuando la variable de respuesta es de tipo numérico, pero sólo puede tomar valores positivos de forma asimétrica, es decir, se encuentra concentrada en un conjunto de valores y su frecuencia disminuye cuando aumenta el valor de la respuesta, se dice que la variable se distribuye gamma. La distribución gamma sólo está definida para valores mayores a cero, por lo que si la variable de respuesta \\(Y\\) toma valores negativos o cero, para poder utilizar esta distribución será necesario realizar una transformación de los datos, sumando una constante lo suficientemente grande que haga todas las obervaciones positivas. Análogo al caso exponencial, la función liga utilizada es el recíproco, de modo que el modelo lineal se expresa como: \\[\\frac{1}{E[Y]}=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{k}X_{k}\\] 13.8.1 GLM Gamma en R La base de datos que utilizaremos para este ejemplo, corresponde a las reclamaciones recibidas por cierta aseguradora para su producto de seguro de automóviles. Los siniestros corresponden al primer trimestre de 2015 y se registraron además del monto total reclamado, características del asegurado asi como del siniestro. La aseguradora busca modelar el monto total reclamado en función de las otras variables medidas para con ello mejorar su cálculo de primas e identificación de grupos que deberian tener sobreprima. Cargamos los datos y realizamos un breve análisis exploratorio de los datos. datos &lt;- read.csv(&quot;example_data/insurance_claims.csv&quot;, header = TRUE) str(datos) ## &#39;data.frame&#39;: 1000 obs. of 22 variables: ## $ months_as_customer : int 328 228 134 256 228 256 137 165 27 212 ... ## $ age : int 48 42 29 41 44 39 34 37 33 42 ... ## $ policy_number : int 521585 342868 687698 227811 367455 104594 413978 429027 485665 636550 ... ## $ policy_state : chr &quot;OH&quot; &quot;IN&quot; &quot;OH&quot; &quot;IL&quot; ... ## $ policy_deductable : int 1000 2000 2000 2000 1000 1000 1000 1000 500 500 ... ## $ policy_annual_premium : num 1407 1197 1413 1416 1584 ... ## $ insured_sex : chr &quot;MALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; &quot;FEMALE&quot; ... ## $ insured_education_level : chr &quot;MD&quot; &quot;MD&quot; &quot;PhD&quot; &quot;PhD&quot; ... ## $ insured_occupation : chr &quot;craft-repair&quot; &quot;machine-op-inspct&quot; &quot;sales&quot; &quot;armed-forces&quot; ... ## $ insured_hobbies : chr &quot;sleeping&quot; &quot;reading&quot; &quot;board-games&quot; &quot;board-games&quot; ... ## $ insured_relationship : chr &quot;husband&quot; &quot;other-relative&quot; &quot;own-child&quot; &quot;unmarried&quot; ... ## $ incident_date : chr &quot;25/01/2015&quot; &quot;21/01/2015&quot; &quot;22/02/2015&quot; &quot;10/01/2015&quot; ... ## $ incident_type : chr &quot;Single Vehicle Collision&quot; &quot;Vehicle Theft&quot; &quot;Multi-vehicle Collision&quot; &quot;Single Vehicle Collision&quot; ... ## $ collision_type : chr &quot;Side Collision&quot; &quot;?&quot; &quot;Rear Collision&quot; &quot;Front Collision&quot; ... ## $ incident_severity : chr &quot;Major Damage&quot; &quot;Minor Damage&quot; &quot;Minor Damage&quot; &quot;Major Damage&quot; ... ## $ authorities_contacted : chr &quot;Police&quot; &quot;Police&quot; &quot;Police&quot; &quot;Police&quot; ... ## $ incident_hour_of_the_day : int 5 8 7 5 20 19 0 23 21 14 ... ## $ number_of_vehicles_involved: int 1 1 3 1 1 3 3 3 1 1 ... ## $ auto_make : chr &quot;Saab&quot; &quot;Mercedes&quot; &quot;Dodge&quot; &quot;Chevrolet&quot; ... ## $ auto_model : chr &quot;92x&quot; &quot;E400&quot; &quot;RAM&quot; &quot;Tahoe&quot; ... ## $ auto_year : int 2004 2007 2007 2014 2009 2003 2012 2015 2012 1996 ... ## $ total_claim_amount : int 71610 5070 34650 63400 6500 64100 78650 51590 27700 42300 ... summary(datos) ## months_as_customer age policy_number policy_state policy_deductable policy_annual_premium insured_sex ## Min. : 0.0 Min. :19.00 Min. :100804 Length:1000 Min. : 500 Min. : 433.3 Length:1000 ## 1st Qu.:115.8 1st Qu.:32.00 1st Qu.:335980 Class :character 1st Qu.: 500 1st Qu.:1089.6 Class :character ## Median :199.5 Median :38.00 Median :533135 Mode :character Median :1000 Median :1257.2 Mode :character ## Mean :204.0 Mean :38.95 Mean :546239 Mean :1136 Mean :1256.4 ## 3rd Qu.:276.2 3rd Qu.:44.00 3rd Qu.:759100 3rd Qu.:2000 3rd Qu.:1415.7 ## Max. :479.0 Max. :64.00 Max. :999435 Max. :2000 Max. :2047.6 ## insured_education_level insured_occupation insured_hobbies insured_relationship incident_date incident_type collision_type ## Length:1000 Length:1000 Length:1000 Length:1000 Length:1000 Length:1000 Length:1000 ## Class :character Class :character Class :character Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character Mode :character Mode :character Mode :character ## ## ## ## incident_severity authorities_contacted incident_hour_of_the_day number_of_vehicles_involved auto_make auto_model auto_year ## Length:1000 Length:1000 Min. : 0.00 Min. :1.000 Length:1000 Length:1000 Min. :1995 ## Class :character Class :character 1st Qu.: 6.00 1st Qu.:1.000 Class :character Class :character 1st Qu.:2000 ## Mode :character Mode :character Median :12.00 Median :1.000 Mode :character Mode :character Median :2005 ## Mean :11.64 Mean :1.839 Mean :2005 ## 3rd Qu.:17.00 3rd Qu.:3.000 3rd Qu.:2010 ## Max. :23.00 Max. :4.000 Max. :2015 ## total_claim_amount ## Min. : 100 ## 1st Qu.: 41813 ## Median : 58055 ## Mean : 52762 ## 3rd Qu.: 70593 ## Max. :114920 Eliminar las columnas que contienen información específica: datos &lt;- datos[,c(-3,-12)] Ajustaremos un primer modelo con la variable de género del asegurado: m1 &lt;- glm(total_claim_amount ~ insured_sex, data = datos, family = Gamma) summary(m1) ## ## Call: ## glm(formula = total_claim_amount ~ insured_sex, family = Gamma, ## data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.2500 -0.2192 0.0987 0.3053 0.8796 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.875e-05 4.049e-07 46.30 &lt;2e-16 *** ## insured_sexMALE 4.519e-07 6.027e-07 0.75 0.454 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.2504562) ## ## Null deviance: 599.52 on 999 degrees of freedom ## Residual deviance: 599.38 on 998 degrees of freedom ## AIC: 23578 ## ## Number of Fisher Scoring iterations: 5 Bondad de ajuste: dev &lt;- m1$deviance nullDev &lt;- m1$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 0.1409954 chidf &lt;- m1$df.null - m1$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0.7072934 La prueba de bondad de ajuste nos dice que el modelo es malo ya que la explicación de la varianza ganada con el modelo no es estadísticamente significativa. Intentemos ahora con la variable de edad m2 &lt;- glm(total_claim_amount ~ age, data = datos, family = Gamma) summary(m2) ## ## Call: ## glm(formula = total_claim_amount ~ age, family = Gamma, data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.2383 -0.2181 0.0940 0.3057 0.8563 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.175e-05 1.313e-06 16.571 &lt;2e-16 *** ## age -7.125e-08 3.228e-08 -2.208 0.0275 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.2506544) ## ## Null deviance: 599.52 on 999 degrees of freedom ## Residual deviance: 598.31 on 998 degrees of freedom ## AIC: 23576 ## ## Number of Fisher Scoring iterations: 5 Bondad de ajuste: dev &lt;- m2$deviance nullDev &lt;- m2$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 1.207078 chidf &lt;- m2$df.null - m2$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0.2719116 ¿Es un buen modelo?, ¿Es mejor que el modelo de género? Probar con la variable de tipo de siniestro. m3 &lt;- glm( total_claim_amount ~ incident_type, data = datos, family = Gamma) summary(m3) ## ## Call: ## glm(formula = total_claim_amount ~ incident_type, family = Gamma, ## data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.44568 -0.18206 -0.01135 0.17050 0.80416 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.622e-05 2.108e-07 76.967 &lt;2e-16 *** ## incident_typeParked Car 1.722e-04 5.471e-06 31.470 &lt;2e-16 *** ## incident_typeSingle Vehicle Collision -7.070e-07 2.944e-07 -2.401 0.0165 * ## incident_typeVehicle Theft 1.650e-04 4.976e-06 33.162 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.07073018) ## ## Null deviance: 599.520 on 999 degrees of freedom ## Residual deviance: 80.724 on 996 degrees of freedom ## AIC: 21492 ## ## Number of Fisher Scoring iterations: 5 Bondad de ajuste: dev &lt;- m3$deviance nullDev &lt;- m3$null.deviance modelChi &lt;- nullDev - dev modelChi ## [1] 518.7961 chidf &lt;- m3$df.null - m3$df.residual chisq.prob &lt;- 1 - pchisq(modelChi, chidf) chisq.prob ## [1] 0 ¿Qué podemos decir de este modelo?, ¿Nos sirve este modelo para clasificar la sobreprima? ¿Podemos encontrar un mejor modelo? "],
["modelos-lineales-generalizados-construcción-y-evaluación.html", "Capítulo 14 Modelos Lineales Generalizados (Construcción y Evaluación) 14.1 Exploración de los datos. 14.2 Elección de la estructura de errores y función liga. 14.3 Bondad de ajuste. 14.4 Simplificación del modelo. 14.5 Criterios de evaluación de modelos. 14.6 Análisis de los residuos. 14.7 Evaluación de GLMs en R", " Capítulo 14 Modelos Lineales Generalizados (Construcción y Evaluación) En la construcción de modelos lineales generalizados es importante tener en cuenta que NO existe un único modelo que sea válido. En la mayoría de los casos, habrá un número variable de modelos plausibles que puedan ajustarse a un conjunto determinado de datos. Parte del trabajo de construcción y evaluación del modelo es determinar cuál de todos estos modelos son adecuados, y entre todos los modelos adecuados, cuál es el que explica la mayor proporción de la varianza sujeto a la restricción de que todos los parámetros del modelo deberán ser estadísticamente significativos. En algunos casos habrá más de un modelo que ajuste igual de bien a los datos y en esos casos queda a criterio del modelador elegir uno u otro. Los pasos que hay que seguir en la construcción y evaluación de un GLM son muy similares a los de cualquier modelo estadístico. 14.1 Exploración de los datos. Siempre es conveniente conocer los datos con los que se esta trabajando. Puede resultar interesante obtener gráficos que nos muestren la relación entre la variable de respuesta y cada una de las variables explicativas, gráficos de caja (box-plot) para variables categóricas, o matrices de correlación entre las variables explicativas. El objetivo de este análisis exploratorio es: Buscar posibles relaciones de la variable respuesta/dependiente con la(s) variable(s) explicativa(s). Considerar la necesidad de aplicar transformaciones de las variables. Eliminar variables explicativas que estén altamente correlacionadas. 14.2 Elección de la estructura de errores y función liga. A veces resultará fácil elegir estas propiedades del modelo basandose en las características de la variable de respuesta. Pero en otras ocasiones resultará tremendamente difícil, y será a posteriori cuando comprobemos, analizando los residuos, la idoneidad de la distribución de errores elegida. Por otro lado, puede ser una práctica recomendable el comparar modelos con distintas funciones liga para ver cuál se ajusta mejor a nuestros datos. 14.3 Bondad de ajuste. Los tests de significación para los estimadores del modelo. (p-values de los estimadores) La cantidad de varianza explicada por el modelo. Esto en GLM se conoce como devianza. La devianza nos da una idea de la variabilidad del los datos. Por ello, para obtener una medida de la variabilidad explicada por el modelo, hemos de comparar la devianza del modelo nulo (Null deviance) con la devianza residual (Residual deviance), esto es, una medida de cuánto de la variabilidad de la variable respuesta no es explicado por el modelo. (Prueba de la Chi-cuadrada sobre la devianza) 14.4 Simplificación del modelo. El principio de parsimonia requiere que el modelo sea tan simple como sea posible. Esto significa que no debe contener parámetros o niveles de un factor que sean redundantes. La simplificación del modelo implica por tanto: La eliminación de las variables explicativas que no sean significativas. La agrupación de los niveles de factores (variables categóricas) que no difieran entre sí. La simplificación del modelo tiene que tener, además, una cierta lógica para el analista y no debe incrementar de manera significativa la devianza residual. 14.5 Criterios de evaluación de modelos. Podemos utilizar la reducción de la devianza como una medida del ajuste del modelo a los datos. Los tests de significación para los parámetros del modelo son también útiles para ayudarnos a simplificar el modelo. Un criterio comúnmente utilizado es el llamado Criterio de Información de Akaike (AIC del inglés Akaike Information Criterion). Este índice evalúa tanto el ajuste del modelo a los datos como la complejidad del modelo. Cuanto más pequeño es el AIC mejor es el ajuste. El AIC es muy útil para comparar modelos similares con distintos grados de complejidad o modelos iguales (mismas variables) pero con funciones liga distintas. 14.6 Análisis de los residuos. Los residuos son las diferencias entre los valores estimados por el modelo y los valores observados. Sin embargo, muchas veces se utilizan los residuos estandarizados, que tienen que seguir una distribución normal. Conviene analizar los siguientes gráficos: Histograma de los residuos. Gráfico de residuos frente a valores estimados. Estos gráficos pueden indicar falta de linealidad, heterocedasticidad (varianza no constante) y valores atípicos. Gráficos de valores atípicos. Existen tests que permiten detectar valores atípicos. Los índices más comunes son el índice de Cook y el de apalancamiento o leverage. Estos gráficos ayudan la evaluación del modelo utilizado. En caso necesario, sería preciso volver a plantear el modelo, tal vez utilizando una estructura de errores más adecuada, otra función liga o incluso eliminando ciertos datos que pueden estar desviando el análisis. 14.7 Evaluación de GLMs en R Ejemplo de meses para default crediticio usando una distribución exponencial. datos &lt;- read.csv(&quot;example_data/default.csv&quot;, header = TRUE) datos &lt;- datos[datos$default==1,] Construcción de la variable del tiempo transcurrido al default: datos$meses_default &lt;- datos$duration-datos$installment_commitment m1 &lt;- glm(meses_default ~ own_telephone, data = datos, family = Gamma) Análisis de puntos influyentes. Distancia de Cook: plot(m1, 4) Puntos palanca para betas: plot(dfbetas(m1)[,1],type=&quot;h&quot;, ylab=&quot;Beta0&quot;) plot(dfbetas(m1)[,2],type=&quot;h&quot;, ylab=&quot;Beta1&quot;) Puntos palanca para los datos ajustados: plot(dffits(m1),type=&quot;h&quot;) "],
["qué-es-una-red-neuronal.html", "Capítulo 15 ¿Qué es una red neuronal? 15.1 Ejemplo", " Capítulo 15 ¿Qué es una red neuronal? Las redes neuronales artificiales están inspiradas en la forma en la que las neuronas de nuestro cerebro trabajan en conjunto para resolver una tarea compleja. Éstas modelan una variable respuesta, ya sea de tipo continua o categórica, como función de las covariables a través de la composición de funciones no lineales. De manera general, una red nueronal consiste de una arquitectura, una regla de activación y una regla de salida. Arquitectura: Puede ser descrita vía un gráfo dirigido cuyo nodos son llamados neuronas. Existen una grán cantidad de arquitecturas dependiendo de la naturaleza del grafo i.e. de las relaciones entre los nodos. Una descripción bastante completa de las distintas arquitecturas y sus nombres puede ser consultada Regla de activación: Típicamente el valor en cada nodo \\(v\\) puede ser calculado como \\[x_v = f(\\sum_{u \\rightarrow v}\\beta_{uv}x_u)\\] donde la suma se obtiene sobre los nodos predecesores de \\(v\\) y \\(\\beta_{uv}\\) son los coeficientes (desconocidos) de la red. A la función \\(f(\\cdot)\\) se le conoce como función de activación. Las más comúnmente usadas son la sigmoide logística \\(f(x) = \\frac{e^y}{1+e^y}\\) y la función rectificadora \\(f(x) = \\max\\{{y,0}\\}\\) 15.1 Ejemplo Dada la siguiente arquitectura calcularemos el valor de salida dado por el nodo \\(x_6\\). \\[\\begin{align*} x_{6} &amp; =f\\left(\\beta_{3,6}\\cdot x_{3}+\\beta_{4,6}\\cdot x_{4}+\\beta_{5,6}\\cdot x_{5}\\right)\\\\ &amp; \\hookrightarrow x_{3}=f\\left(\\beta_{1,3}\\cdot x_{1}+\\beta_{2,3}\\cdot x_{2}\\right)\\\\ &amp; \\hookrightarrow x_{4}=f\\left(\\beta_{1,4}\\cdot x_{1}+\\beta_{2,4}\\cdot x_{2}\\right)\\\\ &amp; \\hookrightarrow x_{5}=f\\left(\\beta_{1,5}\\cdot x_{1}+\\beta_{2,5}\\cdot x_{2}\\right)\\\\ &amp; \\vdots \\end{align*}\\] donde \\(f(\\cdot)\\) es la función de activación. Si suponemos que \\(f(x) = \\mathbb{I}(x)\\) entonces el problema a resolver será el de regresión lineal. Nodos de sesgo: Dependiendo de la utilidad, podríamos estar interesados, como en el problema de regresión lineal, en un término de intercepto o de nivel base, para ello puede simplemente crearse un nodo extra cuyo valor sea la constante 1. Regla de salida: En el nodo de salida \\(v\\) se calcula: \\[s_v = \\sum_{u\\rightarrow v}\\beta_{uv}x_u\\] y en lugar de aplicar la función de activación \\(f(\\cdot)\\), se usa una regla de salida para obtener el valor o vector de ajuste/predicción. Por ejemplo, en un problema de clasificación de \\(N\\) clases, la capa de salida consitirá de \\(N\\) nodos, cada uno asociado a cada clase; la predicción/ajuste \\(\\hat{y}\\in \\mathbb{R}^p\\) está dada por \\[\\hat{y}=\\arg max\\{s_{vn}\\}\\] Notemos que para este ejemplo, si la arquitectura de la red carece de capas intermedias/ocultas y con función de activación sigmoide logística entonces el modelo corresponde al de regresión logística múltiple. Al igual que en los modelos logísticos, podemos hacer que la red neuronal tenga como salida un vector de probabilidades \\(z=(z_1,\\dots,z_N)\\) dado por \\[z_n=\\frac{e^{s_{v_n}}}{e^{s_{v_1}}+\\dots+e^{s_{v_N}}}\\] Al proceso de normalización de los valores exponenciados se le conoce como la operación softmax. De manera particular una red neuronal cuya arquitectura está representada por un grafo dirigido no cíclico, (feedforward) con función de activación sigmoide logística y regla de salida softmax es un modelo paramétrico descrito por: \\[y \\sim Multinomial(1, (p_1,\\dots,p_N)\\] \\[(p_1,\\dots,p_N)=g^{nnet}(x_v; \\{\\beta_{uv}:u\\rightarrow v\\})\\] "],
["teorema-de-universalidad.html", "Capítulo 16 Teorema de Universalidad", " Capítulo 16 Teorema de Universalidad La flexibilidad y el poder de las redes neuronales radica en la estructura de composición de funciones de activación no lineales. El siguiente teorema implica que una red neuronal puede aproximar cualquier función en particular nos es relevante el poder aproximar cualquier modelo no paramétrico de regresión. Teorema. Sea \\(\\Delta^{L}=\\left\\{ \\left(y_{1},\\ldots,y_{L}\\right)\\in\\mathbb{R}_{\\geq0}^{L}\\,|\\,y_{1}+\\dots+y_{L}=1\\right\\} \\in \\mathbb{R}^{L}\\), y \\(B\\subseteq\\mathbb{R}^{p}\\) un conjunto acotado que contiene al origen y \\(\\mu\\) una medida de probabilidad en \\(B\\). Para \\(g:\\,B\\rightarrow\\Delta\\) una función medible arbitraria existe una red neuronal totalmente conectada (feedforward), de una capa oculta que tiene \\(m\\) neuronas, activación sigmoide y regla output softmax tal que su correspondiente función de probabilidades \\(g^{NNET}\\) satisface: \\[\\int_{B}\\left\\lVert g-g^{NNET}\\right\\lVert _{2}^{2}d\\mu\\leq\\frac{Clog^{2}\\left(n\\right)}{n}\\] para alguna constante universal \\(C\\). "],
["entrenamiento-de-una-red-neuronal.html", "Capítulo 17 Entrenamiento de una red neuronal 17.1 Back-propagation 17.2 Saturación 17.3 Regularización 17.4 Redes Neuronales en R", " Capítulo 17 Entrenamiento de una red neuronal En el ámbito del machine learning se le conoce como entrenamiento al proceso de encontrar buenos estimadores de los parámetros del mmodelo usado, en este caso, de la red neuronal. Sea \\(\\beta = (\\beta_{uv}; u\\rightarrow v)\\) el vector de parámetros de una red neuronal. Dados los datos de entrenamiento \\((x^1,y^1),\\dots,(x^n,y^n)\\), la log-verosimilitud de \\(\\beta\\) está dada por: \\[{\\it l}\\left(\\beta\\right)=\\sum_{i=1}^{n}\\sum_{{\\it l}=1}^{L}\\mathbb{I}_{\\left\\{ y_{i}={\\it l}\\right\\} }log\\left(\\frac{e^{s_{v_{{\\it l}}}\\left(x_{i},\\beta\\right)}}{e^{s_{v_{1}}\\left(x_{1},\\beta\\right)}+\\dots+e^{s_{v_{L}}\\left(x_{L},\\beta\\right)}}\\right) =\\colon \\sum_{i=1}^{n}{\\it l}_{i}\\left(\\beta\\right)\\] A continuación mostraremos cómo podemos maximizar la log-verosimilitud para obtener los parámetros. 17.1 Back-propagation Gracias a la estructura de composición de funciones del modelo, la derivada puede encontrarse fácilmente usando la regla de la cadena. 17.1.1 Ejemplo (back-propagation). Consideremos la siguiente red neuronal: Dado, el ejemplo de entrenamiento \\(x^i=(x_1^i,x_2^i)\\) con etiqueta/valor \\(y^i\\) la red neuronal calcula la probabilidad de salida \\(z_1=\\mathbb{P}[Y=1|X = x^i]\\) de la siguiente forma: \\[\\begin{align*} s_{3}=\\beta_{1,3}x_{1}+\\beta_{2,3}x_{2} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,x_{3}\\leftarrow f\\left(s_{3}\\right)\\\\ s_{4}=\\beta_{1,4}x_{1}+\\beta_{2,4}x_{2} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,x_{4}\\leftarrow f\\left(s_{4}\\right)\\\\ s_{5}=\\beta_{1,5}x_{1}+\\beta_{2,5}x_{2} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,x_{5}\\leftarrow f\\left(s_{5}\\right)\\\\ s_{6}=\\beta_{3,6}x_{3}+\\beta_{4,6}x_{4}+\\beta_{5,6}x_{5} &amp; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,z_{1}\\leftarrow f\\left(s_{6}\\right) \\end{align*}\\] Donde, por facilidad estamos considerando que \\(f(x)=\\frac{e^x}{1+e^x}\\). El valor de la log-verosimilitud para este ejemplo de entrenamiento se calcula como: \\[{\\it l}_{i}\\left(\\beta;\\,\\left(x^{i},\\,y^{i}\\right)\\right)=\\mathbb{I}_{\\left\\{ y^{i}=1\\right\\} }log\\left(z_{1}\\right)+\\mathbb{I}_{\\left\\{ y^{i}=2\\right\\} }log\\left(1-z_{1}\\right)\\] Y el gradiente estocástico como: \\(\\frac{\\partial l}{\\partial z_1} = \\frac{\\mathbb{I}_{\\{y^1=1\\}}}{z_1}-\\frac{\\mathbb{I}_{\\{y^1=2\\}}}{1-z_1}\\) \\(\\frac{\\partial l}{\\partial s_6} = \\frac{\\partial l}{\\partial z_1}\\cdot f&#39;(s_6)\\) \\(\\frac{\\partial l}{\\partial \\beta_{3,6}} = \\frac{\\partial l}{\\partial s_6}\\cdot x_3\\) \\(\\frac{\\partial l}{\\partial \\beta_{4,6}} = \\frac{\\partial l}{\\partial s_6}\\cdot x_4\\) \\(\\frac{\\partial l}{\\partial \\beta_{5,6}} = \\frac{\\partial l}{\\partial s_6}\\cdot x_5\\) \\(\\frac{\\partial l}{\\partial x_3} = \\frac{\\partial l}{\\partial s_6}\\cdot \\beta_{3,6}\\) \\(\\frac{\\partial l}{\\partial x_4} = \\frac{\\partial l}{\\partial s_6}\\cdot \\beta_{4,6}\\) \\(\\frac{\\partial l}{\\partial x_5} = \\frac{\\partial l}{\\partial s_6}\\cdot \\beta_{5,6}\\) \\(\\frac{\\partial l}{\\partial s_3} = \\frac{\\partial l}{\\partial x_3}\\cdot f&#39;(s_3)\\) \\(\\frac{\\partial l}{\\partial s_4} = \\frac{\\partial l}{\\partial x_4}\\cdot f&#39;(s_4)\\) \\(\\frac{\\partial l}{\\partial s_4} = \\frac{\\partial l}{\\partial x_5}\\cdot f&#39;(s_5)\\) \\(\\frac{\\partial l}{\\partial \\beta_{1,3}} = \\frac{\\partial l}{\\partial s_3}\\cdot x_1\\) \\(\\frac{\\partial l}{\\partial \\beta_{2,3}} = \\frac{\\partial l}{\\partial s_3}\\cdot x_2\\) \\(\\frac{\\partial l}{\\partial \\beta_{1,4}} = \\frac{\\partial l}{\\partial s_4}\\cdot x_1\\) \\(\\frac{\\partial l}{\\partial \\beta_{2,4}} = \\frac{\\partial l}{\\partial s_4}\\cdot x_2\\) \\(\\frac{\\partial l}{\\partial \\beta_{1,5}} = \\frac{\\partial l}{\\partial s_5}\\cdot x_1\\) \\(\\frac{\\partial l}{\\partial \\beta_{2,5}} = \\frac{\\partial l}{\\partial s_5}\\cdot x_2\\) \\(\\frac{\\partial l}{\\partial x_1} = \\frac{\\partial l}{\\partial s_3}\\cdot \\beta_{1,3}+\\frac{\\partial l}{\\partial s_4}\\cdot \\beta_{1,4}+\\frac{\\partial l}{\\partial s_5}\\cdot \\beta_{1,5}\\) \\(\\frac{\\partial l}{\\partial x_2} = \\frac{\\partial l}{\\partial s_3}\\cdot \\beta_{2,3}+\\frac{\\partial l}{\\partial s_4}\\cdot \\beta_{2,4}+\\frac{\\partial l}{\\partial s_5}\\cdot \\beta_{2,5}\\) En resúmen, para optimizar la verosimilitud por gradiente estocástico debemos seguir lo siguientes pasos (llamados _back-propagation): 1. Inicializar los parámetros \\(\\hat{\\beta}^{(0,n)}=\\{\\hat{\\beta_{u,v}}^{(0,n)}:u\\rightarrow v\\}\\) 2. Para cada fase de entrenamiento \\(t=1,2,\\dots\\) Para \\(i = 1,2,\\dots,n\\) - Obtener el vector de probabilidades \\(z = (z_1,\\dots,z_N)\\) usando como input el ejemplo \\(x^i\\) y los parámteros \\(\\hat{\\beta}^{(t, i-1)}\\) Obtener las derivadas parciales del la log-verosimilitud con respecto a: \\[\\{z_l:1\\leq l\\leq N\\} \\ \\{s_v:v\\in S_M\\} \\ \\{\\beta_{uv}:u\\in S_{M-1}, v\\in S_M\\}\\] \\[\\{x_v:v\\in S_{M-1}\\} \\ \\{s_v:v\\in S_{M-1}\\} \\ \\{\\beta_{uv}:u\\in S_{M-2}, v\\in S_{M-1}\\}\\] \\[\\{x_v:v\\in S_{M-2}\\} \\ \\{s_v:v\\in S_{M-2}\\} \\ \\{\\beta_{uv}:u\\in S_{M-3}, v\\in S_{M-1}\\}\\] \\[\\vdots\\] \\[\\{x_v:v\\in S_2\\} \\ \\{s_v:v\\in S_2\\} \\ \\{\\beta_{uv}:u\\in S_1, v\\in S_2\\}\\] en ese orden. 3. Actualizar los parámetros \\[\\hat{\\beta_{uv}}^{(t,i)} \\leftarrow \\hat{\\beta_{uv}}^{(t,i-1)}+\\alpha \\cdot \\frac{\\partial l_i}{\\partial \\beta_{uv}}\\] En general, a este procedimiento de optimización se le conoce como descenso gradiente. 17.2 Saturación Si durante el proceso de entrenamiento, algún nodo \\(v\\) tiene una valor \\(|s_v|\\) muy grande entonces el valor de \\(f&#39;(s_v)\\) será muy cercano a cero para funciones de activación sigmoides. Debido a la regla de la cadena, los valores de \\(\\beta_{uv}\\) se moverán muy lentamente hacia el óptimo, en este caso se dice que el nodo \\(v\\) está saturado. Para evitar problemas de saturación al inicio del entrenamiento comúnmente debemos estandarizar las covariables para que tengan media cero y varianza unitaria. También es conveniente inicializar los parámetos \\(\\hat{\\beta}^{(0,n)}\\) cercanos a cero, comúnmente elegidos uniformemente entre \\([-c,c]\\) para \\(c \\in (0,1)\\). Sin embargo debemos tener cuidado pues si \\(\\hat{\\beta}^{(0,n)}=0\\) entonces \\(\\frac{\\partial l}{\\partial x_v}=0 \\ \\forall \\ v\\in S_{M-1}\\) lo que hará que el algoritmo no se mueva. 17.3 Regularización Un problema típico en las redes neuronales es que suelen sobre ajustar los datos de entrenamiento dado que suelen haber más parámetros que variables. Una forma de regularizar es imponer una penalización equivalente al cuadrado de los parámetros, también llamada ridge o L2, a la log-verosimilitud y maximizar respecto a esta log-verosimilitud penalizada. \\[{\\it l}\\left(\\beta\\right)^{ridge}={\\it l}\\left(\\beta\\right)+\\frac{\\lambda}{2}||\\beta||_2^2 = \\sum_{i=1}^{n}\\sum_{{\\it l}=1}^{L}\\mathbb{I}_{\\left\\{ y_{i}={\\it l}\\right\\} }log\\left(\\frac{e^{s_{v_{{\\it l}}}\\left(x_{i},\\beta\\right)}}{e^{s_{v_{1}}\\left(x_{1},\\beta\\right)}+\\dots+e^{s_{v_{L}}\\left(x_{L},\\beta\\right)}}\\right)+\\frac{\\lambda}{2}||\\beta||_2^2\\] 17.4 Redes Neuronales en R …WIP… "],
["qué-es-una-svm.html", "Capítulo 18 ¿Qué es una SVM?", " Capítulo 18 ¿Qué es una SVM? Las maquinas de soporte vectorial son, en principio, un clasificador lineal con las característica de que no asume ninguna dsitribución a los datos y directamente busca el hiperplano óptimo que separa los datos en clases. De forma inherente una SVM es un clasificador de dos clases, sin embargo para usarlas en problemas con \\(L&gt;2\\) clases bastará con aplicar el algorítmo repetidamente comparando cada clase contra el resto o bien aplicar distintas SVM para cada par de clases y finalmente clasificar un nuevo punto vía mayoría clasificada para las SVMs. En este capítulo exploraremos brevemente la teoría detras de las SVM para dos clases. Será conveniente etiquetar las clases como \\(\\mathcal{L}=\\{-1,-1\\}\\). Dado cualquier par \\((\\beta,\\beta_0) \\in \\mathbb{S}^{p-1}\\times\\mathbb{R}\\) existe un clasificador lineal que asigna a los puntos \\(\\{x:x^T\\beta+\\beta_0&gt;0\\}\\) a 1 y a los puntos \\(\\{x:x^T\\beta+\\beta_0&lt;0\\}\\) a -1. Este clasificador es ambiguo en la decisión para la frontera del hiperplano dado por \\(H_{\\beta_0,\\beta}\\colon= \\{x:x^T\\beta+\\beta_0=0\\}\\). Para cualquier observación \\((x_i,y_i)\\), el clasificador funciona correctamente si y solo si \\(y_i(x_i^T\\beta +\\beta_0)&gt;0\\). Más aún, el valor \\(|y_i(x_i^T\\beta +\\beta_0)|\\) indica la distancia de \\(x_i\\) a la frontera de decisión. Las SMV buscan aquel hiperplano que separa completamente a las dos clases y que maximiza la distancia hacia el punto más cercano. Este plano puede determinarse a través del siguiente problema de optimización: \\[ \\begin{equation} \\max_{\\beta,\\beta_0,||\\beta||_2=1} M \\ sujeto \\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq M,\\ i=1,\\dots,n \\end{equation} \\] Donde \\(M\\) lo podemos interpretar como el margen alrededor del hiperplano optimo que no contiene a ninguna observación. "],
["estimación-de-los-coeficientes.html", "Capítulo 19 Estimación de los coeficientes", " Capítulo 19 Estimación de los coeficientes El problema de optimización mostrado es complicado de resolver fundamentalmente por la restricción \\(||\\beta||_2=1\\), sin embargo dado que el plano \\(H_{\\beta_0,\\beta}\\) es invariente bajo escalamiento sobre \\(\\beta_0\\) y \\(\\beta\\) podemos aplicar el factor \\(1/M\\) (de tal forma que \\(||\\beta||_2=1/M\\)) y el problema se transformaría en: \\[\\begin{equation} \\max_{\\beta,\\beta_0}\\frac{1}{||\\beta||_2} \\ sujeto\\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq 1,\\ i=1,\\dots,n \\end{equation} \\] Si además trabajamos con el problema equivalente \\[\\begin{equation} \\min_{\\beta,\\beta_0}\\frac{1}{2}||\\beta||_2^2 \\ sujeto\\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq 1,\\tag{19.1},n (#eq:optim) \\end{equation} \\] tendremos ahora un problema de optimización con una función objetivo cuadrática y restricciones lineales que puede resolverse eficientemente usando optimizadores convexos estándar. Para resolver el problema de optimización se usa el problema dual de Lagrange cuya teoría nos dice que la mejor cota inferior para \\(||\\beta||_2^2/2\\) es igual al valor óptimo: \\[\\begin{equation} \\max_{\\lambda\\in \\mathbb{R_{\\geq0}^n}} \\min_{\\beta,\\beta_0} L(\\beta,\\beta_0;\\lambda) \\end{equation} \\] donde \\(L(\\beta,\\beta_0;\\lambda)\\) es el lagrangiano de \\(\\@ref(eq:optim)\\). Los estimadores de los coeficientes son calculados usando a los puntos más cercanos a la frontera de decisión como puntos soporte de ahí que a esos puntos se les llame vectores soporte Sin meternos demasiado en la teoría, el proceso de estimación de los coeficientes de una SVM y la obtención de predicciones usando a dualidad de Lagrange es el que sigue: Resolver el problema dual de \\(\\@ref(eq:optim)\\) Sean \\(S=\\{i:\\lambda_i^*\\neq0\\}\\) los índices de los vectores soporte Calcular \\(\\beta^*=\\sum_{i\\in S}\\lambda_i^*y_i x_i\\) y \\(\\beta_0^*=-\\frac{1}{2}\\{\\min_{i:y_i=1}x_i^T\\beta^*+\\max_{i:y_i=-1}x_i^T\\beta^*\\}\\) Para cada punto nuevo \\(x\\), lo clasificamos con \\[\\psi^{SVM}(x)=sgn(x^T\\beta^*+\\beta_0^*)\\] "],
["rkhs-y-el-método-kernel.html", "Capítulo 20 RKHS y el método kernel 20.1 ¿Cómo escoger un kernel k?", " Capítulo 20 RKHS y el método kernel Típicamente nos encontraremos con datos para los cuales no será evidente la existencia de un hiperplano que pueda separar a las clases. Esto puede suceder porque la frontera de clasificación es no lineal, o bien porque los datos tienen mucha varianza (ruido) y las densidades de las clases se intersectan. Existen dos formas de generalizar el hiperplano de separación para resolver ambos problemas. Una forma de resolver el problema de no separabilidad es llevar los datos a alguna dimensión superior mediante \\(x \\mapsto \\phi(x)=(\\phi_1(x),\\phi_2(x),\\dots)\\) donde \\(\\phi_1,\\phi_2,\\dots\\) son funciones reales en algún conjunto \\(\\mathcal{X}\\), donde \\(\\mathcal{X}\\) no requiere alguna estructura específica. A la función \\(\\phi\\) se le conoce como función característica (feature map) y a sus componentes \\(\\phi_1,\\phi_2,\\dots\\) se les llama características (features) En dimensiones suficientemente grandes los puntos correspondientes a dos clases son siempre separables. Una pregunta natural es ¿cómo escoger buenas características y cuántas deberíamos escoger? Antes de responder estas preguntas asumamos que ya tenemos la función \\(\\phi\\) y definamos \\(k(x,x&#39;)=\\langle \\phi(x),\\phi(x&#39;)\\rangle\\) para cualesquiera \\(x,x&#39;\\in \\mathcal{X}\\), donde \\(\\langle \\cdot,\\cdot \\rangle\\) es el producto interior en el espacio de características. Con ello podemos obtener los coeficientes de la SVM como sigue: 1: Resolver el problema dual en el espacio de características \\[\\max_{\\lambda\\in \\mathbb{R}_{\\geq 0}^n,\\sum_i\\lambda_i y_i=0}\\sum_{i=1}^n\\lambda_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\lambda_i \\lambda_j y_i y_j k(x_i,x_j)\\] para obtener \\(\\lambda_1^*,\\cdots,\\lambda_n^*\\) 2: Sea \\(S=\\{i:\\lambda_i^*\\neq 0\\}\\) el conjunto de índices de los vectores soporte. 3: Calcular \\(\\beta_0^*=\\sum_{i\\in S}\\lambda_i^*y_i \\phi(x_i)\\) y \\[\\beta_0^*=-\\frac{1}{2}\\{\\min_{i:y_i=1}\\sum_{j\\in S}\\lambda _j^*y_j k(x_i,x_j)+\\max_{i:y_i=-1}\\sum_{j \\in S}\\lambda_j^* y_j k(x_i,x_j) \\}\\] 4: Cada nueva observación la clasificamos de acuerdo a \\[sgn\\{ \\sum_{i\\in S}\\lambda_i^* y_i k(x_i,x) + \\beta_0^* \\}\\] Notemos que el procedimiento anterior depende de \\(\\phi\\) a través de \\(k(\\cdot,\\cdot)\\), excepto por \\(\\beta^*\\) pero si solo queremos hacer predicciones entonces no es necesario saber exactamente quién es \\(\\phi\\) siempre que la función \\(k\\) esté dada. El siguiente teorema nos dice que cualquier kernel definido positivo \\(k\\) siempre puede obtenerse como el producto interno en algún espacio de características de dimensión infinita. Recordemos que un espacio de Hilbert es un espacio producto interior donde toda sucesión de Cauchy tiene límite respecto a la norma asociada al producto interior. Podemos pensar a un espacio de Hilbert como la generalización del espacio Euclideano. Definición: Una función simétrica \\(k:\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}\\) se llama kernel definido positivo en \\(\\mathcal{X}\\) si \\((k(x_i,x_j))_{i,j=1,\\dots,n}\\) es una matriz semi-definida positiva para cualquier \\(n\\in \\mathbb{N}\\) y \\(x_1,\\dots,x_n\\in \\mathcal{X}\\) Teorema: Si \\(k:\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}\\) es un kernel definido positivo, entonces existe un espacio de Hilbert \\(\\mathcal{H}\\) y una función \\(\\phi :\\mathcal{X}\\rightarrow \\mathcal{H}\\) tal que \\(k(x_1,x_2)=\\langle\\phi(x_1),\\phi(x_2)\\rangle\\) donde \\(\\langle\\cdot,\\cdot \\rangle_\\mathcal{H}\\) es el producto interno en \\(\\mathcal{H}\\). Esto significa que en lugar de trabajar directamente en espacios de dimensión superior, podemos trabajar con kernels definidos positivos, más aún la calidad de las SVM dependerá solamente de la elección de las funciones krnel \\(k\\). 20.1 ¿Cómo escoger un kernel k? Algunos kernels usados en la practica: Kernel lineal: \\(k(x,x&#39;)=x^Tx&#39;\\) Kernel Polinomial: $ k(x,x’)=(c+xTx’)d$. Típicamente usado si suponemos que la similaridad entre dos observaciones está dada por las covariables y interacciones de ellas. Kernel Gaussiano: \\(k(x,x&#39;)=\\exp\\{-\\frac{1}{2\\sigma^2}||x-x&#39;||_2^2\\}\\). Es el más popular para trabajar con características no lineales. Kernel de Laplace: \\(k(x,x&#39;)=\\exp\\{-\\frac{1}{\\sigma}||x-x&#39;||_2\\}\\). Similar al gaussiano, éste mide la similaridad de observaciones basados en la distancia en \\(\\mathcal{X}\\) "],
["margen-suave.html", "Capítulo 21 Margen suave", " Capítulo 21 Margen suave Recordemos que en el contexto de las SVM llamamos margen al espacio entre la frontera de decisión y el punto más cercano de cada clase. Una segunda forma de lidiar con problemas de no separabilidad es buscar el hiperplano óptimo con el margen \\(M\\), más amplio tal que el número de observaciones clasificadas erroneamente sea pequeño. Matemáticamente, lo que hacemos es agregar variables de holgura al problema de optimización original: \\[ \\begin{equation} \\max_{\\beta,\\beta_0,||\\beta||_2=1} M \\ sujeto \\ a \\ y_i(x_i^T\\beta+\\beta_0)\\geq M(1-\\xi_i),\\ i=1,\\dots,n \\end{equation} \\] \\[\\sum_{i=1}^n \\xi_i\\leq K\\] "],
["svms-en-r.html", "Capítulo 22 SVMs en R", " Capítulo 22 SVMs en R ..WIP… "],
["antecedentes.html", "Capítulo 23 Antecedentes", " Capítulo 23 Antecedentes También llamados métodos basados en árboles, son algorítmos que hacen particiones al espacio de covariables y a cada una de ellas le ajustan un modelo simple, por ejemplo una constante. Para hacer la tarea sencilla pensemos las particiones como binarias, es decir, el primer pasó será partir el espacio de covariables en dos, luego cada parte se divide en dos regiones más y continuamos el proceso hasta cumplir alguna regla de paro. Una parte muy importante de este tipo de modelos binarios es la interpretabilidad pues el árbol estratifica la población de acuerdo a sus características. "],
["árboles-de-regresión.html", "Capítulo 24 Árboles de regresión", " Capítulo 24 Árboles de regresión Supongamos que tenemos \\(p\\) covariables, una variable de respuesta/objetivo y \\(N\\) observaciones. El argoritmo debe poder decidir las variables a particionar, los puntos de quiebre y la topología del árbol. Supongamos que tenemos una partición de en \\(M\\) regiones \\(R_1,R_2,\\dots,R_M\\) y modelamos la respuesta como la constante \\(c_m\\) en cada región: \\[f(x)=\\sum_{m=1}^M c_mI(x\\in R_m)\\] Si tomamos como criterio minimizar la suma de cuadrados \\(\\sum(y_i-f(x_i))^2\\), es sencillo obtener que la mejor \\(\\hat{c}_m\\) es el promedio de \\(y_i\\) en la región \\(R_m\\): \\[\\hat{c}_m=prom(y_i|x_i\\in R_m)\\] Ahora para encontrar la mejor partición binaria en términos de la suma de cuadrados seguimos lo siguiente pasos: Consideremos la variable de partición \\(j\\) y el punto de quiebre \\(s\\) definimos el par de semiplanos \\[R_1(j,s)=\\{X|X_j\\leq s\\}\\] \\[R_2(j,s)=\\{X|X_j &gt;s\\}\\] Entonces buscamos la variabe \\(j\\) y el punto de quiebre \\(s\\) que resuelve \\[\\min_{j,s}[\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]\\] Para cualquier elección de \\(j\\) y \\(s\\) la minimización dentro de los corchetes se resuelve como \\[\\hat{c}_1=prom(y_i|x_i \\in R_1(j,s))\\] y \\[\\hat{c}_2=prom(y_i|x_i \\in R_2(j,s))\\] Para cada variable de partición, encontrar el punto de quiebre \\(s\\) se puede hacer muy rápidamente pasando por todos los puntos. Una vez encontrada la mejor partición, volvemos a hacer particiones siguiendo el mismo método. ¿Qué tanto debemos dejar que un árbol crezca? El crecimiento de un árbol hace referencia a la cantidad de particiones que contiene. En este sentido un árbol muy grande podría sobre ajustar los datos mientras que uno muy pequeño podría no capturar la estructura de ellos. El tamaño del árbol es un parámetro que incide directamente en la complejidad del modelo y el tamaño óptimo debe ser elegido de forma adaptativa según los datos. La solución usada es dejar crecer el árbol (\\(T_0\\)) hasta tener un tamaño de nodo específico y entonces podar el árbol usando una estrategia costo-complejidad como mostramos en seguida. Definimos un sub-árbol \\(T \\subset T_0\\) como cualquier árbol que pueda ser obtenido de podar el árbol \\(T_0\\), es decir, colapsar el número de sus nodos internos. Indexemos los nodos terminales con \\(m\\), siendo el nodo \\(m\\) representante de la región \\(R_m\\). Sea \\(|T|\\) el número de nodos terminales en el árbol \\(T\\) y sean \\[N_m=\\#\\{x_i\\in R_m\\}\\] \\[\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m}y_i\\] \\[Q_m(T)=\\frac{1}{N_m}\\sum_{x_i\\in R_m}(y_i-\\hat{c}_m)^2\\] definimos entonces el criterio costo-complejidad como: \\[C_\\alpha(T)=\\sum_{m=1}^{|T|}N_mQ_m(T)+\\alpha |T|\\] La idea es encontrar, para cada \\(\\alpha\\), el sub-árbol \\(T_\\alpha \\subseteq T_0\\) que minimice \\(C_\\alpha(T)\\). El parámetro \\(\\alpha \\geq 0\\) controla el tradeoff entre el tamaño del árbol y la bondad de ajuste a los datos. Valores grandes de \\(\\alpha\\) resultará en árboles pequeños y viceversa. "],
["árboles-de-clasificación.html", "Capítulo 25 Árboles de clasificación", " Capítulo 25 Árboles de clasificación Si el objetivo es clasificar los cambios necesarios al algoritmo ocurren en los criterios de partición y de poda. Para árboles de regresión usamos la medida de impureza del nodo, error cuadrático, \\(Q_m(T)\\) pero ésta no es útil para clasificación. Para cada nodo \\(m\\), representando una región \\(R_m\\) con \\(N_m\\) observaciones sea \\[\\hat{p}_{mk}=\\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i=k)\\] la proporción de observaciones de la clase \\(k\\) en el nodo \\(m\\). Clasificamos, entonces, las observaciones en el nodo \\(m\\) a la clase \\(k(m)=\\arg \\max_k \\ \\hat{p}_{mk}\\), es decir, la clase mayoritaria en el nodo \\(m\\). Algunas medidas \\(Q_m(T)\\) de impureza del nodo son: Error de clasificación: \\(\\frac{1}{N_m}\\sum_{i \\in R_m}I(y_i \\neq k(m))=1-\\hat{p}_{mk(m)}\\) Índice GINI: \\(\\sum_{k \\ne k&#39;}\\hat{p}_{mk}\\hat{p}_{mk&#39;}=\\sum_{k=1}^K\\hat{p}_{mk}(1-\\hat{p}_{mk})\\) Entropía cruzada: \\(-\\sum_{k=1}^K\\hat{p}_{mk}\\log\\hat{p}_{mk}\\) En el caso de dos clases, si \\(p\\) es la proporción de la segunda clase, las medidas son: Error de clasificación: \\(1-max(p,1-p)\\) Índice GINI: \\(2p(1-p)\\) Entropía: \\(-p\\log p-(1-p) \\log (1-p)\\) "],
["alguno-problemas-en-los-árboles.html", "Capítulo 26 Alguno problemas en los árboles 26.1 Covariables categóricas 26.2 La matriz de pérdida", " Capítulo 26 Alguno problemas en los árboles 26.1 Covariables categóricas Al trabajar con una covariable/predictora categórica con \\(q\\) posibles valores existen \\(2^{q-1}-1\\) posibles particiones binarias y el tiempo de computación se vuelve enorme para valores grandes de \\(q\\). Sin embargo si tenemos un problema de clasificación de dos clases \\((0,1)\\) podemos simplificar el problema ordenando los valores de la variable categórica de acuerdo a la propoción en la clase 1, entonces hacemos las particiones como si la variable fuera ordinal. Es posible demostrar que este procedimiento logra la partición óptima en términos de entropía o índice Gini de entre las \\(2^{q-1}-1\\) posibles. 26.2 La matriz de pérdida En problemas de clasificación, las consecuencias de hacerlo incorrectamente pueden ser más importantes para alguna clase que para otras. Para resolver esta situación definimos una matriz de pérdida \\(L\\) de \\(K\\times K\\), con \\(L_{kk&#39;}\\) siendo la pérdida incurrida por clasificar erroneamente una clase \\(k\\) como una \\(k&#39;\\). Típicamente \\(L_{kk}=0\\ \\forall k\\). Para incorporar las pérdidas en el modelo podemos modificar el índice Gini como \\(\\sum_{k \\neq k&#39;}L_{kk&#39;}\\hat{p}_{mk}\\hat{p}_{mk&#39;}\\). Aunque en el caso de \\(k&gt;2\\) clases es muy útil, no lo es para el caso \\(k=2\\), ¿por qué? "],
["árboles-en-r.html", "Capítulo 27 Árboles en R", " Capítulo 27 Árboles en R …WIP… "]
]
