<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 19 Motivación | Herramientas Estadísticas para Ciencia de Datos</title>
  <meta name="description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 19 Motivación | Herramientas Estadísticas para Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="github-repo" content="alberto-mateos-mo/seminario_est_libro" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 19 Motivación | Herramientas Estadísticas para Ciencia de Datos" />
  
  <meta name="twitter:description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  

<meta name="author" content="Sofía Villers Gómez" />
<meta name="author" content="David Alberto Mateos Montes de Oca" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="clusterización-jerárquica.html"/>

<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Herramientas Estadísticas para Ciencia de Datos</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#objetivos"><i class="fa fa-check"></i><b>0.1</b> Objetivos</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estructura"><i class="fa fa-check"></i><b>0.2</b> Estructura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#detalles-técnicos"><i class="fa fa-check"></i><b>0.3</b> Detalles técnicos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licencia"><i class="fa fa-check"></i>Licencia</a></li>
</ul></li>
<li class="part"><span><b>I El argot de ciencia de datos</b></span></li>
<li class="chapter" data-level="1" data-path="notación.html"><a href="notación.html"><i class="fa fa-check"></i><b>1</b> Notación</a></li>
<li class="chapter" data-level="2" data-path="glosario-dscml-estadística.html"><a href="glosario-dscml-estadística.html"><i class="fa fa-check"></i><b>2</b> Glosario DSc/ML - Estadística</a></li>
<li class="chapter" data-level="3" data-path="entrenamiento-de-modelos.html"><a href="entrenamiento-de-modelos.html"><i class="fa fa-check"></i><b>3</b> Entrenamiento de modelos</a></li>
<li class="part"><span><b>II Modelos Lineales</b></span></li>
<li class="chapter" data-level="4" data-path="regresión-lineal.html"><a href="regresión-lineal.html"><i class="fa fa-check"></i><b>4</b> Regresión Lineal</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#un-poco-de-história"><i class="fa fa-check"></i><b>4.1</b> Un poco de história</a></li>
<li class="chapter" data-level="4.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#objetivos-del-análisis-de-regresión"><i class="fa fa-check"></i><b>4.2</b> Objetivos del análisis de regresión</a></li>
<li class="chapter" data-level="4.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#el-algorítmo-de-regresión-lineal"><i class="fa fa-check"></i><b>4.3</b> El algorítmo de regresión lineal</a></li>
<li class="chapter" data-level="4.4" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-simple"><i class="fa fa-check"></i><b>4.4</b> Regresión lineal simple</a></li>
<li class="chapter" data-level="4.5" data-path="regresión-lineal.html"><a href="regresión-lineal.html#solución-al-problema-de-regresión-lineal-simple"><i class="fa fa-check"></i><b>4.5</b> Solución al problema de regresión lineal simple</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#mínimos-cuadrados-ordinarios"><i class="fa fa-check"></i><b>4.5.1</b> Mínimos cuadrados ordinarios</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-múltiple"><i class="fa fa-check"></i><b>4.6</b> Regresión lineal múltiple</a></li>
<li class="chapter" data-level="4.7" data-path="regresión-lineal.html"><a href="regresión-lineal.html#solución-al-problema-de-regresión-lineal-múltiple."><i class="fa fa-check"></i><b>4.7</b> Solución al problema de regresión lineal múltiple.</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#ecuaciones-normales"><i class="fa fa-check"></i><b>4.7.1</b> Ecuaciones normales</a></li>
<li class="chapter" data-level="4.7.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#evaluación-de-supuestos"><i class="fa fa-check"></i><b>4.7.2</b> Evaluación de supuestos</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="regresión-lineal.html"><a href="regresión-lineal.html#aplicación-en-r"><i class="fa fa-check"></i><b>4.8</b> Aplicación en R</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html"><i class="fa fa-check"></i><b>5</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística"><i class="fa fa-check"></i><b>5.1</b> Regresión logística</a></li>
<li class="chapter" data-level="5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-de-regresión-logísitica-simple"><i class="fa fa-check"></i><b>5.2</b> Modelo de regresión logísitica simple</a></li>
<li class="chapter" data-level="5.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-de-regresión-logísitica-multiple"><i class="fa fa-check"></i><b>5.3</b> Modelo de regresión logísitica multiple</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística-simple-en-r"><i class="fa fa-check"></i><b>5.3.1</b> Regresión logística simple en R</a></li>
<li class="chapter" data-level="5.3.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejercicio."><i class="fa fa-check"></i><b>5.3.2</b> Ejercicio.</a></li>
<li class="chapter" data-level="5.3.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística-múltiple-en-r"><i class="fa fa-check"></i><b>5.3.3</b> Regresión logística múltiple en R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-multinomial"><i class="fa fa-check"></i><b>5.4</b> Regresión Multinomial</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-multinomial-en-r"><i class="fa fa-check"></i><b>5.4.1</b> Regresión Multinomial en R</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelos-para-conteos"><i class="fa fa-check"></i><b>5.5</b> Modelos para conteos</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#sobredispersión-en-glm-poisson"><i class="fa fa-check"></i><b>5.5.1</b> Sobredispersión en GLM Poisson</a></li>
<li class="chapter" data-level="5.5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-poisson-r"><i class="fa fa-check"></i><b>5.5.2</b> GLM Poisson R</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-binomial-negativa"><i class="fa fa-check"></i><b>5.6</b> GLM binomial negativa</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-binomial-negativa-en-r"><i class="fa fa-check"></i><b>5.6.1</b> GLM Binomial Negativa en R</a></li>
<li class="chapter" data-level="5.6.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejercicio"><i class="fa fa-check"></i><b>5.6.2</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#exponencial"><i class="fa fa-check"></i><b>5.7</b> Exponencial</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-exponencial-en-r"><i class="fa fa-check"></i><b>5.7.1</b> GLM Exponencial en R</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#gamma"><i class="fa fa-check"></i><b>5.8</b> Gamma</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-gamma-en-r"><i class="fa fa-check"></i><b>5.8.1</b> GLM Gamma en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html"><i class="fa fa-check"></i><b>6</b> Modelos Lineales Generalizados (Construcción y Evaluación)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#exploración-de-los-datos."><i class="fa fa-check"></i><b>6.1</b> Exploración de los datos.</a></li>
<li class="chapter" data-level="6.2" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#elección-de-la-estructura-de-errores-y-función-liga."><i class="fa fa-check"></i><b>6.2</b> Elección de la estructura de errores y función liga.</a></li>
<li class="chapter" data-level="6.3" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#bondad-de-ajuste."><i class="fa fa-check"></i><b>6.3</b> Bondad de ajuste.</a></li>
<li class="chapter" data-level="6.4" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#simplificación-del-modelo."><i class="fa fa-check"></i><b>6.4</b> Simplificación del modelo.</a></li>
<li class="chapter" data-level="6.5" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#criterios-de-evaluación-de-modelos."><i class="fa fa-check"></i><b>6.5</b> Criterios de evaluación de modelos.</a></li>
<li class="chapter" data-level="6.6" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#análisis-de-los-residuos."><i class="fa fa-check"></i><b>6.6</b> Análisis de los residuos.</a></li>
<li class="chapter" data-level="6.7" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#evaluación-de-glms-en-r"><i class="fa fa-check"></i><b>6.7</b> Evaluación de GLMs en R</a></li>
</ul></li>
<li class="part"><span><b>III Redes neuronales</b></span></li>
<li class="chapter" data-level="7" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html"><i class="fa fa-check"></i><b>7</b> ¿Qué es una red neuronal?</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#ejemplo"><i class="fa fa-check"></i><b>7.0.1</b> Ejemplo</a></li>
<li class="chapter" data-level="7.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#teorema-de-universalidad"><i class="fa fa-check"></i><b>7.1</b> Teorema de Universalidad</a></li>
<li class="chapter" data-level="7.2" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#entrenamiento-de-una-red-neuronal"><i class="fa fa-check"></i><b>7.2</b> Entrenamiento de una red neuronal</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#back-propagation"><i class="fa fa-check"></i><b>7.2.1</b> Back-propagation</a></li>
<li class="chapter" data-level="7.2.2" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#saturación"><i class="fa fa-check"></i><b>7.2.2</b> Saturación</a></li>
<li class="chapter" data-level="7.2.3" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#regularización"><i class="fa fa-check"></i><b>7.2.3</b> Regularización</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#redes-neuronales-en-r"><i class="fa fa-check"></i><b>7.3</b> Redes Neuronales en R</a></li>
</ul></li>
<li class="part"><span><b>IV Maquinas de Soporte Vectorial (SVM)</b></span></li>
<li class="chapter" data-level="8" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html"><i class="fa fa-check"></i><b>8</b> ¿Qué es una SVM?</a>
<ul>
<li class="chapter" data-level="8.1" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#estimación-de-los-coeficientes"><i class="fa fa-check"></i><b>8.1</b> Estimación de los coeficientes</a></li>
<li class="chapter" data-level="8.2" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#rkhs-y-el-método-kernel"><i class="fa fa-check"></i><b>8.2</b> RKHS y el método kernel</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#cómo-escoger-un-kernel-k"><i class="fa fa-check"></i><b>8.2.1</b> ¿Cómo escoger un kernel k?</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#margen-suave"><i class="fa fa-check"></i><b>8.3</b> <em>Margen suave</em></a></li>
<li class="chapter" data-level="8.4" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#svms-en-r"><i class="fa fa-check"></i><b>8.4</b> SVMs en R</a></li>
</ul></li>
<li class="part"><span><b>V Árboles de regresión y clasificación</b></span></li>
<li class="chapter" data-level="9" data-path="antecedentes.html"><a href="antecedentes.html"><i class="fa fa-check"></i><b>9</b> Antecedentes</a>
<ul>
<li class="chapter" data-level="9.1" data-path="antecedentes.html"><a href="antecedentes.html#árboles-de-regresión"><i class="fa fa-check"></i><b>9.1</b> Árboles de regresión</a></li>
<li class="chapter" data-level="9.2" data-path="antecedentes.html"><a href="antecedentes.html#árboles-de-clasificación"><i class="fa fa-check"></i><b>9.2</b> Árboles de clasificación</a></li>
<li class="chapter" data-level="9.3" data-path="antecedentes.html"><a href="antecedentes.html#algunos-problemas-en-los-árboles"><i class="fa fa-check"></i><b>9.3</b> Algunos problemas en los árboles</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="antecedentes.html"><a href="antecedentes.html#covariables-categóricas"><i class="fa fa-check"></i><b>9.3.1</b> Covariables categóricas</a></li>
<li class="chapter" data-level="9.3.2" data-path="antecedentes.html"><a href="antecedentes.html#la-matriz-de-pérdida"><i class="fa fa-check"></i><b>9.3.2</b> La matriz de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="antecedentes.html"><a href="antecedentes.html#árboles-en-r"><i class="fa fa-check"></i><b>9.4</b> Árboles en R</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="antecedentes.html"><a href="antecedentes.html#ejercicios"><i class="fa fa-check"></i><b>9.4.1</b> Ejercicios</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Random Forests</b></span></li>
<li class="chapter" data-level="10" data-path="qué-es-un-random-forest.html"><a href="qué-es-un-random-forest.html"><i class="fa fa-check"></i><b>10</b> ¿Qué es un random forest?</a></li>
<li class="chapter" data-level="11" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html"><i class="fa fa-check"></i><b>11</b> ¿Cómo funciona Random Forest?</a>
<ul>
<li class="chapter" data-level="11.1" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html#ventajas-y-desventajas-de-random-forest"><i class="fa fa-check"></i><b>11.1</b> Ventajas y Desventajas de Random Forest</a></li>
<li class="chapter" data-level="11.2" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html#hiper-parámetros-más-útiles-del-random-forest"><i class="fa fa-check"></i><b>11.2</b> Hiper-parámetros más útiles del Random Forest:</a></li>
<li class="chapter" data-level="11.3" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html#otros-parámetros-también-disponibles-para-árboles"><i class="fa fa-check"></i><b>11.3</b> Otros parámetros también disponibles para árboles:</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="random-forest-en-r.html"><a href="random-forest-en-r.html"><i class="fa fa-check"></i><b>12</b> Random forest en R</a></li>
<li class="part"><span><b>VII Reducción de dimensiones</b></span></li>
<li class="chapter" data-level="13" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html"><i class="fa fa-check"></i><b>13</b> Análisis de Componentes Principales</a>
<ul>
<li class="chapter" data-level="13.1" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#introducción"><i class="fa fa-check"></i><b>13.1</b> Introducción</a></li>
<li class="chapter" data-level="13.2" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#los-componentes-principales"><i class="fa fa-check"></i><b>13.2</b> Los componentes principales</a></li>
<li class="chapter" data-level="13.3" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#interpretación-geométrica"><i class="fa fa-check"></i><b>13.3</b> Interpretación geométrica</a></li>
<li class="chapter" data-level="13.4" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#consideraciones"><i class="fa fa-check"></i><b>13.4</b> Consideraciones</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#escalamiento-de-variables"><i class="fa fa-check"></i><b>13.4.1</b> Escalamiento de variables</a></li>
<li class="chapter" data-level="13.4.2" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#varianza-explicada-por-los-componentes-principales"><i class="fa fa-check"></i><b>13.4.2</b> Varianza explicada por los componentes principales</a></li>
<li class="chapter" data-level="13.4.3" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#número-de-componentes-a-usar"><i class="fa fa-check"></i><b>13.4.3</b> Número de componentes a usar</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#pca-en-r"><i class="fa fa-check"></i><b>13.5</b> PCA en R</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="análisis-factorial.html"><a href="análisis-factorial.html"><i class="fa fa-check"></i><b>14</b> Análisis factorial</a>
<ul>
<li class="chapter" data-level="14.1" data-path="análisis-factorial.html"><a href="análisis-factorial.html#variables-latentes"><i class="fa fa-check"></i><b>14.1</b> Variables Latentes</a></li>
<li class="chapter" data-level="14.2" data-path="análisis-factorial.html"><a href="análisis-factorial.html#solución-factorial"><i class="fa fa-check"></i><b>14.2</b> Solución factorial</a></li>
<li class="chapter" data-level="14.3" data-path="análisis-factorial.html"><a href="análisis-factorial.html#rotaciones-de-factores"><i class="fa fa-check"></i><b>14.3</b> Rotaciones de factores</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="análisis-factorial.html"><a href="análisis-factorial.html#rotaciones-ortogonales"><i class="fa fa-check"></i><b>14.3.1</b> Rotaciones ortogonales</a></li>
<li class="chapter" data-level="14.3.2" data-path="análisis-factorial.html"><a href="análisis-factorial.html#rotaciones-oblicuas"><i class="fa fa-check"></i><b>14.3.2</b> Rotaciones oblicuas</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="análisis-factorial.html"><a href="análisis-factorial.html#análisis-factorial-en-r"><i class="fa fa-check"></i><b>14.4</b> Análisis factorial en R</a></li>
</ul></li>
<li class="part"><span><b>VIII Clusterización</b></span></li>
<li class="chapter" data-level="15" data-path="qué-son-los-métodos-de-clusterización.html"><a href="qué-son-los-métodos-de-clusterización.html"><i class="fa fa-check"></i><b>15</b> ¿Qué son los métodos de clusterización?</a></li>
<li class="chapter" data-level="16" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html"><i class="fa fa-check"></i><b>16</b> Conceptos teóricos</a>
<ul>
<li class="chapter" data-level="16.1" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#medidas-de-disimilaridad"><i class="fa fa-check"></i><b>16.1</b> Medidas de disimilaridad</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#datos-numéricos"><i class="fa fa-check"></i><b>16.1.1</b> Datos numéricos</a></li>
<li class="chapter" data-level="16.1.2" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#datos-ordinales"><i class="fa fa-check"></i><b>16.1.2</b> Datos ordinales</a></li>
<li class="chapter" data-level="16.1.3" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#datos-categóricos"><i class="fa fa-check"></i><b>16.1.3</b> Datos categóricos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="k-medias.html"><a href="k-medias.html"><i class="fa fa-check"></i><b>17</b> K-medias</a>
<ul>
<li class="chapter" data-level="17.1" data-path="k-medias.html"><a href="k-medias.html#algorítmo-k-medias"><i class="fa fa-check"></i><b>17.1</b> Algorítmo k-medias</a></li>
<li class="chapter" data-level="17.2" data-path="k-medias.html"><a href="k-medias.html#variantes"><i class="fa fa-check"></i><b>17.2</b> Variantes</a></li>
<li class="chapter" data-level="17.3" data-path="k-medias.html"><a href="k-medias.html#desventajas"><i class="fa fa-check"></i><b>17.3</b> Desventajas</a></li>
<li class="chapter" data-level="17.4" data-path="k-medias.html"><a href="k-medias.html#k-medias-en-r"><i class="fa fa-check"></i><b>17.4</b> K-medias en R</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html"><i class="fa fa-check"></i><b>18</b> Clusterización jerárquica</a>
<ul>
<li class="chapter" data-level="18.1" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html#paradígma-aglomerativo"><i class="fa fa-check"></i><b>18.1</b> Paradígma aglomerativo</a></li>
<li class="chapter" data-level="18.2" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html#representación-gráfica"><i class="fa fa-check"></i><b>18.2</b> Representación gráfica</a></li>
<li class="chapter" data-level="18.3" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html#el-algorítmo-aglomerativo-de-clusterización"><i class="fa fa-check"></i><b>18.3</b> El algorítmo aglomerativo de clusterización</a></li>
<li class="chapter" data-level="18.4" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html#clusterización-jerárquica-en-r"><i class="fa fa-check"></i><b>18.4</b> Clusterización jerárquica en R</a></li>
</ul></li>
<li class="part"><span><b>IX Selección de modelos</b></span></li>
<li class="chapter" data-level="19" data-path="motivación.html"><a href="motivación.html"><i class="fa fa-check"></i><b>19</b> Motivación</a>
<ul>
<li class="chapter" data-level="19.0.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#ejemplo"><i class="fa fa-check"></i><b>19.0.1</b> Ejemplo</a></li>
<li class="chapter" data-level="19.1" data-path="motivación.html"><a href="motivación.html#esquemas-básicos"><i class="fa fa-check"></i><b>19.1</b> Esquemas básicos</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="motivación.html"><a href="motivación.html#akaike-information-criterion."><i class="fa fa-check"></i><b>19.1.1</b> Akaike Information Criterion.</a></li>
<li class="chapter" data-level="19.1.2" data-path="motivación.html"><a href="motivación.html#bayesian-information-criterion."><i class="fa fa-check"></i><b>19.1.2</b> Bayesian Information Criterion.</a></li>
<li class="chapter" data-level="19.1.3" data-path="motivación.html"><a href="motivación.html#cross-validation"><i class="fa fa-check"></i><b>19.1.3</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="motivación.html"><a href="motivación.html#esquemas-adicionales"><i class="fa fa-check"></i><b>19.2</b> Esquemas adicionales</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="motivación.html"><a href="motivación.html#bootstrap"><i class="fa fa-check"></i><b>19.2.1</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="motivación.html"><a href="motivación.html#information-value-woe"><i class="fa fa-check"></i><b>19.3</b> Information Value &amp; WoE</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="motivación.html"><a href="motivación.html#weight-of-evidence-woe"><i class="fa fa-check"></i><b>19.3.1</b> Weight of Evidence (WoE)</a></li>
<li class="chapter" data-level="19.3.2" data-path="motivación.html"><a href="motivación.html#information-value"><i class="fa fa-check"></i><b>19.3.2</b> Information Value</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="motivación.html"><a href="motivación.html#matrices-de-confusión-y-derivados"><i class="fa fa-check"></i><b>19.4</b> Matrices de confusión (y derivados…)</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="motivación.html"><a href="motivación.html#estadísticos-derivados-de-las-matrices-de-confusión"><i class="fa fa-check"></i><b>19.4.1</b> Estadísticos derivados de las matrices de confusión</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="motivación.html"><a href="motivación.html#curvas-auc-roc"><i class="fa fa-check"></i><b>19.5</b> Curvas AUC-ROC</a>
<ul>
<li class="chapter" data-level="19.5.1" data-path="motivación.html"><a href="motivación.html#construcción-de-una-curva-auc-roc"><i class="fa fa-check"></i><b>19.5.1</b> Construcción de una curva AUC-ROC</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Herramientas Estadísticas para Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="motivación" class="section level1" number="19">
<h1><span class="header-section-number">Capítulo 19</span> Motivación</h1>
<p>En estadística nos enfrentaremos a una número importante de modelos que pueden usarse para explicar el fenómeno presente en nuestros datos. Y además para cada uno de ellos siempre existirá la pregunta ¿qué variables deberíamos incluir?</p>
<p>Para dar respuesta a esta necesidad es importante contar con un esquema de selección de modelos que nos permita llegar al más adecuado de manera óptima.</p>
<p>Un esquema de selección debe combinar, por un lado, una estrategia de busqueda en el espacio de modelos posibles y, por otro lado, un criterio de comparación que nos permita evaluar la calidad de cada modelo.</p>
<p>En ocasiones será sencillo escoger al grupo de modelos candidatos sin embargo para otros problemas las psoibilidades pueden ser enormes.</p>
<p>Cuando el número de posibles modelos es muy grande generalmente se emplea una técnica <em>ambiciosa</em> que parte de un modelo inicial y en cada paso se explora el espacio de modelos posibles escogiendo aquel que sea mejor de entre los <em>cercanos</em> al último explorado.</p>
<div id="ejemplo" class="section level3" number="19.0.1">
<h3><span class="header-section-number">19.0.1</span> Ejemplo</h3>
<p>Para un modelo lineal cuyo un espacio de inputs de tamaño <em>p</em> hay <span class="math inline">\(2^p\)</span> posibles sub-modelos. Si <em>p</em> fuera suficientemente pequeño podríamos listar todos los modelos sin embargo en la práctica <em>p</em> suele ser grande.</p>
<p>En este caso, para llegar a un modelo adecuado suele utilizarse alguna de las siguientes dos técnicas:</p>
<ul>
<li><p><strong>Forward Selection:</strong> Se empieza con el modelo sin variables y éstas se agregan una por una escogiendo aquella que cumple cierto criterio e.g. que la variable pase la prueba de significancia o que al agregarla al modelo el <em>accuracy</em> mejore en cierto grado.</p></li>
<li><p><strong>Backward Selection:</strong> Empezamos con el modelo de todas las variables y vamos quitando aquellas menos <em>importantes</em> de acuerdo a algún otro criterio e.g. nivel de significancia para el modelo.</p></li>
</ul>
</div>
<div id="esquemas-básicos" class="section level2" number="19.1">
<h2><span class="header-section-number">19.1</span> Esquemas básicos</h2>
<ul>
<li><p>Akaike Information Criterion</p></li>
<li><p>Bayesian Information Criterion</p></li>
<li><p>Cross-validation</p></li>
</ul>
<p>El esquema Akaike Information Criterion (AIC) busca maximizar la probabilidad de seleccionar el mejor modelo bajo el supuesto de que éste estuvo dentro del espacio de modelos evaluados.</p>
<p>Por otro lado, los esquemas Bayesian Information Criterion (BIC) y validación cruzada buscan optimizar el desempeño predictivo del modelo elegido.</p>
<div id="akaike-information-criterion." class="section level3" number="19.1.1">
<h3><span class="header-section-number">19.1.1</span> Akaike Information Criterion.</h3>
<p>Es una aproximación asíntótica a la divergencia Kullback-Leibler entre el modelo de interés y <em>la verdad</em>. También llamada entropía relativa, se define como la esperanza del logaritmo de las diferencias entre <em>P</em> y <em>Q</em>.</p>
<p>Supongamos una colección de modelos <span class="math inline">\(\mathbb{M} = \{\mathcal{M_1},\dots,\mathcal{M_K}\}\)</span> donde <span class="math inline">\(\mathcal{M_k}:=\{f(y|\theta_k):\theta_k\in\Theta_k\}\)</span>.</p>
<p>Para cada <span class="math inline">\(\mathcal{M}_k\)</span> sea <span class="math inline">\(\hat{\theta}_k\)</span> el estimador máximo verosímil y <span class="math inline">\(\hat{f}_k=f(.|\hat{\theta}_k)\)</span>.</p>
<p>Usando la divergencia Kullback-Leibler podemos calcular <span class="math display">\[D(f_0||\hat{f}_k)=\int f_0\ log(f_0)-\int f_0\ log(\hat{f}_k)\]</span></p>
<p>Dado que en realidad observamos la distribución empírica podemos estimar el término negativo como <span class="math display">\[\hat{H}_k=\frac{l_k(\hat{\theta}_k)}{n}\]</span></p>
<p>Akaike propuse el siguiente estimador: <span class="math display">\[\hat{H}_k=\frac{l_k(\hat{\theta}_k)-dim(\Theta_k)}{n}\]</span></p>
<p>De donde se deriva el número AIC como: <span class="math display">\[AIC(\mathcal{M}_k)=-2n \hat{H}_k\]</span></p>
<p>Un error común es pensar que el AIC solo puede usarse en modelos anidados sin embargo puede usarse entre modelos distintos siempre que la verosimilitud se calcule con los mismos datos.</p>
</div>
<div id="bayesian-information-criterion." class="section level3" number="19.1.2">
<h3><span class="header-section-number">19.1.2</span> Bayesian Information Criterion.</h3>
<p>BIC es una aproximación para la selección de modelos Bayesiana <em>a posteriori</em>, máxima, dados los datos.</p>
<p>Supongamos que establecemos una probabilidad <em>a priori</em> <span class="math inline">\(p_k\)</span> para el modelo <span class="math inline">\(\mathcal{M}_k\)</span> y una <em>a priori</em> para <span class="math inline">\(\theta_k|\mathcal{M}_K\)</span> de <span class="math inline">\(\pi_k\)</span>.</p>
<p>Buscamos elegir el modelo con la mayor probabilidad <em>a posteriori</em>; del teorema de Bayes la log-probabilidad <em>a posteriori</em> es: <span class="math display">\[log(\mathbb{P[\mathcal{M}|_1,...,Y_n]})=const+log(p_k)+log(\int exp(l_k(\theta_k))\pi_k(\theta_k) d\theta_k\]</span></p>
<p>De donde derivamos que el mejor modelo se obtiene al minimizar <span class="math display">\[BIC(\mathcal{M}_k)=-2l_k(\hat{\theta}_k)+dim(\Theta_k)log(n)\]</span></p>
<p>Comparado con el AIC, se impone una mayor penalización por cada parámetro adicional, por lo que el BIC tenderá a seleccionar modelos más simples.</p>
</div>
<div id="cross-validation" class="section level3" number="19.1.3">
<h3><span class="header-section-number">19.1.3</span> Cross-validation</h3>
<p>Si buscamos elegir el modelo cuyo desempeño predictivo sea el mejor, lo ideal es contar con un conjunto de prueba <em>aislado</em>. En ausencia de esto podemos probar el modelo con una parte de los datos de entrenamiento.</p>
<p>Esto puede hacerse repetidamente escogiendo porciones distintas cada vez.</p>
<div id="v-fold-cross-validation" class="section level4" number="19.1.3.1">
<h4><span class="header-section-number">19.1.3.1</span> V-fold cross-validation</h4>
<p>Los datos se dividen en <span class="math inline">\(V\)</span> subconjuntos del <em>mismo</em> tamaño. En cada paso usamos <span class="math inline">\(V-1\)</span> subconjuntos para estimar los parámetros (entrenar el modelo) y probamos en el subconjunto restante.</p>
<p>Repertimos <span class="math inline">\(V\)</span> ocasiones y se reporta el error promedio.</p>
<p>Elecciones comunes del valor <span class="math inline">\(V\)</span> son: 5, 10 y n.</p>
<p>Para <em>n</em> se define leave-one-out cross validation. Donde se usan todos los datos salvo una observación y se predice para ella.</p>
<p>Para el caso continuo se utiliza MSE y para clasificación el número de observaciones mal clasificadas.</p>
</div>
</div>
</div>
<div id="esquemas-adicionales" class="section level2" number="19.2">
<h2><span class="header-section-number">19.2</span> Esquemas adicionales</h2>
<ul>
<li><p>Bootstrap.</p></li>
<li><p>Matrices de confusión.</p></li>
<li><p>Information Value.</p></li>
<li><p>Curvas CAP y ROC.</p></li>
</ul>
<div id="bootstrap" class="section level3" number="19.2.1">
<h3><span class="header-section-number">19.2.1</span> Bootstrap</h3>
<p>Dado que nuestros modelos están construidos con la información que pudimos observar y no con la información de la población, surgen las siguientes preguntas:</p>
<ul>
<li><p>¿Hasta qué grado podemos confiar en que nuestros resultados serán <em>ciertos</em> para toda la población?</p></li>
<li><p>¿Qué tanto podrían variar bajo distintos sesgos en la información usada?</p></li>
</ul>
<p>La técnica de <em>bootstrapping</em> trata de resolver estas preguntas y con ello evaluar la calidad de un modelo a través del resampleo de estadísticos de un modelo.</p>
<div id="ejemplo-1" class="section level4" number="19.2.1.1">
<h4><span class="header-section-number">19.2.1.1</span> Ejemplo</h4>
<p>Utilizando la técnica de <em>bootstrapping</em> evaluaremos las variaciones en la <span class="math inline">\(R^2\)</span> de un modelo de regresión lineal.</p>
<p>En primer llugar debemos definir una función que extraiga la(s) métrica(s) de interés, en este caso la <span class="math inline">\(R^2\)</span>:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="motivación.html#cb175-1"></a><span class="kw">library</span>(boot)</span>
<span id="cb175-2"><a href="motivación.html#cb175-2"></a></span>
<span id="cb175-3"><a href="motivación.html#cb175-3"></a>r2 &lt;-<span class="st"> </span><span class="cf">function</span>(formula, data, index){</span>
<span id="cb175-4"><a href="motivación.html#cb175-4"></a>  d &lt;-<span class="st"> </span>data[index,]</span>
<span id="cb175-5"><a href="motivación.html#cb175-5"></a>  mod &lt;-<span class="st"> </span><span class="kw">lm</span>(formula, <span class="dt">data =</span> d)</span>
<span id="cb175-6"><a href="motivación.html#cb175-6"></a>  </span>
<span id="cb175-7"><a href="motivación.html#cb175-7"></a>  <span class="kw">return</span>(<span class="kw">summary</span>(mod)<span class="op">$</span>r.square)</span>
<span id="cb175-8"><a href="motivación.html#cb175-8"></a>}</span></code></pre></div>
<p>Posteriormente usamos el paquete <code>boot</code> para aplicar la técnica y evaluar las variaciones de la <span class="math inline">\(R^2\)</span> bajo diferentes escenarios muestrales:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="motivación.html#cb176-1"></a>results &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> mtcars, <span class="dt">statistic =</span> r2, <span class="dt">R =</span> <span class="dv">1000</span>, <span class="dt">formula =</span> mpg<span class="op">~</span>wt<span class="op">+</span>disp)</span></code></pre></div>
<p>Podemos imprimir los resultados:</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="motivación.html#cb177-1"></a>results</span></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = mtcars, statistic = r2, R = 1000, formula = mpg ~ 
##     wt + disp)
## 
## 
## Bootstrap Statistics :
##      original     bias    std. error
## t1* 0.7809306 0.01164103  0.05134422</code></pre>
<p>O bien, podemos graficarlos para leerlos más facilmente:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="motivación.html#cb179-1"></a><span class="kw">plot</span>(results)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-130-1.png" width="672" /></p>
</div>
<div id="ejercicios" class="section level4" number="19.2.1.2">
<h4><span class="header-section-number">19.2.1.2</span> Ejercicios</h4>
<ol style="list-style-type: decimal">
<li>Aplique la técnica de <em>bootstrapping</em> para evaluar variaciones en la <span class="math inline">\(R^2\)</span> de los siguientes modelos al mismo tiempo:</li>
</ol>
<ul>
<li><p>mpg~data$wt+disp</p></li>
<li><p>mpg~data$wt+disp+cyl</p></li>
<li><p>mpg~data$wt+disp+cyl+hp</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Aplique la técnica de <em>bootstrapping</em> para otro estadístico que le parezca relevante.</li>
</ol>
</div>
</div>
</div>
<div id="information-value-woe" class="section level2" number="19.3">
<h2><span class="header-section-number">19.3</span> Information Value &amp; WoE</h2>
<p>Derivaron del uso de regresión logística particularmente en problemas de riesgo de crédito.</p>
<p>Se utilizan para medir qué <em>tan bien</em> una variable logra <em>distinguir</em> una respuesta binaria.</p>
<div id="weight-of-evidence-woe" class="section level3" number="19.3.1">
<h3><span class="header-section-number">19.3.1</span> Weight of Evidence (WoE)</h3>
<p>Se calcula de la siguiente forma:</p>
<p><span class="math display">\[WoE_{x=i} = log(\frac{P[y=1 |x=i]}{P[y=0|x=i]})\]</span></p>
</div>
<div id="information-value" class="section level3" number="19.3.2">
<h3><span class="header-section-number">19.3.2</span> Information Value</h3>
<p>Su cálculo se realiza de la siguiente forma:</p>
<p><span class="math display">\[IV_{x_i} = (P[Y = 1|x=i]-P[y=0|x=i])*WoE_{x_i}\]</span></p>
<div id="interpretación-del-information-value" class="section level4" number="19.3.2.1">
<h4><span class="header-section-number">19.3.2.1</span> Interpretación del Information Value</h4>
<table>
<thead>
<tr class="header">
<th>IVx</th>
<th>Poder predictivo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>&lt;0.02</td>
<td>Variable no útil</td>
</tr>
<tr class="even">
<td>0.02-0.1</td>
<td>Poder débil</td>
</tr>
<tr class="odd">
<td>0.1-0.3</td>
<td>Poder medio</td>
</tr>
<tr class="even">
<td>0.3-0.5</td>
<td>Poder alto</td>
</tr>
<tr class="odd">
<td>0.5</td>
<td>Sospechosa</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="matrices-de-confusión-y-derivados" class="section level2" number="19.4">
<h2><span class="header-section-number">19.4</span> Matrices de confusión (y derivados…)</h2>
<ul>
<li><p>Son una forma de medir el desempeño de un algorítmo de clasificación.</p></li>
<li><p>De ellas derivan diferentes medidas que nos ayudan a entender más a fondo el desempeño de nuestro algorítmo.</p></li>
<li><p>En un problema de clasificación de 2 clases, la matriz se construye con la tabla cruzada entre las clases reales y las clases ajustadas/predichas</p></li>
</ul>
<table>
<thead>
<tr class="header">
<th></th>
<th>Positivo Real</th>
<th>Negativo Real</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Positivo predicho</td>
<td>TP</td>
<td>FP</td>
</tr>
<tr class="even">
<td>Negativo predicho</td>
<td>FN</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p>Las matrices de confusión se pueden extender a problemás de más de dos clases.</p>
<div id="estadísticos-derivados-de-las-matrices-de-confusión" class="section level3" number="19.4.1">
<h3><span class="header-section-number">19.4.1</span> Estadísticos derivados de las matrices de confusión</h3>
<ul>
<li><span class="math inline">\(Sensitivity = \frac{TP}{TP+FN}\)</span></li>
</ul>
<p>También llamada tasa de verdaderos positivos, mide la proporción de positivos predichos de entre los verdaderos reales.</p>
<ul>
<li><span class="math inline">\(Specificity = \frac{TN}{TN+FP}\)</span></li>
</ul>
<p>O tasa de verdaderos negativos, mide la proporción de negativos predichos de entre los negativos reales.</p>
<ul>
<li><span class="math inline">\(Prevalence = \frac{TP+FN}{TP+FP+TN+FN}\)</span></li>
</ul>
<p>Mide cuántos valores reales hay</p>
<ul>
<li><span class="math inline">\(PPV = \frac{sensitivity*prevalence}{(sensitivity*prevalence)+((1-specificity)*(1-prevalence))}\)</span></li>
</ul>
<p>PPV: positive predicted values</p>
<ul>
<li><span class="math inline">\(NPV = \frac{sensitivity*(1-prevalence)}{((1-sensitivity)*prevalence)+((specificity)*(1-prevalence))}\)</span></li>
</ul>
<p>NPV: negative predicted values</p>
<ul>
<li><span class="math inline">\(Detection \ rate = \frac{TP}{TP+FP+TN+FN}\)</span></li>
</ul>
<p>Mide cuántos verdaderos positivos esta detectando el modelo</p>
<ul>
<li><span class="math inline">\(Detection \ prevalence = \frac{TP+FP}{TP+FP+TN+FN}\)</span></li>
</ul>
<p>Mide cuántos positivos predichos tiene el modelo</p>
<ul>
<li><p><span class="math inline">\(Balanced \ accuracy = \frac{sensitivity+specificity}{2}\)</span></p></li>
<li><p><span class="math inline">\(Precision = \frac{TP}{TP+FP}\)</span></p></li>
</ul>
<p>Proporción de verdaderos positivos de entre los positivos predichos</p>
<ul>
<li><span class="math inline">\(Recall = \frac{TP}{TP+FN}\)</span></li>
</ul>
<p>Proporción de verdaderos positivos de entre los positivos reales</p>
<ul>
<li><span class="math inline">\(F-beta = \frac{(1+beta^2)*precision*recall}{(beta^2*precision)+recall}\)</span></li>
</ul>
</div>
</div>
<div id="curvas-auc-roc" class="section level2" number="19.5">
<h2><span class="header-section-number">19.5</span> Curvas AUC-ROC</h2>
<p>Muchos algoritmos de clasificación no generan directamente el vector de clases predichas sino que primero obtienen el vector de probabilidades de pertenencia a cada clase.</p>
<p>Dado un umbral o punto de corte para el vector de probabilidades se puede generar entonces un vector de clases asociado a ese vector.</p>
<p>Es claro que el valor de ese umbral afectará directamente al <em>accuracy</em> de clasificación del modelo.</p>
<p>Las curvas AUC-ROC nos permiten medir el desempeño del algorítmo bajo distintos umbrales.</p>
<p>ROC es una curva probabilística mientras que AUC es una medida de <em>separación.</em></p>
<div id="construcción-de-una-curva-auc-roc" class="section level3" number="19.5.1">
<h3><span class="header-section-number">19.5.1</span> Construcción de una curva AUC-ROC</h3>
<p>Para generar una curva de ROC debemos graficar la métrica <strong>1-specificity</strong> vs <strong>sensitivity</strong>.</p>
<p>Para obtener el valor AUC debemos calcular el área bajo la curva ROC.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clusterización-jerárquica.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
