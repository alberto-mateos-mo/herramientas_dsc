<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 13 Análisis de Componentes Principales | Herramientas Estadísticas para Ciencia de Datos</title>
  <meta name="description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 13 Análisis de Componentes Principales | Herramientas Estadísticas para Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="github-repo" content="alberto-mateos-mo/seminario_est_libro" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 13 Análisis de Componentes Principales | Herramientas Estadísticas para Ciencia de Datos" />
  
  <meta name="twitter:description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  

<meta name="author" content="Sofía Villers Gómez" />
<meta name="author" content="David Alberto Mateos Montes de Oca" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="random-forest-en-r.html"/>
<link rel="next" href="análisis-factorial.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Herramientas Estadísticas para Ciencia de Datos</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#objetivos"><i class="fa fa-check"></i><b>0.1</b> Objetivos</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estructura"><i class="fa fa-check"></i><b>0.2</b> Estructura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#detalles-técnicos"><i class="fa fa-check"></i><b>0.3</b> Detalles técnicos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licencia"><i class="fa fa-check"></i>Licencia</a></li>
</ul></li>
<li class="part"><span><b>I El argot de ciencia de datos</b></span></li>
<li class="chapter" data-level="1" data-path="notación.html"><a href="notación.html"><i class="fa fa-check"></i><b>1</b> Notación</a></li>
<li class="chapter" data-level="2" data-path="glosario-dscml-estadística.html"><a href="glosario-dscml-estadística.html"><i class="fa fa-check"></i><b>2</b> Glosario DSc/ML - Estadística</a></li>
<li class="chapter" data-level="3" data-path="entrenamiento-de-modelos.html"><a href="entrenamiento-de-modelos.html"><i class="fa fa-check"></i><b>3</b> Entrenamiento de modelos</a></li>
<li class="part"><span><b>II Modelos Lineales</b></span></li>
<li class="chapter" data-level="4" data-path="regresión-lineal.html"><a href="regresión-lineal.html"><i class="fa fa-check"></i><b>4</b> Regresión Lineal</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#un-poco-de-história"><i class="fa fa-check"></i><b>4.1</b> Un poco de história</a></li>
<li class="chapter" data-level="4.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#objetivos-del-análisis-de-regresión"><i class="fa fa-check"></i><b>4.2</b> Objetivos del análisis de regresión</a></li>
<li class="chapter" data-level="4.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#el-algorítmo-de-regresión-lineal"><i class="fa fa-check"></i><b>4.3</b> El algorítmo de regresión lineal</a></li>
<li class="chapter" data-level="4.4" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-simple"><i class="fa fa-check"></i><b>4.4</b> Regresión lineal simple</a></li>
<li class="chapter" data-level="4.5" data-path="regresión-lineal.html"><a href="regresión-lineal.html#solución-al-problema-de-regresión-lineal-simple"><i class="fa fa-check"></i><b>4.5</b> Solución al problema de regresión lineal simple</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#mínimos-cuadrados-ordinarios"><i class="fa fa-check"></i><b>4.5.1</b> Mínimos cuadrados ordinarios</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-múltiple"><i class="fa fa-check"></i><b>4.6</b> Regresión lineal múltiple</a></li>
<li class="chapter" data-level="4.7" data-path="regresión-lineal.html"><a href="regresión-lineal.html#solución-al-problema-de-regresión-lineal-múltiple."><i class="fa fa-check"></i><b>4.7</b> Solución al problema de regresión lineal múltiple.</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#ecuaciones-normales"><i class="fa fa-check"></i><b>4.7.1</b> Ecuaciones normales</a></li>
<li class="chapter" data-level="4.7.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#evaluación-de-supuestos"><i class="fa fa-check"></i><b>4.7.2</b> Evaluación de supuestos</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="regresión-lineal.html"><a href="regresión-lineal.html#aplicación-en-r"><i class="fa fa-check"></i><b>4.8</b> Aplicación en R</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html"><i class="fa fa-check"></i><b>5</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística"><i class="fa fa-check"></i><b>5.1</b> Regresión logística</a></li>
<li class="chapter" data-level="5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-de-regresión-logísitica-simple"><i class="fa fa-check"></i><b>5.2</b> Modelo de regresión logísitica simple</a></li>
<li class="chapter" data-level="5.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-de-regresión-logísitica-multiple"><i class="fa fa-check"></i><b>5.3</b> Modelo de regresión logísitica multiple</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística-simple-en-r"><i class="fa fa-check"></i><b>5.3.1</b> Regresión logística simple en R</a></li>
<li class="chapter" data-level="5.3.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejercicio."><i class="fa fa-check"></i><b>5.3.2</b> Ejercicio.</a></li>
<li class="chapter" data-level="5.3.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística-múltiple-en-r"><i class="fa fa-check"></i><b>5.3.3</b> Regresión logística múltiple en R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-multinomial"><i class="fa fa-check"></i><b>5.4</b> Regresión Multinomial</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-multinomial-en-r"><i class="fa fa-check"></i><b>5.4.1</b> Regresión Multinomial en R</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelos-para-conteos"><i class="fa fa-check"></i><b>5.5</b> Modelos para conteos</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#sobredispersión-en-glm-poisson"><i class="fa fa-check"></i><b>5.5.1</b> Sobredispersión en GLM Poisson</a></li>
<li class="chapter" data-level="5.5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-poisson-r"><i class="fa fa-check"></i><b>5.5.2</b> GLM Poisson R</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-binomial-negativa"><i class="fa fa-check"></i><b>5.6</b> GLM binomial negativa</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-binomial-negativa-en-r"><i class="fa fa-check"></i><b>5.6.1</b> GLM Binomial Negativa en R</a></li>
<li class="chapter" data-level="5.6.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejercicio"><i class="fa fa-check"></i><b>5.6.2</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#exponencial"><i class="fa fa-check"></i><b>5.7</b> Exponencial</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-exponencial-en-r"><i class="fa fa-check"></i><b>5.7.1</b> GLM Exponencial en R</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#gamma"><i class="fa fa-check"></i><b>5.8</b> Gamma</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-gamma-en-r"><i class="fa fa-check"></i><b>5.8.1</b> GLM Gamma en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html"><i class="fa fa-check"></i><b>6</b> Modelos Lineales Generalizados (Construcción y Evaluación)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#exploración-de-los-datos."><i class="fa fa-check"></i><b>6.1</b> Exploración de los datos.</a></li>
<li class="chapter" data-level="6.2" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#elección-de-la-estructura-de-errores-y-función-liga."><i class="fa fa-check"></i><b>6.2</b> Elección de la estructura de errores y función liga.</a></li>
<li class="chapter" data-level="6.3" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#bondad-de-ajuste."><i class="fa fa-check"></i><b>6.3</b> Bondad de ajuste.</a></li>
<li class="chapter" data-level="6.4" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#simplificación-del-modelo."><i class="fa fa-check"></i><b>6.4</b> Simplificación del modelo.</a></li>
<li class="chapter" data-level="6.5" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#criterios-de-evaluación-de-modelos."><i class="fa fa-check"></i><b>6.5</b> Criterios de evaluación de modelos.</a></li>
<li class="chapter" data-level="6.6" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#análisis-de-los-residuos."><i class="fa fa-check"></i><b>6.6</b> Análisis de los residuos.</a></li>
<li class="chapter" data-level="6.7" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#evaluación-de-glms-en-r"><i class="fa fa-check"></i><b>6.7</b> Evaluación de GLMs en R</a></li>
</ul></li>
<li class="part"><span><b>III Redes neuronales</b></span></li>
<li class="chapter" data-level="7" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html"><i class="fa fa-check"></i><b>7</b> ¿Qué es una red neuronal?</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#ejemplo"><i class="fa fa-check"></i><b>7.0.1</b> Ejemplo</a></li>
<li class="chapter" data-level="7.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#teorema-de-universalidad"><i class="fa fa-check"></i><b>7.1</b> Teorema de Universalidad</a></li>
<li class="chapter" data-level="7.2" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#entrenamiento-de-una-red-neuronal"><i class="fa fa-check"></i><b>7.2</b> Entrenamiento de una red neuronal</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#back-propagation"><i class="fa fa-check"></i><b>7.2.1</b> Back-propagation</a></li>
<li class="chapter" data-level="7.2.2" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#saturación"><i class="fa fa-check"></i><b>7.2.2</b> Saturación</a></li>
<li class="chapter" data-level="7.2.3" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#regularización"><i class="fa fa-check"></i><b>7.2.3</b> Regularización</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#redes-neuronales-en-r"><i class="fa fa-check"></i><b>7.3</b> Redes Neuronales en R</a></li>
</ul></li>
<li class="part"><span><b>IV Maquinas de Soporte Vectorial (SVM)</b></span></li>
<li class="chapter" data-level="8" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html"><i class="fa fa-check"></i><b>8</b> ¿Qué es una SVM?</a>
<ul>
<li class="chapter" data-level="8.1" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#estimación-de-los-coeficientes"><i class="fa fa-check"></i><b>8.1</b> Estimación de los coeficientes</a></li>
<li class="chapter" data-level="8.2" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#rkhs-y-el-método-kernel"><i class="fa fa-check"></i><b>8.2</b> RKHS y el método kernel</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#cómo-escoger-un-kernel-k"><i class="fa fa-check"></i><b>8.2.1</b> ¿Cómo escoger un kernel k?</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#margen-suave"><i class="fa fa-check"></i><b>8.3</b> <em>Margen suave</em></a></li>
<li class="chapter" data-level="8.4" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#svms-en-r"><i class="fa fa-check"></i><b>8.4</b> SVMs en R</a></li>
</ul></li>
<li class="part"><span><b>V Árboles de regresión y clasificación</b></span></li>
<li class="chapter" data-level="9" data-path="antecedentes.html"><a href="antecedentes.html"><i class="fa fa-check"></i><b>9</b> Antecedentes</a>
<ul>
<li class="chapter" data-level="9.1" data-path="antecedentes.html"><a href="antecedentes.html#árboles-de-regresión"><i class="fa fa-check"></i><b>9.1</b> Árboles de regresión</a></li>
<li class="chapter" data-level="9.2" data-path="antecedentes.html"><a href="antecedentes.html#árboles-de-clasificación"><i class="fa fa-check"></i><b>9.2</b> Árboles de clasificación</a></li>
<li class="chapter" data-level="9.3" data-path="antecedentes.html"><a href="antecedentes.html#algunos-problemas-en-los-árboles"><i class="fa fa-check"></i><b>9.3</b> Algunos problemas en los árboles</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="antecedentes.html"><a href="antecedentes.html#covariables-categóricas"><i class="fa fa-check"></i><b>9.3.1</b> Covariables categóricas</a></li>
<li class="chapter" data-level="9.3.2" data-path="antecedentes.html"><a href="antecedentes.html#la-matriz-de-pérdida"><i class="fa fa-check"></i><b>9.3.2</b> La matriz de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="antecedentes.html"><a href="antecedentes.html#árboles-en-r"><i class="fa fa-check"></i><b>9.4</b> Árboles en R</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="antecedentes.html"><a href="antecedentes.html#ejercicios"><i class="fa fa-check"></i><b>9.4.1</b> Ejercicios</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Random Forests</b></span></li>
<li class="chapter" data-level="10" data-path="qué-es-un-random-forest.html"><a href="qué-es-un-random-forest.html"><i class="fa fa-check"></i><b>10</b> ¿Qué es un random forest?</a></li>
<li class="chapter" data-level="11" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html"><i class="fa fa-check"></i><b>11</b> ¿Cómo funciona Random Forest?</a>
<ul>
<li class="chapter" data-level="11.1" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html#ventajas-y-desventajas-de-random-forest"><i class="fa fa-check"></i><b>11.1</b> Ventajas y Desventajas de Random Forest</a></li>
<li class="chapter" data-level="11.2" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html#hiper-parámetros-más-útiles-del-random-forest"><i class="fa fa-check"></i><b>11.2</b> Hiper-parámetros más útiles del Random Forest:</a></li>
<li class="chapter" data-level="11.3" data-path="cómo-funciona-random-forest.html"><a href="cómo-funciona-random-forest.html#otros-parámetros-también-disponibles-para-árboles"><i class="fa fa-check"></i><b>11.3</b> Otros parámetros también disponibles para árboles:</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="random-forest-en-r.html"><a href="random-forest-en-r.html"><i class="fa fa-check"></i><b>12</b> Random forest en R</a></li>
<li class="part"><span><b>VII Reducción de dimensiones</b></span></li>
<li class="chapter" data-level="13" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html"><i class="fa fa-check"></i><b>13</b> Análisis de Componentes Principales</a>
<ul>
<li class="chapter" data-level="13.1" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#introducción"><i class="fa fa-check"></i><b>13.1</b> Introducción</a></li>
<li class="chapter" data-level="13.2" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#los-componentes-principales"><i class="fa fa-check"></i><b>13.2</b> Los componentes principales</a></li>
<li class="chapter" data-level="13.3" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#interpretación-geométrica"><i class="fa fa-check"></i><b>13.3</b> Interpretación geométrica</a></li>
<li class="chapter" data-level="13.4" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#consideraciones"><i class="fa fa-check"></i><b>13.4</b> Consideraciones</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#escalamiento-de-variables"><i class="fa fa-check"></i><b>13.4.1</b> Escalamiento de variables</a></li>
<li class="chapter" data-level="13.4.2" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#varianza-explicada-por-los-componentes-principales"><i class="fa fa-check"></i><b>13.4.2</b> Varianza explicada por los componentes principales</a></li>
<li class="chapter" data-level="13.4.3" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#número-de-componentes-a-usar"><i class="fa fa-check"></i><b>13.4.3</b> Número de componentes a usar</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="análisis-de-componentes-principales.html"><a href="análisis-de-componentes-principales.html#pca-en-r"><i class="fa fa-check"></i><b>13.5</b> PCA en R</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="análisis-factorial.html"><a href="análisis-factorial.html"><i class="fa fa-check"></i><b>14</b> Análisis factorial</a>
<ul>
<li class="chapter" data-level="14.1" data-path="análisis-factorial.html"><a href="análisis-factorial.html#variables-latentes"><i class="fa fa-check"></i><b>14.1</b> Variables Latentes</a></li>
<li class="chapter" data-level="14.2" data-path="análisis-factorial.html"><a href="análisis-factorial.html#solución-factorial"><i class="fa fa-check"></i><b>14.2</b> Solución factorial</a></li>
<li class="chapter" data-level="14.3" data-path="análisis-factorial.html"><a href="análisis-factorial.html#rotaciones-de-factores"><i class="fa fa-check"></i><b>14.3</b> Rotaciones de factores</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="análisis-factorial.html"><a href="análisis-factorial.html#rotaciones-ortogonales"><i class="fa fa-check"></i><b>14.3.1</b> Rotaciones ortogonales</a></li>
<li class="chapter" data-level="14.3.2" data-path="análisis-factorial.html"><a href="análisis-factorial.html#rotaciones-oblicuas"><i class="fa fa-check"></i><b>14.3.2</b> Rotaciones oblicuas</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="análisis-factorial.html"><a href="análisis-factorial.html#análisis-factorial-en-r"><i class="fa fa-check"></i><b>14.4</b> Análisis factorial en R</a></li>
</ul></li>
<li class="part"><span><b>VIII Clusterización</b></span></li>
<li class="chapter" data-level="15" data-path="qué-son-los-métodos-de-clusterización.html"><a href="qué-son-los-métodos-de-clusterización.html"><i class="fa fa-check"></i><b>15</b> ¿Qué son los métodos de clusterización?</a></li>
<li class="chapter" data-level="16" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html"><i class="fa fa-check"></i><b>16</b> Conceptos teóricos</a>
<ul>
<li class="chapter" data-level="16.1" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#medidas-de-disimilaridad"><i class="fa fa-check"></i><b>16.1</b> Medidas de disimilaridad</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#datos-numéricos"><i class="fa fa-check"></i><b>16.1.1</b> Datos numéricos</a></li>
<li class="chapter" data-level="16.1.2" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#datos-ordinales"><i class="fa fa-check"></i><b>16.1.2</b> Datos ordinales</a></li>
<li class="chapter" data-level="16.1.3" data-path="conceptos-teóricos.html"><a href="conceptos-teóricos.html#datos-categóricos"><i class="fa fa-check"></i><b>16.1.3</b> Datos categóricos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="k-medias.html"><a href="k-medias.html"><i class="fa fa-check"></i><b>17</b> K-medias</a>
<ul>
<li class="chapter" data-level="17.1" data-path="k-medias.html"><a href="k-medias.html#algorítmo-k-medias"><i class="fa fa-check"></i><b>17.1</b> Algorítmo k-medias</a></li>
<li class="chapter" data-level="17.2" data-path="k-medias.html"><a href="k-medias.html#variantes"><i class="fa fa-check"></i><b>17.2</b> Variantes</a></li>
<li class="chapter" data-level="17.3" data-path="k-medias.html"><a href="k-medias.html#desventajas"><i class="fa fa-check"></i><b>17.3</b> Desventajas</a></li>
<li class="chapter" data-level="17.4" data-path="k-medias.html"><a href="k-medias.html#k-medias-en-r"><i class="fa fa-check"></i><b>17.4</b> K-medias en R</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html"><i class="fa fa-check"></i><b>18</b> Clusterización jerárquica</a>
<ul>
<li class="chapter" data-level="18.1" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html#paradígma-aglomerativo"><i class="fa fa-check"></i><b>18.1</b> Paradígma aglomerativo</a></li>
<li class="chapter" data-level="18.2" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html#representación-gráfica"><i class="fa fa-check"></i><b>18.2</b> Representación gráfica</a></li>
<li class="chapter" data-level="18.3" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html#el-algorítmo-aglomerativo-de-clusterización"><i class="fa fa-check"></i><b>18.3</b> El algorítmo aglomerativo de clusterización</a></li>
<li class="chapter" data-level="18.4" data-path="clusterización-jerárquica.html"><a href="clusterización-jerárquica.html#clusterización-jerárquica-en-r"><i class="fa fa-check"></i><b>18.4</b> Clusterización jerárquica en R</a></li>
</ul></li>
<li class="part"><span><b>IX Selección de modelos</b></span></li>
<li class="chapter" data-level="19" data-path="motivación.html"><a href="motivación.html"><i class="fa fa-check"></i><b>19</b> Motivación</a>
<ul>
<li class="chapter" data-level="19.0.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#ejemplo"><i class="fa fa-check"></i><b>19.0.1</b> Ejemplo</a></li>
<li class="chapter" data-level="19.1" data-path="motivación.html"><a href="motivación.html#esquemas-básicos"><i class="fa fa-check"></i><b>19.1</b> Esquemas básicos</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="motivación.html"><a href="motivación.html#akaike-information-criterion."><i class="fa fa-check"></i><b>19.1.1</b> Akaike Information Criterion.</a></li>
<li class="chapter" data-level="19.1.2" data-path="motivación.html"><a href="motivación.html#bayesian-information-criterion."><i class="fa fa-check"></i><b>19.1.2</b> Bayesian Information Criterion.</a></li>
<li class="chapter" data-level="19.1.3" data-path="motivación.html"><a href="motivación.html#cross-validation"><i class="fa fa-check"></i><b>19.1.3</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="motivación.html"><a href="motivación.html#esquemas-adicionales"><i class="fa fa-check"></i><b>19.2</b> Esquemas adicionales</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="motivación.html"><a href="motivación.html#bootstrap"><i class="fa fa-check"></i><b>19.2.1</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="motivación.html"><a href="motivación.html#information-value-woe"><i class="fa fa-check"></i><b>19.3</b> Information Value &amp; WoE</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="motivación.html"><a href="motivación.html#weight-of-evidence-woe"><i class="fa fa-check"></i><b>19.3.1</b> Weight of Evidence (WoE)</a></li>
<li class="chapter" data-level="19.3.2" data-path="motivación.html"><a href="motivación.html#information-value"><i class="fa fa-check"></i><b>19.3.2</b> Information Value</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="motivación.html"><a href="motivación.html#matrices-de-confusión-y-derivados"><i class="fa fa-check"></i><b>19.4</b> Matrices de confusión (y derivados…)</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="motivación.html"><a href="motivación.html#estadísticos-derivados-de-las-matrices-de-confusión"><i class="fa fa-check"></i><b>19.4.1</b> Estadísticos derivados de las matrices de confusión</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="motivación.html"><a href="motivación.html#curvas-auc-roc"><i class="fa fa-check"></i><b>19.5</b> Curvas AUC-ROC</a>
<ul>
<li class="chapter" data-level="19.5.1" data-path="motivación.html"><a href="motivación.html#construcción-de-una-curva-auc-roc"><i class="fa fa-check"></i><b>19.5.1</b> Construcción de una curva AUC-ROC</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Herramientas Estadísticas para Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="análisis-de-componentes-principales" class="section level1" number="13">
<h1><span class="header-section-number">Capítulo 13</span> Análisis de Componentes Principales</h1>
<div id="introducción" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Introducción</h2>
<p>Supongamos que tenemos un conjunto de datos en <span class="math inline">\(\mathbb{R}^p\)</span> y que queremos visualizarlos. Si <span class="math inline">\(p\)</span> es muy grande no podremos visulizar todo el conjunto con un solo gráfico, una manera de visualizar los datos es hacer gráficos de dsipersión de dos dimensiones.</p>
<p>Sin embargo la cantidad de gráficos de dos dimensiones que podemos hacer son <span class="math inline">\(p(1-p)/2\)</span>. Lo cual vuelve la tarea compleja conforme el valor de <span class="math inline">\(p\)</span> aumenta.</p>
<p>Lo que nos gustaría es poder encontrar una representación de los datos con menos dimensiones pero que conserva la mayor cantidad de información posible.</p>
<p>El análisis de componentes principales es una herramienta que nos permitirá lograr eso, enocntrar representaciones en menos dimensiones que conserven la mayor cantidad de varianza.</p>
</div>
<div id="los-componentes-principales" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Los componentes principales</h2>
<p>Los componentes principales de un conjunto de datos en <span class="math inline">\(\mathbb{R}^p\)</span> son una serie de combinaciones lineales de rango <span class="math inline">\(q \leq p\)</span> que, colectivamente, explican la mayoría de la varianza original.</p>
<p>Si denotamos las observaciones por <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>, sea <span class="math display">\[f(\lambda) = \mu+V_q\lambda\]</span> el modelo lineal de rango <span class="math inline">\(q\)</span> que las representa; esta ecuación es la representación paramétrica de un hiperplano afín de rango <span class="math inline">\(q\)</span>.</p>
<p>Si resolvemos el problema que nos permita encontrar el plano afín que minimice el <em>error de reconstrucción</em>: <span class="math display">\[min_{\mu,\lambda_i,V_q} \sum_{i=1}^N ||x_i-\mu -V_q\lamda_i||^2\]</span> obtenemos</p>
<p><span class="math display">\[\hat{\mu}=\bar{x}\]</span></p>
<p><span class="math display">\[\hat{\lambda}_i=V_q^T(x_i-\bar{x})\]</span></p>
<p>Con lo cual debemos encontrar la matriz ortogonal <span class="math inline">\(V_q\)</span> tal que:</p>
<p><span class="math display">\[min_{V_q}\sum_{i=1}^N||x_i-\bar{x}-V_q V_q^T(x_i-\bar{x})||^2\]</span></p>
<p>Por facilidad, asumimos que <span class="math inline">\(\bar{x}=0\)</span>. En caso contrario centramos las variables para tener media cero.</p>
<p>Entoncesla matriz <span class="math inline">\(H_q = V_q V_q^T\)</span> es una matriz de proyección que mapea cada punto <span class="math inline">\(x_i\)</span> en su reconstrucción <span class="math inline">\(H_q x_i\)</span> que es la proyección ortogonal de <span class="math inline">\(x_i\)</span> en el subespacio generado por las columnas de <span class="math inline">\(V_q\)</span></p>
<p>La solución puede ser expresada como:</p>
<p><span class="math display" id="eq:svd">\[\begin{equation} 
  X = UDV^T
  
  \tag{13.1}
\end{equation}\]</span></p>
<p>esto es, la descomposicion en valores singulares de <span class="math inline">\(X\)</span> donde:</p>
<ul>
<li><p><span class="math inline">\(U\)</span> es una matriz ortogonal de <span class="math inline">\(N\times p\)</span> cuyas columnas llamamos <em>valores singulares izquierdos</em></p></li>
<li><p><span class="math inline">\(V\)</span> es una matriz ortogonal de <span class="math inline">\(p\times p\)</span> cuyas columnas llamamos <em>valores singulares derechos</em></p></li>
<li><p><span class="math inline">\(D\)</span> es una matriz diagonal de <span class="math inline">\(p \times p\)</span> cuyos elementos cumplen <span class="math inline">\(d_1\geq d_2\geq \dots \geq d_p\geq 0\)</span> llamados valores singulares</p></li>
</ul>
<p>A las columnas de <span class="math inline">\(UD\)</span> las llamamos componentes principales de <span class="math inline">\(X\)</span></p>
<p>El primer componente principal está dado por la combinación lineal normalizada de las columnas de los datos originales cuya varianza es máxima:</p>
<p><span class="math display">\[Z_1 = \phi_{11}X_1+\phi_{21}X_2+ \dots+\phi_{p1}X_p\]</span>
N.B. <span class="math inline">\(\sum_{j=1}^p\phi_{j1}^2=1\)</span></p>
<p>A los valores <span class="math inline">\(\phi_{j1}\)</span> los llamamos <em>pesos</em> del primer componente principal. Éstos tienen la restricción de que su suma debe ser igual a uno para evitar que el primer componente tenga varianza arbitraria.</p>
</div>
<div id="interpretación-geométrica" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Interpretación geométrica</h2>
<p>Para el primer componente principal, <span class="math inline">\(\phi_1\)</span> define la dirección en el espacio para la cual los datos presentan mayor variación.</p>
<p>El segundo componente principal es aquella combinación lineal ortogonal al primer componente que tenga máxima varianzay aspi sucesivamente con el resto de componentes.</p>
<p>Los scores de los componentes principales son las proyecciones de los datos en las direcciones descritas por <span class="math inline">\(\phi_i\)</span></p>
</div>
<div id="consideraciones" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Consideraciones</h2>
<div id="escalamiento-de-variables" class="section level3" number="13.4.1">
<h3><span class="header-section-number">13.4.1</span> Escalamiento de variables</h3>
<p>Los resultados del análisis de componentes principales serán distintos si las variables no se centran (media cero) e incluso serán distintos si éstas se escalan individualmente.</p>
<p>Las diferencias en los resultados están ligadas directamente con la escala en la que estén medidas las diferentes variables del conjunto de datos.</p>
<p>Consideremos por ejemplo los datos <code>USArrests</code>, en este conjunto las variables <code>Murder</code>, <code>Rape</code> y <code>Assault</code> están medidas como ocurrencia por cada 100,000 habitantes y <code>UrbanPop</code> como porcentaje de población, en este sentido, si analizamos las varianzas:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="análisis-de-componentes-principales.html#cb149-1"></a><span class="kw">lapply</span>(USArrests, var)</span></code></pre></div>
<pre><code>## $Murder
## [1] 18.97047
## 
## $Assault
## [1] 6945.166
## 
## $UrbanPop
## [1] 209.5188
## 
## $Rape
## [1] 87.72916</code></pre>
<p>veremos que <code>Assault</code> tiene la varianza más grande lo cual causará si no escalaramos los datos, que el primer componente tengo un peso muy grande para esta variable.</p>
<p>Para evitar que los componentes principales dependan de la escala o de la elección de algún factor de escala deberemos transformar las variables para que tengan desviación estandar unitaria antes de aplicar el algoritmo de componentes principales.</p>
<p>N.B. Si las variables fueron medidas en la misma escala entonces no es necesario aplicar ninguna transformación.</p>
</div>
<div id="varianza-explicada-por-los-componentes-principales" class="section level3" number="13.4.2">
<h3><span class="header-section-number">13.4.2</span> Varianza explicada por los componentes principales</h3>
<p>La varianza total del conjunto de datos está dada por <span class="math display">\[\sum_{j=1}^pVar(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2\]</span></p>
<p>La varianza explicada por el m-ésimo componente principal es <span class="math display">\[\frac{1}{n}\sum_{i=1}^n(\sum_{j=1}^p\phi_{jm}x_{ij})^2\]</span></p>
<p>Y la proporción de varianza explicada por el m-ésimo componente es <span class="math display">\[\frac{\sum_{i=1}^n(\sum_{j=1}^p\phi_{jm}x_{ij})^2}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2}\]</span></p>
</div>
<div id="número-de-componentes-a-usar" class="section level3" number="13.4.3">
<h3><span class="header-section-number">13.4.3</span> Número de componentes a usar</h3>
<p>De manera general, un conjunto de datos n-dimensional tiene <span class="math inline">\(min(n-1,p)\)</span> componentes principales distintos. Sin embargo, dado que no típicamente usaremos este análisis para reducir la dimensión de los datos no estamos interesados en usarlos todos.</p>
<p>De hecho lo que buscamos es el menor número de componentes que nos permitan capturar una <em>buena</em> cantidad de información.</p>
<p>Para encontrar este número de componentes no existe una respuesta única. Comúnmente usaremos como ayuda el <em>screeplot</em> o gráfica de codo de la varianza del m-ésimo componente principal y escogeremos la cantidad que nos permita capturar ya sea cierta cantidad de varianza e.g. el 70% de ella o bien aquel número de componentes a partir del cual el incremento de varianza por cada componente adicional sea marginal.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
</div>
</div>
<div id="pca-en-r" class="section level2" number="13.5">
<h2><span class="header-section-number">13.5</span> PCA en R</h2>
<p>Usaremos datos de números escritos a mano para reducir las dimensiones.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="análisis-de-componentes-principales.html#cb151-1"></a>digitos &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;example_data/train.csv&quot;</span>)</span></code></pre></div>
<p>El conjunto de datos tiene información sobre 784 pixeles para 1000 números.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="análisis-de-componentes-principales.html#cb152-1"></a>un_numero &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">as.matrix</span>(digitos[<span class="dv">315</span>,]), <span class="dt">ncol =</span> <span class="dv">28</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</span>
<span id="cb152-2"><a href="análisis-de-componentes-principales.html#cb152-2"></a></span>
<span id="cb152-3"><a href="análisis-de-componentes-principales.html#cb152-3"></a><span class="kw">heatmap</span>(<span class="dt">x =</span> un_numero, <span class="dt">Colv =</span> <span class="ot">NA</span>, <span class="dt">Rowv =</span> <span class="ot">NA</span>, <span class="dt">revC =</span> T, <span class="dt">scale =</span> <span class="st">&quot;none&quot;</span>, <span class="dt">col =</span> <span class="kw">grey.colors</span>(<span class="dv">1000</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-105-1.png" width="672" /></p>
<p>Cada número consta de 784 pixels, la idea es reducir la dimensión de los número para representarlo con una menor cantidad de ellos.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="análisis-de-componentes-principales.html#cb153-1"></a>componentes &lt;-<span class="st"> </span><span class="kw">prcomp</span>(digitos, <span class="dt">center =</span> <span class="ot">TRUE</span>, <span class="dt">scale. =</span> F, <span class="dt">rank. =</span> <span class="dv">100</span>, <span class="dt">retx =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="análisis-de-componentes-principales.html#cb154-1"></a><span class="kw">summary</span>(componentes)</span></code></pre></div>
<pre><code>## Importance of first k=100 (out of 784) components:
##                              PC1       PC2       PC3       PC4       PC5
## Standard deviation     561.86766 490.76826 468.37392 440.74081 408.35793
## Proportion of Variance   0.09178   0.07002   0.06377   0.05647   0.04848
## Cumulative Proportion    0.09178   0.16179   0.22557   0.28204   0.33052
##                              PC6       PC7       PC8       PC9      PC10
## Standard deviation     397.26889 330.06467 323.24940 309.72557 280.48302
## Proportion of Variance   0.04588   0.03167   0.03038   0.02789   0.02287
## Cumulative Proportion    0.37640   0.40807   0.43845   0.46633   0.48920
##                             PC11      PC12      PC13      PC14      PC15
## Standard deviation     270.46930 260.12743 244.82230 242.63829 231.37822
## Proportion of Variance   0.02127   0.01967   0.01742   0.01712   0.01556
## Cumulative Proportion    0.51047   0.53014   0.54757   0.56468   0.58024
##                             PC16      PC17      PC18      PC19     PC20
## Standard deviation     226.88223 221.91522 217.69853 205.86651 204.0297
## Proportion of Variance   0.01496   0.01432   0.01378   0.01232   0.0121
## Cumulative Proportion    0.59521   0.60953   0.62330   0.63562   0.6477
##                             PC21      PC22     PC23      PC24      PC25
## Standard deviation     194.69877 191.20772 184.5454 183.85717 176.70210
## Proportion of Variance   0.01102   0.01063   0.0099   0.00983   0.00908
## Cumulative Proportion    0.65875   0.66937   0.6793   0.68910   0.69818
##                             PC26      PC27      PC28      PC29      PC30
## Standard deviation     174.51903 166.72761 165.20845 161.50046 157.06771
## Proportion of Variance   0.00885   0.00808   0.00793   0.00758   0.00717
## Cumulative Proportion    0.70703   0.71511   0.72305   0.73063   0.73780
##                             PC31     PC32      PC33      PC34      PC35
## Standard deviation     154.11878 151.7943 146.20414 141.49294 140.80434
## Proportion of Variance   0.00691   0.0067   0.00621   0.00582   0.00576
## Cumulative Proportion    0.74471   0.7514   0.75762   0.76344   0.76920
##                             PC36      PC37      PC38      PC39      PC40
## Standard deviation     135.72952 135.17497 133.28084 131.86642 126.22832
## Proportion of Variance   0.00536   0.00531   0.00516   0.00506   0.00463
## Cumulative Proportion    0.77456   0.77987   0.78504   0.79009   0.79472
##                             PC41      PC42      PC43      PC44      PC45
## Standard deviation     124.33321 121.16178 120.96106 118.47836 117.96289
## Proportion of Variance   0.00449   0.00427   0.00425   0.00408   0.00405
## Cumulative Proportion    0.79922   0.80349   0.80774   0.81182   0.81586
##                            PC46      PC47      PC48      PC49      PC50
## Standard deviation     114.3650 113.71241 111.81485 110.84387 108.59844
## Proportion of Variance   0.0038   0.00376   0.00363   0.00357   0.00343
## Cumulative Proportion    0.8197   0.82343   0.82706   0.83063   0.83406
##                             PC51      PC52      PC53      PC54      PC55
## Standard deviation     107.67339 103.86519 102.76554 101.25518 100.63854
## Proportion of Variance   0.00337   0.00314   0.00307   0.00298   0.00294
## Cumulative Proportion    0.83743   0.84057   0.84364   0.84662   0.84956
##                            PC56     PC57     PC58     PC59     PC60
## Standard deviation     99.32478 97.46050 96.11925 93.50884 93.43295
## Proportion of Variance  0.00287  0.00276  0.00269  0.00254  0.00254
## Cumulative Proportion   0.85243  0.85519  0.85788  0.86042  0.86296
##                            PC61     PC62    PC63    PC64     PC65     PC66
## Standard deviation     91.23502 89.83818 88.8591 86.9524 85.36302 84.59068
## Proportion of Variance  0.00242  0.00235  0.0023  0.0022  0.00212  0.00208
## Cumulative Proportion   0.86538  0.86772  0.8700  0.8722  0.87434  0.87642
##                            PC67    PC68     PC69     PC70     PC71
## Standard deviation     83.24666 82.9586 81.89783 81.24899 79.89416
## Proportion of Variance  0.00201  0.0020  0.00195  0.00192  0.00186
## Cumulative Proportion   0.87843  0.8804  0.88238  0.88430  0.88616
##                            PC72     PC73     PC74     PC75     PC76
## Standard deviation     79.00510 77.63465 77.28005 75.78120 75.20461
## Proportion of Variance  0.00181  0.00175  0.00174  0.00167  0.00164
## Cumulative Proportion   0.88797  0.88972  0.89146  0.89313  0.89477
##                            PC77     PC78     PC79     PC80     PC81
## Standard deviation     74.74010 72.97750 72.30186 72.02557 70.92370
## Proportion of Variance  0.00162  0.00155  0.00152  0.00151  0.00146
## Cumulative Proportion   0.89640  0.89794  0.89946  0.90097  0.90243
##                            PC82     PC83     PC84     PC85     PC86
## Standard deviation     69.65708 69.57450 68.86347 68.51498 68.32834
## Proportion of Variance  0.00141  0.00141  0.00138  0.00136  0.00136
## Cumulative Proportion   0.90385  0.90525  0.90663  0.90800  0.90935
##                            PC87     PC88     PC89    PC90     PC91
## Standard deviation     67.08805 65.94946 65.13713 64.1382 63.89187
## Proportion of Variance  0.00131  0.00126  0.00123  0.0012  0.00119
## Cumulative Proportion   0.91066  0.91193  0.91316  0.9144  0.91554
##                            PC92     PC93     PC94    PC95     PC96
## Standard deviation     63.15300 62.06389 61.84139 61.5554 60.31362
## Proportion of Variance  0.00116  0.00112  0.00111  0.0011  0.00106
## Cumulative Proportion   0.91670  0.91782  0.91893  0.9200  0.92109
##                            PC97     PC98     PC99   PC100
## Standard deviation     60.23987 59.82256 59.20697 58.5542
## Proportion of Variance  0.00105  0.00104  0.00102  0.0010
## Cumulative Proportion   0.92215  0.92319  0.92421  0.9252</code></pre>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="análisis-de-componentes-principales.html#cb156-1"></a>cumulative_prop &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">pc =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>, <span class="dt">cumvar =</span> <span class="kw">cumsum</span>(componentes<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(componentes<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span>))[<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>])</span>
<span id="cb156-2"><a href="análisis-de-componentes-principales.html#cb156-2"></a></span>
<span id="cb156-3"><a href="análisis-de-componentes-principales.html#cb156-3"></a><span class="kw">ggplot</span>(cumulative_prop, <span class="kw">aes</span>(pc, cumvar))<span class="op">+</span></span>
<span id="cb156-4"><a href="análisis-de-componentes-principales.html#cb156-4"></a><span class="st">  </span><span class="kw">geom_line</span>()<span class="op">+</span></span>
<span id="cb156-5"><a href="análisis-de-componentes-principales.html#cb156-5"></a><span class="st">  </span><span class="kw">geom_point</span>()<span class="op">+</span></span>
<span id="cb156-6"><a href="análisis-de-componentes-principales.html#cb156-6"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Componente&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Varianza acumulada&quot;</span>)<span class="op">+</span></span>
<span id="cb156-7"><a href="análisis-de-componentes-principales.html#cb156-7"></a><span class="st">  </span><span class="kw">theme_unam</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-109-1.png" width="672" /></p>
<p>El gráfico de varianza acumulada nos sugiere que 100 componentes son más que suficientes para representar las 784 dimensiones originales.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="análisis-de-componentes-principales.html#cb157-1"></a>nueva_rep &lt;-<span class="st"> </span><span class="kw">matrix</span>(componentes<span class="op">$</span>x[<span class="dv">315</span>,], <span class="dt">nrow =</span> <span class="dv">10</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</span>
<span id="cb157-2"><a href="análisis-de-componentes-principales.html#cb157-2"></a></span>
<span id="cb157-3"><a href="análisis-de-componentes-principales.html#cb157-3"></a><span class="kw">heatmap</span>(nueva_rep, <span class="dt">Colv =</span> <span class="ot">NA</span>, <span class="dt">Rowv =</span> <span class="ot">NA</span>, <span class="dt">revC =</span> T, <span class="dt">scale =</span> <span class="st">&quot;none&quot;</span>, <span class="dt">col =</span> <span class="kw">grey.colors</span>(<span class="dv">2</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-110-1.png" width="672" /></p>
<p>Naturalmente, para nosotros, esta representación tiene poco (o ningún) sentido pero es una buena representación de la información original que puede usarse como <em>input</em> para otros modelos e.g. predictivos.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-forest-en-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="análisis-factorial.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
