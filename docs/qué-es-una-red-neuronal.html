<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 7 ¿Qué es una red neuronal? | Herramientas Estadísticas para Ciencia de Datos</title>
  <meta name="description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 7 ¿Qué es una red neuronal? | Herramientas Estadísticas para Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  <meta name="github-repo" content="alberto-mateos-mo/seminario_est_libro" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 7 ¿Qué es una red neuronal? | Herramientas Estadísticas para Ciencia de Datos" />
  
  <meta name="twitter:description" content="Material para el curso Herramientas Estadíticas para Ciencia de Datos del Seminario de Estadística de la Facultad de Ciencias, Universidad Nacional Autónoma de México" />
  

<meta name="author" content="Sofía Villers Gómez" />
<meta name="author" content="David Alberto Mateos Montes de Oca" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modelos-lineales-generalizados-construcción-y-evaluación.html"/>
<link rel="next" href="qué-es-una-svm.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Herramientas Estadísticas para Ciencia de Datos</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#objetivos"><i class="fa fa-check"></i><b>0.1</b> Objetivos</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estructura"><i class="fa fa-check"></i><b>0.2</b> Estructura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#detalles-técnicos"><i class="fa fa-check"></i><b>0.3</b> Detalles técnicos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licencia"><i class="fa fa-check"></i>Licencia</a></li>
</ul></li>
<li class="part"><span><b>I El argot de ciencia de datos</b></span></li>
<li class="chapter" data-level="1" data-path="notación.html"><a href="notación.html"><i class="fa fa-check"></i><b>1</b> Notación</a></li>
<li class="chapter" data-level="2" data-path="glosario-dscml-estadística.html"><a href="glosario-dscml-estadística.html"><i class="fa fa-check"></i><b>2</b> Glosario DSc/ML - Estadística</a></li>
<li class="chapter" data-level="3" data-path="entrenamiento-de-modelos.html"><a href="entrenamiento-de-modelos.html"><i class="fa fa-check"></i><b>3</b> Entrenamiento de modelos</a></li>
<li class="part"><span><b>II Modelos Lineales</b></span></li>
<li class="chapter" data-level="4" data-path="regresión-lineal.html"><a href="regresión-lineal.html"><i class="fa fa-check"></i><b>4</b> Regresión Lineal</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#un-poco-de-história"><i class="fa fa-check"></i><b>4.1</b> Un poco de história</a></li>
<li class="chapter" data-level="4.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#objetivos-del-análisis-de-regresión"><i class="fa fa-check"></i><b>4.2</b> Objetivos del análisis de regresión</a></li>
<li class="chapter" data-level="4.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#el-algorítmo-de-regresión-lineal"><i class="fa fa-check"></i><b>4.3</b> El algorítmo de regresión lineal</a></li>
<li class="chapter" data-level="4.4" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-simple"><i class="fa fa-check"></i><b>4.4</b> Regresión lineal simple</a></li>
<li class="chapter" data-level="4.5" data-path="regresión-lineal.html"><a href="regresión-lineal.html#solución-al-problema-de-regresión-lineal-simple"><i class="fa fa-check"></i><b>4.5</b> Solución al problema de regresión lineal simple</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#mínimos-cuadrados-ordinarios"><i class="fa fa-check"></i><b>4.5.1</b> Mínimos cuadrados ordinarios</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-múltiple"><i class="fa fa-check"></i><b>4.6</b> Regresión lineal múltiple</a></li>
<li class="chapter" data-level="4.7" data-path="regresión-lineal.html"><a href="regresión-lineal.html#solución-al-problema-de-regresión-lineal-múltiple."><i class="fa fa-check"></i><b>4.7</b> Solución al problema de regresión lineal múltiple.</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#ecuaciones-normales"><i class="fa fa-check"></i><b>4.7.1</b> Ecuaciones normales</a></li>
<li class="chapter" data-level="4.7.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#evaluación-de-supuestos"><i class="fa fa-check"></i><b>4.7.2</b> Evaluación de supuestos</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="regresión-lineal.html"><a href="regresión-lineal.html#aplicación-en-r"><i class="fa fa-check"></i><b>4.8</b> Aplicación en R</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html"><i class="fa fa-check"></i><b>5</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística"><i class="fa fa-check"></i><b>5.1</b> Regresión logística</a></li>
<li class="chapter" data-level="5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-de-regresión-logísitica-simple"><i class="fa fa-check"></i><b>5.2</b> Modelo de regresión logísitica simple</a></li>
<li class="chapter" data-level="5.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-de-regresión-logísitica-multiple"><i class="fa fa-check"></i><b>5.3</b> Modelo de regresión logísitica multiple</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística-simple-en-r"><i class="fa fa-check"></i><b>5.3.1</b> Regresión logística simple en R</a></li>
<li class="chapter" data-level="5.3.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejercicio."><i class="fa fa-check"></i><b>5.3.2</b> Ejercicio.</a></li>
<li class="chapter" data-level="5.3.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística-múltiple-en-r"><i class="fa fa-check"></i><b>5.3.3</b> Regresión logística múltiple en R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-multinomial"><i class="fa fa-check"></i><b>5.4</b> Regresión Multinomial</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-multinomial-en-r"><i class="fa fa-check"></i><b>5.4.1</b> Regresión Multinomial en R</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelos-para-conteos"><i class="fa fa-check"></i><b>5.5</b> Modelos para conteos</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#sobredispersión-en-glm-poisson"><i class="fa fa-check"></i><b>5.5.1</b> Sobredispersión en GLM Poisson</a></li>
<li class="chapter" data-level="5.5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-poisson-r"><i class="fa fa-check"></i><b>5.5.2</b> GLM Poisson R</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-binomial-negativa"><i class="fa fa-check"></i><b>5.6</b> GLM binomial negativa</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-binomial-negativa-en-r"><i class="fa fa-check"></i><b>5.6.1</b> GLM Binomial Negativa en R</a></li>
<li class="chapter" data-level="5.6.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejercicio"><i class="fa fa-check"></i><b>5.6.2</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#exponencial"><i class="fa fa-check"></i><b>5.7</b> Exponencial</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-exponencial-en-r"><i class="fa fa-check"></i><b>5.7.1</b> GLM Exponencial en R</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#gamma"><i class="fa fa-check"></i><b>5.8</b> Gamma</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glm-gamma-en-r"><i class="fa fa-check"></i><b>5.8.1</b> GLM Gamma en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html"><i class="fa fa-check"></i><b>6</b> Modelos Lineales Generalizados (Construcción y Evaluación)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#exploración-de-los-datos."><i class="fa fa-check"></i><b>6.1</b> Exploración de los datos.</a></li>
<li class="chapter" data-level="6.2" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#elección-de-la-estructura-de-errores-y-función-liga."><i class="fa fa-check"></i><b>6.2</b> Elección de la estructura de errores y función liga.</a></li>
<li class="chapter" data-level="6.3" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#bondad-de-ajuste."><i class="fa fa-check"></i><b>6.3</b> Bondad de ajuste.</a></li>
<li class="chapter" data-level="6.4" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#simplificación-del-modelo."><i class="fa fa-check"></i><b>6.4</b> Simplificación del modelo.</a></li>
<li class="chapter" data-level="6.5" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#criterios-de-evaluación-de-modelos."><i class="fa fa-check"></i><b>6.5</b> Criterios de evaluación de modelos.</a></li>
<li class="chapter" data-level="6.6" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#análisis-de-los-residuos."><i class="fa fa-check"></i><b>6.6</b> Análisis de los residuos.</a></li>
<li class="chapter" data-level="6.7" data-path="modelos-lineales-generalizados-construcción-y-evaluación.html"><a href="modelos-lineales-generalizados-construcción-y-evaluación.html#evaluación-de-glms-en-r"><i class="fa fa-check"></i><b>6.7</b> Evaluación de GLMs en R</a></li>
</ul></li>
<li class="part"><span><b>III Redes neuronales</b></span></li>
<li class="chapter" data-level="7" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html"><i class="fa fa-check"></i><b>7</b> ¿Qué es una red neuronal?</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#ejemplo"><i class="fa fa-check"></i><b>7.0.1</b> Ejemplo</a></li>
<li class="chapter" data-level="7.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#teorema-de-universalidad"><i class="fa fa-check"></i><b>7.1</b> Teorema de Universalidad</a></li>
<li class="chapter" data-level="7.2" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#entrenamiento-de-una-red-neuronal"><i class="fa fa-check"></i><b>7.2</b> Entrenamiento de una red neuronal</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#back-propagation"><i class="fa fa-check"></i><b>7.2.1</b> Back-propagation</a></li>
<li class="chapter" data-level="7.2.2" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#saturación"><i class="fa fa-check"></i><b>7.2.2</b> Saturación</a></li>
<li class="chapter" data-level="7.2.3" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#regularización"><i class="fa fa-check"></i><b>7.2.3</b> Regularización</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="qué-es-una-red-neuronal.html"><a href="qué-es-una-red-neuronal.html#redes-neuronales-en-r"><i class="fa fa-check"></i><b>7.3</b> Redes Neuronales en R</a></li>
</ul></li>
<li class="part"><span><b>IV Maquinas de Soporte Vectorial (SVM)</b></span></li>
<li class="chapter" data-level="8" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html"><i class="fa fa-check"></i><b>8</b> ¿Qué es una SVM?</a>
<ul>
<li class="chapter" data-level="8.1" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#estimación-de-los-coeficientes"><i class="fa fa-check"></i><b>8.1</b> Estimación de los coeficientes</a></li>
<li class="chapter" data-level="8.2" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#rkhs-y-el-método-kernel"><i class="fa fa-check"></i><b>8.2</b> RKHS y el método kernel</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#cómo-escoger-un-kernel-k"><i class="fa fa-check"></i><b>8.2.1</b> ¿Cómo escoger un kernel k?</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#margen-suave"><i class="fa fa-check"></i><b>8.3</b> <em>Margen suave</em></a></li>
<li class="chapter" data-level="8.4" data-path="qué-es-una-svm.html"><a href="qué-es-una-svm.html#svms-en-r"><i class="fa fa-check"></i><b>8.4</b> SVMs en R</a></li>
</ul></li>
<li class="part"><span><b>V Árboles de regresión y clasificación</b></span></li>
<li class="chapter" data-level="9" data-path="antecedentes.html"><a href="antecedentes.html"><i class="fa fa-check"></i><b>9</b> Antecedentes</a>
<ul>
<li class="chapter" data-level="9.1" data-path="antecedentes.html"><a href="antecedentes.html#árboles-de-regresión"><i class="fa fa-check"></i><b>9.1</b> Árboles de regresión</a></li>
<li class="chapter" data-level="9.2" data-path="antecedentes.html"><a href="antecedentes.html#árboles-de-clasificación"><i class="fa fa-check"></i><b>9.2</b> Árboles de clasificación</a></li>
<li class="chapter" data-level="9.3" data-path="antecedentes.html"><a href="antecedentes.html#algunos-problemas-en-los-árboles"><i class="fa fa-check"></i><b>9.3</b> AlgunoS problemas en los árboles</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="antecedentes.html"><a href="antecedentes.html#covariables-categóricas"><i class="fa fa-check"></i><b>9.3.1</b> Covariables categóricas</a></li>
<li class="chapter" data-level="9.3.2" data-path="antecedentes.html"><a href="antecedentes.html#la-matriz-de-pérdida"><i class="fa fa-check"></i><b>9.3.2</b> La matriz de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="antecedentes.html"><a href="antecedentes.html#árboles-en-r"><i class="fa fa-check"></i><b>9.4</b> Árboles en R</a></li>
</ul></li>
<li class="part"><span><b>VI Random Forests</b></span></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Herramientas Estadísticas para Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="qué-es-una-red-neuronal" class="section level1" number="7">
<h1><span class="header-section-number">Capítulo 7</span> ¿Qué es una red neuronal?</h1>
<p>Las redes neuronales artificiales están inspiradas en la forma en la que las neuronas de nuestro cerebro trabajan en conjunto para resolver una tarea compleja. Éstas modelan una variable respuesta, ya sea de tipo continua o categórica, como función de las covariables a través de la composición de funciones no lineales.</p>
<p>De manera general, una red nueronal consiste de una <em>arquitectura</em>, una <em>regla de activación</em> y una <em>regla de salida</em>.</p>
<p><strong>Arquitectura:</strong> Puede ser descrita vía un gráfo dirigido cuyo nodos son llamados <em>neuronas</em>. Existen una grán cantidad de arquitecturas dependiendo de la naturaleza del grafo i.e. de las relaciones entre los nodos. Una descripción bastante completa de las distintas arquitecturas y sus nombres puede ser consultada <img src="https://www.asimovinstitute.org/neural-network-zoo/" alt="en este sitio." /></p>
<p><strong>Regla de activación:</strong> Típicamente el valor en cada nodo <span class="math inline">\(v\)</span> puede ser calculado como <span class="math display">\[x_v = f(\sum_{u \rightarrow v}\beta_{uv}x_u)\]</span> donde la suma se obtiene sobre los nodos <em>predecesores</em> de <span class="math inline">\(v\)</span> y <span class="math inline">\(\beta_{uv}\)</span> son los coeficientes (desconocidos) de la red.</p>
<p>A la función <span class="math inline">\(f(\cdot)\)</span> se le conoce como <em>función de activación</em>. Las más comúnmente usadas son la <em>sigmoide logística</em> <span class="math inline">\(f(x) = \frac{e^y}{1+e^y}\)</span> y la función <em>rectificadora</em> <span class="math inline">\(f(x) = \max\{{y,0}\}\)</span></p>
<div id="ejemplo" class="section level3" number="7.0.1">
<h3><span class="header-section-number">7.0.1</span> Ejemplo</h3>
<p>Dada la siguiente arquitectura calcularemos el valor de salida dado por el nodo <span class="math inline">\(x_6\)</span>.</p>
<p><img src="ejemplo.png" /></p>
<p><span class="math display">\[\begin{align*}
x_{6} &amp; =f\left(\beta_{3,6}\cdot x_{3}+\beta_{4,6}\cdot x_{4}+\beta_{5,6}\cdot x_{5}\right)\\
 &amp; \hookrightarrow x_{3}=f\left(\beta_{1,3}\cdot x_{1}+\beta_{2,3}\cdot x_{2}\right)\\
 &amp; \hookrightarrow x_{4}=f\left(\beta_{1,4}\cdot x_{1}+\beta_{2,4}\cdot x_{2}\right)\\
 &amp; \hookrightarrow x_{5}=f\left(\beta_{1,5}\cdot x_{1}+\beta_{2,5}\cdot x_{2}\right)\\
 &amp; \vdots
\end{align*}\]</span></p>
<p>donde <span class="math inline">\(f(\cdot)\)</span> es la función de activación.</p>
<p>Si suponemos que <span class="math inline">\(f(x) = \mathbb{I}(x)\)</span> entonces el problema a resolver será el de regresión lineal.</p>
<p><strong>Nodos de <em>sesgo:</em></strong> Dependiendo de la utilidad, podríamos estar interesados, como en el problema de regresión lineal, en un término de intercepto o de nivel base, para ello puede simplemente crearse un nodo extra cuyo valor sea la constante 1.</p>
<p><strong>Regla de salida:</strong> En el nodo de salida <span class="math inline">\(v\)</span> se calcula: <span class="math display">\[s_v = \sum_{u\rightarrow v}\beta_{uv}x_u\]</span> y en lugar de aplicar la función de activación <span class="math inline">\(f(\cdot)\)</span>, se usa una regla de salida para obtener el valor o vector de ajuste/predicción.</p>
<p>Por ejemplo, en un problema de clasificación de <span class="math inline">\(N\)</span> clases, la <em>capa de salida</em> consitirá de <span class="math inline">\(N\)</span> nodos, cada uno asociado a cada clase; la predicción/ajuste <span class="math inline">\(\hat{y}\in \mathbb{R}^p\)</span> está dada por <span class="math display">\[\hat{y}=\arg max\{s_{vn}\}\]</span></p>
<p>Notemos que para este ejemplo, si la arquitectura de la red carece de <em>capas</em> intermedias/ocultas y con función de activación <em>sigmoide logística</em> entonces el modelo corresponde al de regresión logística múltiple.</p>
<p>Al igual que en los modelos logísticos, podemos hacer que la red neuronal tenga como salida un vector de probabilidades <span class="math inline">\(z=(z_1,\dots,z_N)\)</span> dado por <span class="math display">\[z_n=\frac{e^{s_{v_n}}}{e^{s_{v_1}}+\dots+e^{s_{v_N}}}\]</span></p>
<p>Al proceso de normalización de los valores <em>exponenciados</em> se le conoce como la operación <em>softmax</em>.</p>
<p>De manera particular una red neuronal cuya arquitectura está representada por un grafo dirigido no cíclico, (<em>feedforward</em>) con función de activación <em>sigmoide logística</em> y regla de salida <em>softmax</em> es un modelo paramétrico descrito por:</p>
<p><span class="math display">\[y \sim Multinomial(1, (p_1,\dots,p_N)\]</span>
<span class="math display">\[(p_1,\dots,p_N)=g^{nnet}(x_v; \{\beta_{uv}:u\rightarrow v\})\]</span></p>
</div>
<div id="teorema-de-universalidad" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Teorema de Universalidad</h2>
<p>La flexibilidad y el poder de las redes neuronales radica en la estructura de composición de funciones de activación no lineales. El siguiente teorema implica que una red neuronal puede aproximar cualquier función en particular nos es relevante el poder aproximar cualquier modelo no paramétrico de regresión.</p>
<p><strong>Teorema.</strong> Sea <span class="math inline">\(\Delta^{L}=\left\{ \left(y_{1},\ldots,y_{L}\right)\in\mathbb{R}_{\geq0}^{L}\,|\,y_{1}+\dots+y_{L}=1\right\} \in \mathbb{R}^{L}\)</span>, y <span class="math inline">\(B\subseteq\mathbb{R}^{p}\)</span> un conjunto acotado que contiene al origen y <span class="math inline">\(\mu\)</span> una medida de probabilidad en <span class="math inline">\(B\)</span>.</p>
<p>Para <span class="math inline">\(g:\,B\rightarrow\Delta\)</span> una función medible arbitraria existe una red neuronal totalmente conectada (<em>feedforward</em>), de una capa oculta que tiene <span class="math inline">\(m\)</span> neuronas, activación sigmoide y regla output softmax tal que su correspondiente función de probabilidades <span class="math inline">\(g^{NNET}\)</span> satisface:</p>
<p><span class="math display">\[\int_{B}\left\lVert g-g^{NNET}\right\lVert _{2}^{2}d\mu\leq\frac{Clog^{2}\left(n\right)}{n}\]</span></p>
<p>para alguna constante universal <span class="math inline">\(C\)</span>.</p>
</div>
<div id="entrenamiento-de-una-red-neuronal" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Entrenamiento de una red neuronal</h2>
<p>En el ámbito del <em>machine learning</em> se le conoce como entrenamiento al proceso de encontrar <em>buenos</em> estimadores de los parámetros del mmodelo usado, en este caso, de la red neuronal.</p>
<p>Sea <span class="math inline">\(\beta = (\beta_{uv}; u\rightarrow v)\)</span> el vector de parámetros de una red neuronal. Dados los datos de entrenamiento <span class="math inline">\((x^1,y^1),\dots,(x^n,y^n)\)</span>, la log-verosimilitud de <span class="math inline">\(\beta\)</span> está dada por:</p>
<p><span class="math display">\[{\it l}\left(\beta\right)=\sum_{i=1}^{n}\sum_{{\it l}=1}^{L}\mathbb{I}_{\left\{ y_{i}={\it l}\right\} }log\left(\frac{e^{s_{v_{{\it l}}}\left(x_{i},\beta\right)}}{e^{s_{v_{1}}\left(x_{1},\beta\right)}+\dots+e^{s_{v_{L}}\left(x_{L},\beta\right)}}\right) =\colon \sum_{i=1}^{n}{\it l}_{i}\left(\beta\right)\]</span>
A continuación mostraremos cómo podemos maximizar la log-verosimilitud para obtener los parámetros.</p>
<div id="back-propagation" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Back-propagation</h3>
<p>Gracias a la estructura de composición de funciones del modelo, la derivada puede encontrarse <em>fácilmente</em> usando la regla de la cadena.</p>
<div id="ejemplo-back-propagation." class="section level4" number="7.2.1.1">
<h4><span class="header-section-number">7.2.1.1</span> Ejemplo (back-propagation).</h4>
<p>Consideremos la siguiente red neuronal:</p>
<p><img src="ejemplo.png" /></p>
<p>Dado, el ejemplo de entrenamiento <span class="math inline">\(x^i=(x_1^i,x_2^i)\)</span> con etiqueta/valor <span class="math inline">\(y^i\)</span> la red neuronal calcula la probabilidad de salida <span class="math inline">\(z_1=\mathbb{P}[Y=1|X = x^i]\)</span> de la siguiente forma:</p>
<p><span class="math display">\[\begin{align*}
s_{3}=\beta_{1,3}x_{1}+\beta_{2,3}x_{2} &amp; \,\,\,\,\,\,\,\,\,\,x_{3}\leftarrow f\left(s_{3}\right)\\
s_{4}=\beta_{1,4}x_{1}+\beta_{2,4}x_{2} &amp; \,\,\,\,\,\,\,\,\,\,x_{4}\leftarrow f\left(s_{4}\right)\\
s_{5}=\beta_{1,5}x_{1}+\beta_{2,5}x_{2} &amp; \,\,\,\,\,\,\,\,\,\,x_{5}\leftarrow f\left(s_{5}\right)\\
s_{6}=\beta_{3,6}x_{3}+\beta_{4,6}x_{4}+\beta_{5,6}x_{5} &amp; \,\,\,\,\,\,\,\,\,\,z_{1}\leftarrow f\left(s_{6}\right)
\end{align*}\]</span></p>
<p>Donde, por facilidad estamos considerando que <span class="math inline">\(f(x)=\frac{e^x}{1+e^x}\)</span>.</p>
<p>El valor de la log-verosimilitud para este ejemplo de entrenamiento se calcula como:</p>
<p><span class="math display">\[{\it l}_{i}\left(\beta;\,\left(x^{i},\,y^{i}\right)\right)=\mathbb{I}_{\left\{ y^{i}=1\right\} }log\left(z_{1}\right)+\mathbb{I}_{\left\{ y^{i}=2\right\} }log\left(1-z_{1}\right)\]</span></p>
<p>Y el <em>gradiente estocástico</em> como:</p>
<ul>
<li><p><span class="math inline">\(\frac{\partial l}{\partial z_1} = \frac{\mathbb{I}_{\{y^1=1\}}}{z_1}-\frac{\mathbb{I}_{\{y^1=2\}}}{1-z_1}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial s_6} = \frac{\partial l}{\partial z_1}\cdot f&#39;(s_6)\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial \beta_{3,6}} = \frac{\partial l}{\partial s_6}\cdot x_3\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial \beta_{4,6}} = \frac{\partial l}{\partial s_6}\cdot x_4\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial \beta_{5,6}} = \frac{\partial l}{\partial s_6}\cdot x_5\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial x_3} = \frac{\partial l}{\partial s_6}\cdot \beta_{3,6}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial x_4} = \frac{\partial l}{\partial s_6}\cdot \beta_{4,6}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial x_5} = \frac{\partial l}{\partial s_6}\cdot \beta_{5,6}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial s_3} = \frac{\partial l}{\partial x_3}\cdot f&#39;(s_3)\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial s_4} = \frac{\partial l}{\partial x_4}\cdot f&#39;(s_4)\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial s_4} = \frac{\partial l}{\partial x_5}\cdot f&#39;(s_5)\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial \beta_{1,3}} = \frac{\partial l}{\partial s_3}\cdot x_1\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial \beta_{2,3}} = \frac{\partial l}{\partial s_3}\cdot x_2\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial \beta_{1,4}} = \frac{\partial l}{\partial s_4}\cdot x_1\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial \beta_{2,4}} = \frac{\partial l}{\partial s_4}\cdot x_2\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial \beta_{1,5}} = \frac{\partial l}{\partial s_5}\cdot x_1\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial \beta_{2,5}} = \frac{\partial l}{\partial s_5}\cdot x_2\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial x_1} = \frac{\partial l}{\partial s_3}\cdot \beta_{1,3}+\frac{\partial l}{\partial s_4}\cdot \beta_{1,4}+\frac{\partial l}{\partial s_5}\cdot \beta_{1,5}\)</span></p></li>
<li><p><span class="math inline">\(\frac{\partial l}{\partial x_2} = \frac{\partial l}{\partial s_3}\cdot \beta_{2,3}+\frac{\partial l}{\partial s_4}\cdot \beta_{2,4}+\frac{\partial l}{\partial s_5}\cdot \beta_{2,5}\)</span></p></li>
</ul>
<p>En resúmen, para optimizar la verosimilitud por gradiente estocástico debemos seguir lo siguientes pasos (llamados _back-propagation):</p>
<p><strong>1.</strong> Inicializar los parámetros <span class="math inline">\(\hat{\beta}^{(0,n)}=\{\hat{\beta_{u,v}}^{(0,n)}:u\rightarrow v\}\)</span></p>
<p><strong>2.</strong> Para cada fase de entrenamiento <span class="math inline">\(t=1,2,\dots\)</span></p>
<p>Para <span class="math inline">\(i = 1,2,\dots,n\)</span>
- Obtener el vector de probabilidades <span class="math inline">\(z = (z_1,\dots,z_N)\)</span> usando como input el <em>ejemplo</em> <span class="math inline">\(x^i\)</span> y los parámteros <span class="math inline">\(\hat{\beta}^{(t, i-1)}\)</span></p>
<ul>
<li>Obtener las derivadas parciales del la log-verosimilitud con respecto a:</li>
</ul>
<p><span class="math display">\[\{z_l:1\leq l\leq N\} \ \{s_v:v\in S_M\} \ \{\beta_{uv}:u\in S_{M-1}, v\in S_M\}\]</span>
<span class="math display">\[\{x_v:v\in S_{M-1}\} \ \{s_v:v\in S_{M-1}\} \ \{\beta_{uv}:u\in S_{M-2}, v\in S_{M-1}\}\]</span>
<span class="math display">\[\{x_v:v\in S_{M-2}\} \ \{s_v:v\in S_{M-2}\} \ \{\beta_{uv}:u\in S_{M-3}, v\in S_{M-1}\}\]</span>
<span class="math display">\[\vdots\]</span>
<span class="math display">\[\{x_v:v\in S_2\} \ \{s_v:v\in S_2\} \ \{\beta_{uv}:u\in S_1, v\in S_2\}\]</span></p>
<p>en ese orden.</p>
<p><strong>3.</strong> Actualizar los parámetros <span class="math display">\[\hat{\beta_{uv}}^{(t,i)} \leftarrow \hat{\beta_{uv}}^{(t,i-1)}+\alpha \cdot \frac{\partial l_i}{\partial \beta_{uv}}\]</span></p>
<p>En general, a este procedimiento de optimización se le conoce como <em>descenso gradiente</em>.</p>
</div>
</div>
<div id="saturación" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Saturación</h3>
<p>Si durante el proceso de entrenamiento, algún nodo <span class="math inline">\(v\)</span> tiene una valor <span class="math inline">\(|s_v|\)</span> muy <em>grande</em> entonces el valor de <span class="math inline">\(f&#39;(s_v)\)</span> será muy cercano a cero para funciones de activación sigmoides.</p>
<p>Debido a la regla de la cadena, los valores de <span class="math inline">\(\beta_{uv}\)</span> se moverán muy lentamente hacia el óptimo, en este caso se dice que el nodo <span class="math inline">\(v\)</span> está saturado.</p>
<p>Para evitar problemas de saturación al inicio del entrenamiento comúnmente debemos estandarizar las covariables para que tengan media cero y varianza unitaria.</p>
<p>También es conveniente inicializar los parámetos <span class="math inline">\(\hat{\beta}^{(0,n)}\)</span> cercanos a cero, comúnmente elegidos uniformemente entre <span class="math inline">\([-c,c]\)</span> para <span class="math inline">\(c \in (0,1)\)</span>.</p>
<p>Sin embargo debemos tener cuidado pues si <span class="math inline">\(\hat{\beta}^{(0,n)}=0\)</span> entonces <span class="math inline">\(\frac{\partial l}{\partial x_v}=0 \ \forall \ v\in S_{M-1}\)</span> lo que hará que el algoritmo <em>no se mueva</em>.</p>
</div>
<div id="regularización" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Regularización</h3>
<p>Un problema típico en las redes neuronales es que suelen <em>sobre ajustar</em> los datos de entrenamiento dado que suelen haber más parámetros que variables.</p>
<p>Una forma de <em>regularizar</em> es imponer una penalización equivalente al cuadrado de los parámetros, también llamada <em>ridge</em> o <em>L2</em>, a la log-verosimilitud y maximizar respecto a esta log-verosimilitud penalizada.</p>
<p><span class="math display">\[{\it l}\left(\beta\right)^{ridge}={\it l}\left(\beta\right)+\frac{\lambda}{2}||\beta||_2^2 = \sum_{i=1}^{n}\sum_{{\it l}=1}^{L}\mathbb{I}_{\left\{ y_{i}={\it l}\right\} }log\left(\frac{e^{s_{v_{{\it l}}}\left(x_{i},\beta\right)}}{e^{s_{v_{1}}\left(x_{1},\beta\right)}+\dots+e^{s_{v_{L}}\left(x_{L},\beta\right)}}\right)+\frac{\lambda}{2}||\beta||_2^2\]</span></p>
</div>
</div>
<div id="redes-neuronales-en-r" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Redes Neuronales en R</h2>
<p>…WIP…</p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="modelos-lineales-generalizados-construcción-y-evaluación.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="qué-es-una-svm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
